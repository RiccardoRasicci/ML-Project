{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiccardoRasicci/ML-Project/blob/master/MLProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0AZlHq9dKpF",
        "colab_type": "code",
        "outputId": "60221d30-f907-46fd-f524-cc9874473a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiCXGsuIJiNs",
        "colab_type": "code",
        "outputId": "87ce146d-c390-46d1-e86f-9a81c55ee5c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "source": [
        "!pip3 install 'torch==1.4.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install --upgrade 'pillow'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.17.5)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.4.0)\n",
            "Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.6/dist-packages (7.0.0.post3)\n",
            "Requirement already up-to-date: pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34S_45KHdJuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9vThnsaWVCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data \n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "import torch.nn.parallel \n",
        "#from torchvision import transforms\n",
        "import torchvision.transforms as transforms \n",
        "import torchvision.models as models \n",
        "import torchvision.datasets as datasets \n",
        "#from torchvision.models import resnet18\n",
        "#from torchvision.models import alexnet\n",
        "from torchvision.models.utils import load_state_dict_from_url\n",
        "import PIL.Image as Image \n",
        "#from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "from distutils.dir_util import copy_tree\n",
        "import shutil\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E8U766CepNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Normalizes tensor with mean and standard deviation\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  \n",
        "])\n",
        "\n",
        "saliency_transform = transforms.Compose([transforms.Resize(256),   # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Normalizes tensor with mean and standard deviation\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLuq3dO5DWoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0JD5v9oAP6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SCRIPT THAT GIVEN A TRAINED MODEL AND THE DATASET COMPUTES THE SALIENCY THROUGH GRAD-CAM AND SAVES THE PATCH AROUND THE PEAK OF THE SALIENCY\n",
        "#!python gradcam.py --arch 'vgg16' --n-classes 100 -j 4 -b 16 --model '/content/drive/My Drive/grad_cam' '/content/drive/My Drive/real'  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h76-6X2dsEF",
        "colab_type": "code",
        "outputId": "f27ab9e8-a0e1-455b-8011-90c27687679b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "train_path = '/content/drive/My Drive/real'\n",
        "test_path = '/content/drive/My Drive/painting'\n",
        "saliency_path = '/content/drive/My Drive/grad_cam/GCAM'\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(train_path, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.ImageFolder(test_path, transform=eval_transform)\n",
        "saliency_dataset =  torchvision.datasets.ImageFolder(saliency_path, transform=saliency_transform)\n",
        "\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))\n",
        "print('Saliency Dataset: {}'.format(len(saliency_dataset)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Dataset: 54784\n",
            "Test Dataset: 25760\n",
            "Saliency Dataset: 54784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLnPYZ5Qip76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 100\n",
        "BATCH_SIZE = 16       \n",
        "\n",
        "LR = 1e-3            \n",
        "LR_tgt = 1e-6\n",
        "LR_discr = 1e-3\n",
        "MOMENTUM = 0.9      \n",
        "WEIGHT_DECAY = 5e-5  \n",
        "NUM_EPOCHS = 5      \n",
        "STEP_SIZE = 7       \n",
        "GAMMA = 0.1       \n",
        "\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJGiUeYkys2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "saliency_dataloader = DataLoader(saliency_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4,drop_last = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c7RrPu27fH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "__all__ = [\n",
        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
        "    'vgg19_bn', 'vgg19',\n",
        "]\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
        "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
        "    #'vgg16': 'https://s3.amazonaws.com/pytorch/models/vgg16-397923af.pth',\n",
        "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
        "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
        "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
        "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
        "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
        "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, features, num_classes=1000, init_weights=True):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        #x = self.classifier(x)\n",
        "        x = self.classifier[0](x)\n",
        "        x = self.classifier[1](x)\n",
        "        x = self.classifier[2](x)\n",
        "        x = self.classifier[3](x)\n",
        "        x = self.classifier[4](x)\n",
        "        x = self.classifier[5](x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfgs = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg11(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"VGG 11-layer model (configuration \"A\") from\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg11', 'A', False, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg11_bn(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg11_bn', 'A', True, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg13(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"VGG 13-layer model (configuration \"B\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg13', 'B', False, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg13_bn(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg13_bn', 'B', True, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg16(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"VGG 16-layer model (configuration \"D\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg16', 'D', False, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg16_bn(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg16_bn', 'D', True, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg19(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"VGG 19-layer model (configuration \"E\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg19', 'E', False, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg19_bn(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg19_bn', 'E', True, pretrained, progress, **kwargs)\n",
        "\n",
        "#####################################\n",
        "# VGG CLASSIFIER, LAST LAYER OF VGG #\n",
        "#####################################\n",
        "\n",
        "class Vgg_classifier(nn.Module):\n",
        "  \"\"\"VGG classifier model for ADDA.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "      \"\"\"Init VGG encoder.\"\"\"\n",
        "      super(Vgg_classifier, self).__init__()\n",
        "      self.fc = nn.Linear(4096,NUM_CLASSES) #20 #NUM_CLASSES\n",
        "      \n",
        "  def forward(self, x):\n",
        "      #x = self.features(x)\n",
        "      #x = self.avgpool(x)\n",
        "      #x = torch.flatten(x, 1)\n",
        "      x = self.fc(x)\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mv7ezObWJ71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWfQuHo-WLIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Discriminator for ADDA, Fully Connected Layers \n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"Discriminator model for source domain.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Init discriminator.\"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.restored = False\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(4096, 500),  \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500, 500),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500, 2),\n",
        "            nn.LogSoftmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Forward the discriminator.\"\"\"\n",
        "        out = self.layer(input)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xTVaFVptjaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def denormalize(inp, title=None, **kwargs):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    return inp\n",
        "\n",
        "\n",
        "def GradCAM(img, c, features_fn, classifier_fn):\n",
        "    feats = features_fn(img.cuda())\n",
        "    _, N, H, W = feats.size()\n",
        "    H=49\n",
        "    W=16\n",
        "    out = classifier_fn(feats)\n",
        "    c_score = out[0, c]\n",
        "    grads = torch.autograd.grad(c_score, feats)\n",
        "    w = grads[0][0].mean(-1).mean(-1)\n",
        "    sal = torch.matmul(w, feats.view(N, H*W))\n",
        "    sal = sal.view(H, W).cpu().detach().numpy()\n",
        "    sal = np.maximum(sal, 0)\n",
        "    return sal\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bio4duV_lrZU",
        "colab_type": "code",
        "outputId": "6d46d670-7cd4-4710-e351-9f201a0a5671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "#Source net\n",
        "src_encoder = vgg16(pretrained = True)\n",
        "src_classifier = Vgg_classifier()\n",
        "src_encoder.classifier[6] = nn.Sequential(*[src_encoder.classifier[i] for i in range(6)]) #delete the last layer\n",
        "print(src_encoder.classifier)\n",
        "\n",
        "#Target net\n",
        "tgt_encoder = vgg16(pretrained = True)\n",
        "tgt_encoder.classifier[6] = nn.Sequential(*[tgt_encoder.classifier[i] for i in range(6)]) #delete the last layer\n",
        "print(tgt_encoder.classifier)\n",
        "\n",
        "#Evidence net\n",
        "evidence_encoder = vgg16(pretrained = True)\n",
        "evidence_classifier = Vgg_classifier()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n",
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_WxTFRzU6T3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlO-kGGvxEI0",
        "colab_type": "code",
        "outputId": "30edaa53-2207-4518-f041-ce481e3481a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss() \n",
        "\n",
        "parameters_to_optimize = src_encoder.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "src_encoder = src_encoder.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "src_classifier = src_classifier.to(DEVICE)\n",
        "\n",
        "source_loss_vector = []\n",
        "\n",
        "cudnn.benchmark \n",
        "\n",
        "current_step = 0\n",
        "# Start iterating over the epochs\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "  # Iterate over the dataset\n",
        "  for images, labels in train_dataloader:\n",
        "    # Bring data over the device of choice\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    src_encoder.train()\n",
        "    src_classifier.train() \n",
        " \n",
        "    optimizer.zero_grad() \n",
        "    \n",
        "    outputs = src_classifier(src_encoder(images))\n",
        "\n",
        "\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = criterion(outputs, labels)\n",
        "    source_loss_vector.append(loss)\n",
        "    # Log loss\n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "\n",
        "    loss.backward()  #\n",
        "    optimizer.step() #\n",
        "\n",
        "    current_step += 1\n",
        "\n",
        "  \n",
        "  scheduler.step() \n",
        "\n",
        "\n",
        "source_model = copy.deepcopy(src_encoder)  \n",
        "source_classifier = copy.deepcopy(src_classifier) \n",
        "\n",
        "plt.plot(source_loss_vector)\n",
        "plt.show()\n",
        "#torch.save(src_encoder, src_encoder_path+'src_encoder.pt')\n",
        "#torch.save(src_classifier, src_classifier_path+'src_classifier.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/5, LR = [0.001]\n",
            "Step 0, Loss 4.713348865509033\n",
            "Step 10, Loss 4.464717388153076\n",
            "Step 20, Loss 4.590317726135254\n",
            "Step 30, Loss 4.775928974151611\n",
            "Step 40, Loss 4.59471321105957\n",
            "Step 50, Loss 4.5011749267578125\n",
            "Step 60, Loss 4.644313812255859\n",
            "Step 70, Loss 4.5069804191589355\n",
            "Step 80, Loss 4.402587413787842\n",
            "Step 90, Loss 4.399096488952637\n",
            "Step 100, Loss 4.556825160980225\n",
            "Step 110, Loss 4.450524806976318\n",
            "Step 120, Loss 4.235389232635498\n",
            "Step 130, Loss 4.407645225524902\n",
            "Step 140, Loss 3.981003761291504\n",
            "Step 150, Loss 3.773704767227173\n",
            "Step 160, Loss 3.3142189979553223\n",
            "Step 170, Loss 3.398442506790161\n",
            "Step 180, Loss 3.1765246391296387\n",
            "Step 190, Loss 3.5766143798828125\n",
            "Step 200, Loss 3.1784448623657227\n",
            "Step 210, Loss 2.536832571029663\n",
            "Step 220, Loss 3.5523524284362793\n",
            "Step 230, Loss 3.07024884223938\n",
            "Step 240, Loss 2.6562256813049316\n",
            "Step 250, Loss 2.799959421157837\n",
            "Step 260, Loss 2.795896053314209\n",
            "Step 270, Loss 2.2139477729797363\n",
            "Step 280, Loss 3.2734947204589844\n",
            "Step 290, Loss 2.509591817855835\n",
            "Step 300, Loss 3.0028598308563232\n",
            "Step 310, Loss 2.2450456619262695\n",
            "Step 320, Loss 2.396374225616455\n",
            "Step 330, Loss 3.1495394706726074\n",
            "Step 340, Loss 2.667262077331543\n",
            "Step 350, Loss 1.3705707788467407\n",
            "Step 360, Loss 2.123683214187622\n",
            "Step 370, Loss 1.9698455333709717\n",
            "Step 380, Loss 2.594611644744873\n",
            "Step 390, Loss 2.1894514560699463\n",
            "Step 400, Loss 2.7803964614868164\n",
            "Step 410, Loss 2.916415214538574\n",
            "Step 420, Loss 2.07401442527771\n",
            "Step 430, Loss 2.1832873821258545\n",
            "Step 440, Loss 1.7712880373001099\n",
            "Step 450, Loss 1.6153217554092407\n",
            "Step 460, Loss 1.4508838653564453\n",
            "Step 470, Loss 1.0346753597259521\n",
            "Step 480, Loss 1.253297209739685\n",
            "Step 490, Loss 1.113541603088379\n",
            "Step 500, Loss 1.5574418306350708\n",
            "Step 510, Loss 1.9588714838027954\n",
            "Step 520, Loss 1.1060062646865845\n",
            "Step 530, Loss 2.400378942489624\n",
            "Step 540, Loss 1.7313587665557861\n",
            "Step 550, Loss 2.2313432693481445\n",
            "Step 560, Loss 2.430159568786621\n",
            "Step 570, Loss 2.5134952068328857\n",
            "Step 580, Loss 0.9956988096237183\n",
            "Step 590, Loss 1.6238486766815186\n",
            "Step 600, Loss 1.6077890396118164\n",
            "Step 610, Loss 2.4869611263275146\n",
            "Step 620, Loss 1.4256788492202759\n",
            "Step 630, Loss 3.2398557662963867\n",
            "Step 640, Loss 1.7531962394714355\n",
            "Step 650, Loss 1.7872008085250854\n",
            "Step 660, Loss 1.389132022857666\n",
            "Step 670, Loss 1.8192411661148071\n",
            "Step 680, Loss 1.619418978691101\n",
            "Step 690, Loss 1.100975751876831\n",
            "Step 700, Loss 1.3722238540649414\n",
            "Step 710, Loss 1.7598328590393066\n",
            "Step 720, Loss 2.049690008163452\n",
            "Step 730, Loss 1.8299461603164673\n",
            "Step 740, Loss 1.366936445236206\n",
            "Step 750, Loss 1.5689679384231567\n",
            "Step 760, Loss 1.8619972467422485\n",
            "Step 770, Loss 1.212881088256836\n",
            "Step 780, Loss 2.255918025970459\n",
            "Step 790, Loss 1.6614099740982056\n",
            "Step 800, Loss 1.961316466331482\n",
            "Step 810, Loss 0.9175068140029907\n",
            "Step 820, Loss 1.1010186672210693\n",
            "Step 830, Loss 1.0495853424072266\n",
            "Step 840, Loss 1.1359400749206543\n",
            "Step 850, Loss 1.0333590507507324\n",
            "Step 860, Loss 1.7970831394195557\n",
            "Step 870, Loss 1.5889806747436523\n",
            "Step 880, Loss 2.1900813579559326\n",
            "Step 890, Loss 1.2651665210723877\n",
            "Step 900, Loss 1.223533272743225\n",
            "Step 910, Loss 1.0652971267700195\n",
            "Step 920, Loss 1.3834469318389893\n",
            "Step 930, Loss 2.081622362136841\n",
            "Step 940, Loss 1.3021454811096191\n",
            "Step 950, Loss 1.7908414602279663\n",
            "Step 960, Loss 0.8131281733512878\n",
            "Step 970, Loss 1.6186224222183228\n",
            "Step 980, Loss 1.4757963418960571\n",
            "Step 990, Loss 2.042320966720581\n",
            "Step 1000, Loss 1.0839561223983765\n",
            "Step 1010, Loss 0.5296847820281982\n",
            "Step 1020, Loss 1.030397653579712\n",
            "Step 1030, Loss 1.2011489868164062\n",
            "Step 1040, Loss 0.6587467789649963\n",
            "Step 1050, Loss 1.3592991828918457\n",
            "Step 1060, Loss 1.0631153583526611\n",
            "Step 1070, Loss 0.8959785103797913\n",
            "Step 1080, Loss 1.8125442266464233\n",
            "Step 1090, Loss 1.2202032804489136\n",
            "Step 1100, Loss 0.99150550365448\n",
            "Step 1110, Loss 1.2437217235565186\n",
            "Step 1120, Loss 1.484931468963623\n",
            "Step 1130, Loss 1.1109645366668701\n",
            "Step 1140, Loss 2.291781187057495\n",
            "Step 1150, Loss 0.6477929949760437\n",
            "Step 1160, Loss 1.475030779838562\n",
            "Step 1170, Loss 1.9053385257720947\n",
            "Step 1180, Loss 2.0571446418762207\n",
            "Step 1190, Loss 1.270898699760437\n",
            "Step 1200, Loss 1.779996395111084\n",
            "Step 1210, Loss 0.6976226568222046\n",
            "Step 1220, Loss 1.7866520881652832\n",
            "Step 1230, Loss 1.8120743036270142\n",
            "Step 1240, Loss 1.1533735990524292\n",
            "Step 1250, Loss 1.3239003419876099\n",
            "Step 1260, Loss 0.4652145504951477\n",
            "Step 1270, Loss 1.4270589351654053\n",
            "Step 1280, Loss 0.8793296217918396\n",
            "Step 1290, Loss 1.1401751041412354\n",
            "Step 1300, Loss 1.2631163597106934\n",
            "Step 1310, Loss 1.0655508041381836\n",
            "Step 1320, Loss 1.255352258682251\n",
            "Step 1330, Loss 1.1781036853790283\n",
            "Step 1340, Loss 0.7794037461280823\n",
            "Step 1350, Loss 1.5335726737976074\n",
            "Step 1360, Loss 0.7957167029380798\n",
            "Step 1370, Loss 1.1450719833374023\n",
            "Step 1380, Loss 0.9169232249259949\n",
            "Step 1390, Loss 1.2644996643066406\n",
            "Step 1400, Loss 1.0234596729278564\n",
            "Step 1410, Loss 1.1780140399932861\n",
            "Step 1420, Loss 0.9683421850204468\n",
            "Step 1430, Loss 1.066382884979248\n",
            "Step 1440, Loss 0.7992451786994934\n",
            "Step 1450, Loss 1.9913759231567383\n",
            "Step 1460, Loss 1.3178791999816895\n",
            "Step 1470, Loss 0.41754329204559326\n",
            "Step 1480, Loss 1.3874804973602295\n",
            "Step 1490, Loss 0.5240870118141174\n",
            "Step 1500, Loss 0.9383388757705688\n",
            "Step 1510, Loss 1.3373136520385742\n",
            "Step 1520, Loss 0.8691186308860779\n",
            "Step 1530, Loss 0.659489095211029\n",
            "Step 1540, Loss 0.6005654335021973\n",
            "Step 1550, Loss 1.677587628364563\n",
            "Step 1560, Loss 1.2560111284255981\n",
            "Step 1570, Loss 1.236734390258789\n",
            "Step 1580, Loss 1.2873003482818604\n",
            "Step 1590, Loss 1.9204069375991821\n",
            "Step 1600, Loss 1.928731083869934\n",
            "Step 1610, Loss 1.2946395874023438\n",
            "Step 1620, Loss 0.831703245639801\n",
            "Step 1630, Loss 0.5993133783340454\n",
            "Step 1640, Loss 1.440880298614502\n",
            "Step 1650, Loss 0.9229960441589355\n",
            "Step 1660, Loss 1.3684039115905762\n",
            "Step 1670, Loss 0.6181250810623169\n",
            "Step 1680, Loss 1.440588116645813\n",
            "Step 1690, Loss 1.4016691446304321\n",
            "Step 1700, Loss 1.3089066743850708\n",
            "Step 1710, Loss 1.4579226970672607\n",
            "Step 1720, Loss 0.7232474684715271\n",
            "Step 1730, Loss 0.6984858512878418\n",
            "Step 1740, Loss 1.1237492561340332\n",
            "Step 1750, Loss 1.0655616521835327\n",
            "Step 1760, Loss 1.5124609470367432\n",
            "Step 1770, Loss 0.8015931844711304\n",
            "Step 1780, Loss 0.581131637096405\n",
            "Step 1790, Loss 1.4473105669021606\n",
            "Step 1800, Loss 0.8066179156303406\n",
            "Step 1810, Loss 0.8855399489402771\n",
            "Step 1820, Loss 1.7457249164581299\n",
            "Step 1830, Loss 0.9554991722106934\n",
            "Step 1840, Loss 1.437981128692627\n",
            "Step 1850, Loss 1.6262375116348267\n",
            "Step 1860, Loss 0.8871245980262756\n",
            "Step 1870, Loss 1.2061398029327393\n",
            "Step 1880, Loss 0.4524111747741699\n",
            "Step 1890, Loss 1.0310661792755127\n",
            "Step 1900, Loss 0.5138946771621704\n",
            "Step 1910, Loss 0.8084577918052673\n",
            "Step 1920, Loss 1.664864420890808\n",
            "Step 1930, Loss 1.0669374465942383\n",
            "Step 1940, Loss 1.2076820135116577\n",
            "Step 1950, Loss 2.122537851333618\n",
            "Step 1960, Loss 1.4046680927276611\n",
            "Step 1970, Loss 1.5135900974273682\n",
            "Step 1980, Loss 1.292017936706543\n",
            "Step 1990, Loss 0.1910904049873352\n",
            "Step 2000, Loss 1.0781391859054565\n",
            "Step 2010, Loss 1.9848484992980957\n",
            "Step 2020, Loss 2.0509867668151855\n",
            "Step 2030, Loss 1.4673577547073364\n",
            "Step 2040, Loss 0.8665567636489868\n",
            "Step 2050, Loss 0.5627970695495605\n",
            "Step 2060, Loss 1.869464635848999\n",
            "Step 2070, Loss 0.917434811592102\n",
            "Step 2080, Loss 0.7410252690315247\n",
            "Step 2090, Loss 1.028442144393921\n",
            "Step 2100, Loss 0.605842113494873\n",
            "Step 2110, Loss 1.9593205451965332\n",
            "Step 2120, Loss 1.7688044309616089\n",
            "Step 2130, Loss 0.6561994552612305\n",
            "Step 2140, Loss 1.8790841102600098\n",
            "Step 2150, Loss 0.5231467485427856\n",
            "Step 2160, Loss 0.7914763689041138\n",
            "Step 2170, Loss 0.9995031356811523\n",
            "Step 2180, Loss 1.8519482612609863\n",
            "Step 2190, Loss 0.744261622428894\n",
            "Step 2200, Loss 0.7481839656829834\n",
            "Step 2210, Loss 1.2636048793792725\n",
            "Step 2220, Loss 1.5475077629089355\n",
            "Step 2230, Loss 1.6106339693069458\n",
            "Step 2240, Loss 0.7852760553359985\n",
            "Step 2250, Loss 1.1372841596603394\n",
            "Step 2260, Loss 0.828662097454071\n",
            "Step 2270, Loss 0.588580310344696\n",
            "Step 2280, Loss 1.1671202182769775\n",
            "Step 2290, Loss 1.7209713459014893\n",
            "Step 2300, Loss 0.6736027002334595\n",
            "Step 2310, Loss 1.2047513723373413\n",
            "Step 2320, Loss 0.6787906289100647\n",
            "Step 2330, Loss 0.46118879318237305\n",
            "Step 2340, Loss 1.873117446899414\n",
            "Step 2350, Loss 0.8875192403793335\n",
            "Step 2360, Loss 0.9344725608825684\n",
            "Step 2370, Loss 0.6750882267951965\n",
            "Step 2380, Loss 0.6477870941162109\n",
            "Step 2390, Loss 2.6025335788726807\n",
            "Step 2400, Loss 1.2177777290344238\n",
            "Step 2410, Loss 0.8923309445381165\n",
            "Step 2420, Loss 2.0055830478668213\n",
            "Step 2430, Loss 1.318467140197754\n",
            "Step 2440, Loss 0.9876663684844971\n",
            "Step 2450, Loss 1.6742322444915771\n",
            "Step 2460, Loss 0.8389718532562256\n",
            "Step 2470, Loss 1.2765785455703735\n",
            "Step 2480, Loss 0.3079327940940857\n",
            "Step 2490, Loss 0.6286872029304504\n",
            "Step 2500, Loss 1.4052358865737915\n",
            "Step 2510, Loss 0.8324319124221802\n",
            "Step 2520, Loss 0.659626305103302\n",
            "Step 2530, Loss 2.63171648979187\n",
            "Step 2540, Loss 0.33835741877555847\n",
            "Step 2550, Loss 0.9258748292922974\n",
            "Step 2560, Loss 1.5359883308410645\n",
            "Step 2570, Loss 1.1463885307312012\n",
            "Step 2580, Loss 0.6660279035568237\n",
            "Step 2590, Loss 1.2458945512771606\n",
            "Step 2600, Loss 0.8784915804862976\n",
            "Step 2610, Loss 1.6445574760437012\n",
            "Step 2620, Loss 1.0785695314407349\n",
            "Step 2630, Loss 1.1697068214416504\n",
            "Step 2640, Loss 1.3252098560333252\n",
            "Step 2650, Loss 1.1598742008209229\n",
            "Step 2660, Loss 0.37263381481170654\n",
            "Step 2670, Loss 1.385697603225708\n",
            "Step 2680, Loss 1.1725860834121704\n",
            "Step 2690, Loss 1.2764663696289062\n",
            "Step 2700, Loss 0.46053260564804077\n",
            "Step 2710, Loss 1.1979687213897705\n",
            "Step 2720, Loss 1.2571873664855957\n",
            "Step 2730, Loss 1.1626895666122437\n",
            "Step 2740, Loss 2.1962478160858154\n",
            "Step 2750, Loss 1.2239487171173096\n",
            "Step 2760, Loss 0.9807434678077698\n",
            "Step 2770, Loss 1.0235645771026611\n",
            "Step 2780, Loss 0.7778880000114441\n",
            "Step 2790, Loss 1.288404941558838\n",
            "Step 2800, Loss 0.49244245886802673\n",
            "Step 2810, Loss 1.5833563804626465\n",
            "Step 2820, Loss 0.6156173944473267\n",
            "Step 2830, Loss 0.7045621275901794\n",
            "Step 2840, Loss 0.3186045289039612\n",
            "Step 2850, Loss 1.047538161277771\n",
            "Step 2860, Loss 1.0250205993652344\n",
            "Step 2870, Loss 1.4012901782989502\n",
            "Step 2880, Loss 0.8300336599349976\n",
            "Step 2890, Loss 1.4064185619354248\n",
            "Step 2900, Loss 1.0263539552688599\n",
            "Step 2910, Loss 1.1891705989837646\n",
            "Step 2920, Loss 1.1999051570892334\n",
            "Step 2930, Loss 0.33776751160621643\n",
            "Step 2940, Loss 0.5677549242973328\n",
            "Step 2950, Loss 1.6466299295425415\n",
            "Step 2960, Loss 1.3023061752319336\n",
            "Step 2970, Loss 0.5552122592926025\n",
            "Step 2980, Loss 1.075927734375\n",
            "Step 2990, Loss 0.8534797430038452\n",
            "Step 3000, Loss 1.31484854221344\n",
            "Step 3010, Loss 1.2477374076843262\n",
            "Step 3020, Loss 1.0882213115692139\n",
            "Step 3030, Loss 0.58247971534729\n",
            "Step 3040, Loss 0.7403668761253357\n",
            "Step 3050, Loss 1.4796741008758545\n",
            "Step 3060, Loss 0.7591958045959473\n",
            "Step 3070, Loss 1.1491726636886597\n",
            "Step 3080, Loss 1.1438993215560913\n",
            "Step 3090, Loss 0.5001609921455383\n",
            "Step 3100, Loss 1.4778869152069092\n",
            "Step 3110, Loss 0.17996665835380554\n",
            "Step 3120, Loss 1.283229112625122\n",
            "Step 3130, Loss 1.3372713327407837\n",
            "Step 3140, Loss 1.8202629089355469\n",
            "Step 3150, Loss 0.9152502417564392\n",
            "Step 3160, Loss 0.6562197208404541\n",
            "Step 3170, Loss 0.32598429918289185\n",
            "Step 3180, Loss 1.1977568864822388\n",
            "Step 3190, Loss 0.32333388924598694\n",
            "Step 3200, Loss 0.9954895377159119\n",
            "Step 3210, Loss 1.1718010902404785\n",
            "Step 3220, Loss 0.6231136322021484\n",
            "Step 3230, Loss 0.8753597140312195\n",
            "Step 3240, Loss 0.6719290018081665\n",
            "Step 3250, Loss 1.0522657632827759\n",
            "Step 3260, Loss 0.8475990295410156\n",
            "Step 3270, Loss 0.883220911026001\n",
            "Step 3280, Loss 0.12253543734550476\n",
            "Step 3290, Loss 1.755447268486023\n",
            "Step 3300, Loss 0.815210223197937\n",
            "Step 3310, Loss 0.33237674832344055\n",
            "Step 3320, Loss 0.4172412157058716\n",
            "Step 3330, Loss 0.414945513010025\n",
            "Step 3340, Loss 0.2846752405166626\n",
            "Step 3350, Loss 0.7050451636314392\n",
            "Step 3360, Loss 0.628449559211731\n",
            "Step 3370, Loss 0.565895140171051\n",
            "Step 3380, Loss 1.369429349899292\n",
            "Step 3390, Loss 0.027529895305633545\n",
            "Step 3400, Loss 0.23227563500404358\n",
            "Step 3410, Loss 1.7176730632781982\n",
            "Step 3420, Loss 1.0255411863327026\n",
            "Starting epoch 2/5, LR = [0.001]\n",
            "Step 3430, Loss 0.7208065390586853\n",
            "Step 3440, Loss 0.9416120648384094\n",
            "Step 3450, Loss 0.1844804584980011\n",
            "Step 3460, Loss 0.9579452276229858\n",
            "Step 3470, Loss 0.9829762578010559\n",
            "Step 3480, Loss 0.6688337326049805\n",
            "Step 3490, Loss 0.6106188297271729\n",
            "Step 3500, Loss 0.9979909062385559\n",
            "Step 3510, Loss 0.2840268313884735\n",
            "Step 3520, Loss 0.6984587907791138\n",
            "Step 3530, Loss 0.5970291495323181\n",
            "Step 3540, Loss 0.35142624378204346\n",
            "Step 3550, Loss 0.893791913986206\n",
            "Step 3560, Loss 0.9079165458679199\n",
            "Step 3570, Loss 0.9538102746009827\n",
            "Step 3580, Loss 0.5253376364707947\n",
            "Step 3590, Loss 0.988355815410614\n",
            "Step 3600, Loss 0.5406427979469299\n",
            "Step 3610, Loss 0.48962804675102234\n",
            "Step 3620, Loss 1.0230789184570312\n",
            "Step 3630, Loss 0.7019829154014587\n",
            "Step 3640, Loss 0.13375049829483032\n",
            "Step 3650, Loss 0.91642165184021\n",
            "Step 3660, Loss 0.72100830078125\n",
            "Step 3670, Loss 0.28596848249435425\n",
            "Step 3680, Loss 0.30205923318862915\n",
            "Step 3690, Loss 1.1889322996139526\n",
            "Step 3700, Loss 1.2613084316253662\n",
            "Step 3710, Loss 0.9314736127853394\n",
            "Step 3720, Loss 0.3922151029109955\n",
            "Step 3730, Loss 1.275303602218628\n",
            "Step 3740, Loss 0.8662312030792236\n",
            "Step 3750, Loss 1.236936092376709\n",
            "Step 3760, Loss 0.8008427619934082\n",
            "Step 3770, Loss 0.5201147198677063\n",
            "Step 3780, Loss 1.274881362915039\n",
            "Step 3790, Loss 1.2377227544784546\n",
            "Step 3800, Loss 0.8594475984573364\n",
            "Step 3810, Loss 0.47725507616996765\n",
            "Step 3820, Loss 0.4967832565307617\n",
            "Step 3830, Loss 0.34833014011383057\n",
            "Step 3840, Loss 0.7287404537200928\n",
            "Step 3850, Loss 0.30004435777664185\n",
            "Step 3860, Loss 0.19399189949035645\n",
            "Step 3870, Loss 1.2311838865280151\n",
            "Step 3880, Loss 0.928676426410675\n",
            "Step 3890, Loss 1.0885045528411865\n",
            "Step 3900, Loss 1.5401315689086914\n",
            "Step 3910, Loss 1.0970724821090698\n",
            "Step 3920, Loss 0.7686534523963928\n",
            "Step 3930, Loss 0.6908261179924011\n",
            "Step 3940, Loss 1.506853699684143\n",
            "Step 3950, Loss 1.1698932647705078\n",
            "Step 3960, Loss 0.7620742917060852\n",
            "Step 3970, Loss 0.9414596557617188\n",
            "Step 3980, Loss 1.4805371761322021\n",
            "Step 3990, Loss 0.2659384608268738\n",
            "Step 4000, Loss 0.5068507790565491\n",
            "Step 4010, Loss 1.185240387916565\n",
            "Step 4020, Loss 0.3802785575389862\n",
            "Step 4030, Loss 0.6289963126182556\n",
            "Step 4040, Loss 0.47249582409858704\n",
            "Step 4050, Loss 0.39808887243270874\n",
            "Step 4060, Loss 0.6677983403205872\n",
            "Step 4070, Loss 0.6545130014419556\n",
            "Step 4080, Loss 0.07513707876205444\n",
            "Step 4090, Loss 0.5203141570091248\n",
            "Step 4100, Loss 0.9085053205490112\n",
            "Step 4110, Loss 0.8452534675598145\n",
            "Step 4120, Loss 1.4206467866897583\n",
            "Step 4130, Loss 0.34602129459381104\n",
            "Step 4140, Loss 0.29269713163375854\n",
            "Step 4150, Loss 0.5456691980361938\n",
            "Step 4160, Loss 0.5130587220191956\n",
            "Step 4170, Loss 0.9902557730674744\n",
            "Step 4180, Loss 1.6017581224441528\n",
            "Step 4190, Loss 0.09137749671936035\n",
            "Step 4200, Loss 0.06282025575637817\n",
            "Step 4210, Loss 1.189213514328003\n",
            "Step 4220, Loss 1.287961721420288\n",
            "Step 4230, Loss 1.492148518562317\n",
            "Step 4240, Loss 0.7677579522132874\n",
            "Step 4250, Loss 0.3405163884162903\n",
            "Step 4260, Loss 0.7598894834518433\n",
            "Step 4270, Loss 1.4646360874176025\n",
            "Step 4280, Loss 1.0391883850097656\n",
            "Step 4290, Loss 0.2788875102996826\n",
            "Step 4300, Loss 0.9901185035705566\n",
            "Step 4310, Loss 0.7973209619522095\n",
            "Step 4320, Loss 0.8642145395278931\n",
            "Step 4330, Loss 0.6978499889373779\n",
            "Step 4340, Loss 0.7340572476387024\n",
            "Step 4350, Loss 1.0284621715545654\n",
            "Step 4360, Loss 0.3687739372253418\n",
            "Step 4370, Loss 1.2719835042953491\n",
            "Step 4380, Loss 0.277536541223526\n",
            "Step 4390, Loss 1.0979828834533691\n",
            "Step 4400, Loss 0.29222917556762695\n",
            "Step 4410, Loss 0.9795406460762024\n",
            "Step 4420, Loss 1.1234455108642578\n",
            "Step 4430, Loss 0.8264535665512085\n",
            "Step 4440, Loss 0.5892221927642822\n",
            "Step 4450, Loss 0.17696410417556763\n",
            "Step 4460, Loss 1.0049736499786377\n",
            "Step 4470, Loss 0.2761668860912323\n",
            "Step 4480, Loss 0.8945488929748535\n",
            "Step 4490, Loss 0.4466843605041504\n",
            "Step 4500, Loss 0.43145671486854553\n",
            "Step 4510, Loss 1.2873882055282593\n",
            "Step 4520, Loss 0.5826722383499146\n",
            "Step 4530, Loss 0.5319647789001465\n",
            "Step 4540, Loss 0.3559938073158264\n",
            "Step 4550, Loss 0.7219592332839966\n",
            "Step 4560, Loss 0.5059303045272827\n",
            "Step 4570, Loss 0.854451060295105\n",
            "Step 4580, Loss 0.3487440347671509\n",
            "Step 4590, Loss 0.5260714292526245\n",
            "Step 4600, Loss 0.44458603858947754\n",
            "Step 4610, Loss 0.5267732739448547\n",
            "Step 4620, Loss 1.2063528299331665\n",
            "Step 4630, Loss 1.0021610260009766\n",
            "Step 4640, Loss 0.49151256680488586\n",
            "Step 4650, Loss 0.5379970669746399\n",
            "Step 4660, Loss 0.45664626359939575\n",
            "Step 4670, Loss 0.07777130603790283\n",
            "Step 4680, Loss 0.8094533681869507\n",
            "Step 4690, Loss 0.8538791537284851\n",
            "Step 4700, Loss 1.000720500946045\n",
            "Step 4710, Loss 0.6058998107910156\n",
            "Step 4720, Loss 0.35702529549598694\n",
            "Step 4730, Loss 0.44202008843421936\n",
            "Step 4740, Loss 1.0338997840881348\n",
            "Step 4750, Loss 0.27121418714523315\n",
            "Step 4760, Loss 0.8848735094070435\n",
            "Step 4770, Loss 0.9038878679275513\n",
            "Step 4780, Loss 0.48969367146492004\n",
            "Step 4790, Loss 0.20436260104179382\n",
            "Step 4800, Loss 0.9300086498260498\n",
            "Step 4810, Loss 0.39120644330978394\n",
            "Step 4820, Loss 1.0954499244689941\n",
            "Step 4830, Loss 0.832381010055542\n",
            "Step 4840, Loss 1.0538525581359863\n",
            "Step 4850, Loss 0.689957857131958\n",
            "Step 4860, Loss 0.7261598110198975\n",
            "Step 4870, Loss 0.6137415766716003\n",
            "Step 4880, Loss 0.7492074370384216\n",
            "Step 4890, Loss 0.3032624423503876\n",
            "Step 4900, Loss 0.4808313846588135\n",
            "Step 4910, Loss 0.39955776929855347\n",
            "Step 4920, Loss 0.07701408863067627\n",
            "Step 4930, Loss 0.361163467168808\n",
            "Step 4940, Loss 0.4278230369091034\n",
            "Step 4950, Loss 0.49750077724456787\n",
            "Step 4960, Loss 0.7980294823646545\n",
            "Step 4970, Loss 1.407768726348877\n",
            "Step 4980, Loss 0.5115659832954407\n",
            "Step 4990, Loss 0.3445388972759247\n",
            "Step 5000, Loss 0.7004289627075195\n",
            "Step 5010, Loss 0.7514029741287231\n",
            "Step 5020, Loss 0.8840641975402832\n",
            "Step 5030, Loss 0.515409529209137\n",
            "Step 5040, Loss 0.2815798819065094\n",
            "Step 5050, Loss 0.7444779872894287\n",
            "Step 5060, Loss 0.5823547840118408\n",
            "Step 5070, Loss 0.21065402030944824\n",
            "Step 5080, Loss 0.35320019721984863\n",
            "Step 5090, Loss 0.29911649227142334\n",
            "Step 5100, Loss 0.3379438519477844\n",
            "Step 5110, Loss 0.6811443567276001\n",
            "Step 5120, Loss 0.8101558685302734\n",
            "Step 5130, Loss 0.9606519937515259\n",
            "Step 5140, Loss 0.6146866083145142\n",
            "Step 5150, Loss 0.3716910183429718\n",
            "Step 5160, Loss 0.4777216911315918\n",
            "Step 5170, Loss 1.255133032798767\n",
            "Step 5180, Loss 1.230765700340271\n",
            "Step 5190, Loss 0.3794870376586914\n",
            "Step 5200, Loss 0.9288071393966675\n",
            "Step 5210, Loss 0.5696418285369873\n",
            "Step 5220, Loss 0.13626152276992798\n",
            "Step 5230, Loss 0.9151451587677002\n",
            "Step 5240, Loss 0.09120845794677734\n",
            "Step 5250, Loss 0.5196141600608826\n",
            "Step 5260, Loss 0.7921472191810608\n",
            "Step 5270, Loss 0.910723865032196\n",
            "Step 5280, Loss 0.8261949419975281\n",
            "Step 5290, Loss 0.9763407707214355\n",
            "Step 5300, Loss 0.48918265104293823\n",
            "Step 5310, Loss 0.5091753005981445\n",
            "Step 5320, Loss 0.6557179093360901\n",
            "Step 5330, Loss 0.5248957872390747\n",
            "Step 5340, Loss 0.3347453474998474\n",
            "Step 5350, Loss 0.9367270469665527\n",
            "Step 5360, Loss 0.5588667392730713\n",
            "Step 5370, Loss 0.49852365255355835\n",
            "Step 5380, Loss 0.9608200788497925\n",
            "Step 5390, Loss 0.7142156362533569\n",
            "Step 5400, Loss 0.7260445356369019\n",
            "Step 5410, Loss 0.6974393129348755\n",
            "Step 5420, Loss 0.6682310700416565\n",
            "Step 5430, Loss 0.2541673481464386\n",
            "Step 5440, Loss 0.7605144381523132\n",
            "Step 5450, Loss 0.27932822704315186\n",
            "Step 5460, Loss 0.30892711877822876\n",
            "Step 5470, Loss 0.8751620650291443\n",
            "Step 5480, Loss 0.34487518668174744\n",
            "Step 5490, Loss 0.292057603597641\n",
            "Step 5500, Loss 0.6669350862503052\n",
            "Step 5510, Loss 0.8366565704345703\n",
            "Step 5520, Loss 1.856757402420044\n",
            "Step 5530, Loss 0.5573388338088989\n",
            "Step 5540, Loss 1.2143607139587402\n",
            "Step 5550, Loss 0.7714079022407532\n",
            "Step 5560, Loss 0.8684815168380737\n",
            "Step 5570, Loss 0.4455990195274353\n",
            "Step 5580, Loss 1.1255016326904297\n",
            "Step 5590, Loss 0.5368088483810425\n",
            "Step 5600, Loss 0.8725042939186096\n",
            "Step 5610, Loss 0.4038558602333069\n",
            "Step 5620, Loss 1.0872855186462402\n",
            "Step 5630, Loss 0.756607174873352\n",
            "Step 5640, Loss 0.859129786491394\n",
            "Step 5650, Loss 0.8427882790565491\n",
            "Step 5660, Loss 0.9970922470092773\n",
            "Step 5670, Loss 0.9805810451507568\n",
            "Step 5680, Loss 0.8998819589614868\n",
            "Step 5690, Loss 0.5296517014503479\n",
            "Step 5700, Loss 0.7862198948860168\n",
            "Step 5710, Loss 0.8964444994926453\n",
            "Step 5720, Loss 0.4921191930770874\n",
            "Step 5730, Loss 1.1484935283660889\n",
            "Step 5740, Loss 0.5708751678466797\n",
            "Step 5750, Loss 0.8627234697341919\n",
            "Step 5760, Loss 0.3152037560939789\n",
            "Step 5770, Loss 1.214440941810608\n",
            "Step 5780, Loss 0.6886038780212402\n",
            "Step 5790, Loss 0.7910932302474976\n",
            "Step 5800, Loss 0.39028990268707275\n",
            "Step 5810, Loss 0.4072905480861664\n",
            "Step 5820, Loss 1.642343521118164\n",
            "Step 5830, Loss 0.7545948028564453\n",
            "Step 5840, Loss 1.3584034442901611\n",
            "Step 5850, Loss 0.07683795690536499\n",
            "Step 5860, Loss 0.41252315044403076\n",
            "Step 5870, Loss 0.8052263259887695\n",
            "Step 5880, Loss 0.9941319227218628\n",
            "Step 5890, Loss 0.3834402561187744\n",
            "Step 5900, Loss 0.4573211669921875\n",
            "Step 5910, Loss 0.38608473539352417\n",
            "Step 5920, Loss 0.9458168745040894\n",
            "Step 5930, Loss 1.175281047821045\n",
            "Step 5940, Loss 0.20808979868888855\n",
            "Step 5950, Loss 0.5996087789535522\n",
            "Step 5960, Loss 1.5382457971572876\n",
            "Step 5970, Loss 1.1934295892715454\n",
            "Step 5980, Loss 0.5528901815414429\n",
            "Step 5990, Loss 0.26475289463996887\n",
            "Step 6000, Loss 0.6679553985595703\n",
            "Step 6010, Loss 0.7099077701568604\n",
            "Step 6020, Loss 0.4097135663032532\n",
            "Step 6030, Loss 1.0445743799209595\n",
            "Step 6040, Loss 0.5455450415611267\n",
            "Step 6050, Loss 0.32172054052352905\n",
            "Step 6060, Loss 0.6618878841400146\n",
            "Step 6070, Loss 0.6073397397994995\n",
            "Step 6080, Loss 0.22097128629684448\n",
            "Step 6090, Loss 0.4006344676017761\n",
            "Step 6100, Loss 0.5090987682342529\n",
            "Step 6110, Loss 1.0142621994018555\n",
            "Step 6120, Loss 1.3773019313812256\n",
            "Step 6130, Loss 0.27869901061058044\n",
            "Step 6140, Loss 0.7779964208602905\n",
            "Step 6150, Loss 0.7229872941970825\n",
            "Step 6160, Loss 0.45161810517311096\n",
            "Step 6170, Loss 0.7256023287773132\n",
            "Step 6180, Loss 0.3157147765159607\n",
            "Step 6190, Loss 0.39246058464050293\n",
            "Step 6200, Loss 1.4221240282058716\n",
            "Step 6210, Loss 0.24858789145946503\n",
            "Step 6220, Loss 0.9931460618972778\n",
            "Step 6230, Loss 0.715316116809845\n",
            "Step 6240, Loss 0.19444647431373596\n",
            "Step 6250, Loss 0.053234249353408813\n",
            "Step 6260, Loss 0.41592520475387573\n",
            "Step 6270, Loss 0.24750439822673798\n",
            "Step 6280, Loss 1.0411829948425293\n",
            "Step 6290, Loss 1.006469964981079\n",
            "Step 6300, Loss 0.6578437089920044\n",
            "Step 6310, Loss 0.2834780514240265\n",
            "Step 6320, Loss 1.0281163454055786\n",
            "Step 6330, Loss 0.8507781624794006\n",
            "Step 6340, Loss 0.6347715854644775\n",
            "Step 6350, Loss 0.5811281204223633\n",
            "Step 6360, Loss 1.1153011322021484\n",
            "Step 6370, Loss 1.111626386642456\n",
            "Step 6380, Loss 0.5462856292724609\n",
            "Step 6390, Loss 0.26120150089263916\n",
            "Step 6400, Loss 0.6540899276733398\n",
            "Step 6410, Loss 0.5329053401947021\n",
            "Step 6420, Loss 0.20986925065517426\n",
            "Step 6430, Loss 0.4956483542919159\n",
            "Step 6440, Loss 1.5056384801864624\n",
            "Step 6450, Loss 0.4828197658061981\n",
            "Step 6460, Loss 0.8075133562088013\n",
            "Step 6470, Loss 0.571095883846283\n",
            "Step 6480, Loss 1.1369386911392212\n",
            "Step 6490, Loss 0.5592236518859863\n",
            "Step 6500, Loss 0.12197250127792358\n",
            "Step 6510, Loss 1.3075320720672607\n",
            "Step 6520, Loss 0.5748549699783325\n",
            "Step 6530, Loss 1.0012235641479492\n",
            "Step 6540, Loss 0.7430230379104614\n",
            "Step 6550, Loss 0.5690724849700928\n",
            "Step 6560, Loss 0.7971863746643066\n",
            "Step 6570, Loss 0.07589611411094666\n",
            "Step 6580, Loss 0.6150625348091125\n",
            "Step 6590, Loss 1.1247401237487793\n",
            "Step 6600, Loss 0.6189575791358948\n",
            "Step 6610, Loss 0.2995317876338959\n",
            "Step 6620, Loss 0.3696076273918152\n",
            "Step 6630, Loss 0.412262886762619\n",
            "Step 6640, Loss 0.7832760810852051\n",
            "Step 6650, Loss 0.9511985182762146\n",
            "Step 6660, Loss 0.20165064930915833\n",
            "Step 6670, Loss 0.49372628331184387\n",
            "Step 6680, Loss 1.0508581399917603\n",
            "Step 6690, Loss 0.6635284423828125\n",
            "Step 6700, Loss 0.2820434272289276\n",
            "Step 6710, Loss 0.8012657761573792\n",
            "Step 6720, Loss 0.17290833592414856\n",
            "Step 6730, Loss 1.137253999710083\n",
            "Step 6740, Loss 0.5586720705032349\n",
            "Step 6750, Loss 1.0275237560272217\n",
            "Step 6760, Loss 0.30233654379844666\n",
            "Step 6770, Loss 1.308546543121338\n",
            "Step 6780, Loss 0.44898152351379395\n",
            "Step 6790, Loss 0.5182806849479675\n",
            "Step 6800, Loss 0.6158449053764343\n",
            "Step 6810, Loss 0.29079338908195496\n",
            "Step 6820, Loss 0.5213205814361572\n",
            "Step 6830, Loss 0.6121295690536499\n",
            "Step 6840, Loss 0.06920242309570312\n",
            "Starting epoch 3/5, LR = [0.001]\n",
            "Step 6850, Loss 0.3001381754875183\n",
            "Step 6860, Loss 0.08234810829162598\n",
            "Step 6870, Loss 0.16429278254508972\n",
            "Step 6880, Loss 0.36686378717422485\n",
            "Step 6890, Loss 0.29194551706314087\n",
            "Step 6900, Loss 0.7961780428886414\n",
            "Step 6910, Loss 1.0940828323364258\n",
            "Step 6920, Loss 0.15031597018241882\n",
            "Step 6930, Loss 0.5587597489356995\n",
            "Step 6940, Loss 0.7215073108673096\n",
            "Step 6950, Loss 0.4920962154865265\n",
            "Step 6960, Loss 0.1123485267162323\n",
            "Step 6970, Loss 0.29101523756980896\n",
            "Step 6980, Loss 0.5849089622497559\n",
            "Step 6990, Loss 0.4256872534751892\n",
            "Step 7000, Loss 0.062406718730926514\n",
            "Step 7010, Loss 0.49825242161750793\n",
            "Step 7020, Loss 0.05686601996421814\n",
            "Step 7030, Loss 0.4145234227180481\n",
            "Step 7040, Loss 0.16679531335830688\n",
            "Step 7050, Loss 0.35420870780944824\n",
            "Step 7060, Loss 0.19631290435791016\n",
            "Step 7070, Loss 0.5111838579177856\n",
            "Step 7080, Loss 0.00766986608505249\n",
            "Step 7090, Loss 1.3340810537338257\n",
            "Step 7100, Loss 0.10755574703216553\n",
            "Step 7110, Loss 0.3091709017753601\n",
            "Step 7120, Loss 0.18074172735214233\n",
            "Step 7130, Loss 0.5085013508796692\n",
            "Step 7140, Loss 0.19167745113372803\n",
            "Step 7150, Loss 0.1793648898601532\n",
            "Step 7160, Loss 0.10847330093383789\n",
            "Step 7170, Loss 0.2231549322605133\n",
            "Step 7180, Loss 0.540192186832428\n",
            "Step 7190, Loss 0.8816072940826416\n",
            "Step 7200, Loss 0.525780439376831\n",
            "Step 7210, Loss 0.16065660119056702\n",
            "Step 7220, Loss 0.2635515332221985\n",
            "Step 7230, Loss 0.627022385597229\n",
            "Step 7240, Loss 0.033502817153930664\n",
            "Step 7250, Loss 0.39401116967201233\n",
            "Step 7260, Loss 0.4229942560195923\n",
            "Step 7270, Loss 0.46639901399612427\n",
            "Step 7280, Loss 0.5228666663169861\n",
            "Step 7290, Loss 0.31359168887138367\n",
            "Step 7300, Loss 0.4844437837600708\n",
            "Step 7310, Loss 0.3716995120048523\n",
            "Step 7320, Loss 0.14029836654663086\n",
            "Step 7330, Loss 0.09129318594932556\n",
            "Step 7340, Loss 0.19667476415634155\n",
            "Step 7350, Loss 0.26885294914245605\n",
            "Step 7360, Loss 0.501968264579773\n",
            "Step 7370, Loss 0.3292821943759918\n",
            "Step 7380, Loss 0.5687884092330933\n",
            "Step 7390, Loss 0.5520319938659668\n",
            "Step 7400, Loss 1.0303949117660522\n",
            "Step 7410, Loss 0.3443736135959625\n",
            "Step 7420, Loss 0.8783078789710999\n",
            "Step 7430, Loss 0.8150501251220703\n",
            "Step 7440, Loss 0.1584232747554779\n",
            "Step 7450, Loss 0.5377755761146545\n",
            "Step 7460, Loss 0.23069122433662415\n",
            "Step 7470, Loss 0.738118052482605\n",
            "Step 7480, Loss 0.42668741941452026\n",
            "Step 7490, Loss 0.47439053654670715\n",
            "Step 7500, Loss 0.2972697615623474\n",
            "Step 7510, Loss 0.2981095314025879\n",
            "Step 7520, Loss 0.13799697160720825\n",
            "Step 7530, Loss 0.48770883679389954\n",
            "Step 7540, Loss 0.30567803978919983\n",
            "Step 7550, Loss 0.9941895008087158\n",
            "Step 7560, Loss 0.5709350109100342\n",
            "Step 7570, Loss 0.4139590859413147\n",
            "Step 7580, Loss 0.07520711421966553\n",
            "Step 7590, Loss 0.2622886300086975\n",
            "Step 7600, Loss 0.3074515461921692\n",
            "Step 7610, Loss 0.5880163311958313\n",
            "Step 7620, Loss 1.1514508724212646\n",
            "Step 7630, Loss 0.0034113526344299316\n",
            "Step 7640, Loss 0.31484362483024597\n",
            "Step 7650, Loss 0.6041123270988464\n",
            "Step 7660, Loss 0.4361844062805176\n",
            "Step 7670, Loss 0.2515171468257904\n",
            "Step 7680, Loss 0.23923543095588684\n",
            "Step 7690, Loss 0.9715511202812195\n",
            "Step 7700, Loss 0.19242769479751587\n",
            "Step 7710, Loss 1.0587832927703857\n",
            "Step 7720, Loss 0.20830464363098145\n",
            "Step 7730, Loss 0.37462419271469116\n",
            "Step 7740, Loss 0.6357818841934204\n",
            "Step 7750, Loss 0.42555034160614014\n",
            "Step 7760, Loss 0.4698360562324524\n",
            "Step 7770, Loss 0.17480215430259705\n",
            "Step 7780, Loss 0.6392357349395752\n",
            "Step 7790, Loss 0.6158689856529236\n",
            "Step 7800, Loss 0.37515774369239807\n",
            "Step 7810, Loss 0.9874294996261597\n",
            "Step 7820, Loss 0.5219702124595642\n",
            "Step 7830, Loss 0.5663471221923828\n",
            "Step 7840, Loss 0.11594599485397339\n",
            "Step 7850, Loss 0.22742971777915955\n",
            "Step 7860, Loss 0.04726657271385193\n",
            "Step 7870, Loss 0.7716541290283203\n",
            "Step 7880, Loss 0.12339365482330322\n",
            "Step 7890, Loss 0.20670118927955627\n",
            "Step 7900, Loss 0.436659038066864\n",
            "Step 7910, Loss 0.31792110204696655\n",
            "Step 7920, Loss 0.8781951665878296\n",
            "Step 7930, Loss 0.40518108010292053\n",
            "Step 7940, Loss 0.3943750858306885\n",
            "Step 7950, Loss 0.3113732933998108\n",
            "Step 7960, Loss 0.5390762090682983\n",
            "Step 7970, Loss 0.005548954010009766\n",
            "Step 7980, Loss 0.13931560516357422\n",
            "Step 7990, Loss 0.8165527582168579\n",
            "Step 8000, Loss 0.4502193331718445\n",
            "Step 8010, Loss 0.5946674942970276\n",
            "Step 8020, Loss 0.2664696276187897\n",
            "Step 8030, Loss 0.21362033486366272\n",
            "Step 8040, Loss 0.42613428831100464\n",
            "Step 8050, Loss 0.3191390335559845\n",
            "Step 8060, Loss 0.39955589175224304\n",
            "Step 8070, Loss 0.6330235600471497\n",
            "Step 8080, Loss 0.8139636516571045\n",
            "Step 8090, Loss 0.23622918128967285\n",
            "Step 8100, Loss 0.8375586271286011\n",
            "Step 8110, Loss 0.5369459390640259\n",
            "Step 8120, Loss 0.1710854172706604\n",
            "Step 8130, Loss 0.7768168449401855\n",
            "Step 8140, Loss 0.5579462051391602\n",
            "Step 8150, Loss 0.39742425084114075\n",
            "Step 8160, Loss 0.17825710773468018\n",
            "Step 8170, Loss 0.23166248202323914\n",
            "Step 8180, Loss 0.3148680627346039\n",
            "Step 8190, Loss 0.3109513819217682\n",
            "Step 8200, Loss 0.5592484474182129\n",
            "Step 8210, Loss 0.23320645093917847\n",
            "Step 8220, Loss 0.7574725151062012\n",
            "Step 8230, Loss 0.45550429821014404\n",
            "Step 8240, Loss 0.8367145657539368\n",
            "Step 8250, Loss 1.1893184185028076\n",
            "Step 8260, Loss 0.9354951977729797\n",
            "Step 8270, Loss 0.40807822346687317\n",
            "Step 8280, Loss 0.4475117325782776\n",
            "Step 8290, Loss 0.94028639793396\n",
            "Step 8300, Loss 0.6322615146636963\n",
            "Step 8310, Loss 0.9549530744552612\n",
            "Step 8320, Loss 0.3491996228694916\n",
            "Step 8330, Loss 1.2409331798553467\n",
            "Step 8340, Loss 0.22371244430541992\n",
            "Step 8350, Loss 0.08525145053863525\n",
            "Step 8360, Loss 0.36537864804267883\n",
            "Step 8370, Loss 0.5773103833198547\n",
            "Step 8380, Loss 0.8431339859962463\n",
            "Step 8390, Loss 0.8006141185760498\n",
            "Step 8400, Loss 0.13778328895568848\n",
            "Step 8410, Loss 0.7071342468261719\n",
            "Step 8420, Loss 0.1870635449886322\n",
            "Step 8430, Loss 0.47077032923698425\n",
            "Step 8440, Loss 0.14758425951004028\n",
            "Step 8450, Loss 0.4541737735271454\n",
            "Step 8460, Loss 0.23452000319957733\n",
            "Step 8470, Loss 0.5566185116767883\n",
            "Step 8480, Loss 0.3623920977115631\n",
            "Step 8490, Loss 0.6739043593406677\n",
            "Step 8500, Loss 0.30146199464797974\n",
            "Step 8510, Loss 0.887675404548645\n",
            "Step 8520, Loss 0.5153032541275024\n",
            "Step 8530, Loss 0.6817648410797119\n",
            "Step 8540, Loss 1.6029891967773438\n",
            "Step 8550, Loss 0.903779149055481\n",
            "Step 8560, Loss 0.12581923604011536\n",
            "Step 8570, Loss 0.5617530345916748\n",
            "Step 8580, Loss 0.5732695460319519\n",
            "Step 8590, Loss 0.9577234983444214\n",
            "Step 8600, Loss 0.6139628887176514\n",
            "Step 8610, Loss 0.34164249897003174\n",
            "Step 8620, Loss 0.6471240520477295\n",
            "Step 8630, Loss 0.03341472148895264\n",
            "Step 8640, Loss 0.4959600865840912\n",
            "Step 8650, Loss 0.299886018037796\n",
            "Step 8660, Loss 0.32980912923812866\n",
            "Step 8670, Loss 0.5345755815505981\n",
            "Step 8680, Loss 0.6562210321426392\n",
            "Step 8690, Loss 0.7426976561546326\n",
            "Step 8700, Loss 0.869677722454071\n",
            "Step 8710, Loss 0.4618934690952301\n",
            "Step 8720, Loss 0.6177436113357544\n",
            "Step 8730, Loss 0.2647966146469116\n",
            "Step 8740, Loss 0.6582439541816711\n",
            "Step 8750, Loss 0.2869497835636139\n",
            "Step 8760, Loss 1.0881855487823486\n",
            "Step 8770, Loss 0.5938021540641785\n",
            "Step 8780, Loss 0.3917413055896759\n",
            "Step 8790, Loss 0.2706306278705597\n",
            "Step 8800, Loss 0.5312965512275696\n",
            "Step 8810, Loss 0.6894934177398682\n",
            "Step 8820, Loss 0.6030004024505615\n",
            "Step 8830, Loss 0.5981773138046265\n",
            "Step 8840, Loss 0.282380074262619\n",
            "Step 8850, Loss 1.1638208627700806\n",
            "Step 8860, Loss 0.3859177231788635\n",
            "Step 8870, Loss 0.12816840410232544\n",
            "Step 8880, Loss 0.49396222829818726\n",
            "Step 8890, Loss 0.1855098009109497\n",
            "Step 8900, Loss 1.048218011856079\n",
            "Step 8910, Loss 1.114815354347229\n",
            "Step 8920, Loss 0.9762169718742371\n",
            "Step 8930, Loss 0.5093246102333069\n",
            "Step 8940, Loss 0.6123877763748169\n",
            "Step 8950, Loss 1.2342815399169922\n",
            "Step 8960, Loss 0.5303613543510437\n",
            "Step 8970, Loss 0.5487396717071533\n",
            "Step 8980, Loss 0.835663378238678\n",
            "Step 8990, Loss 0.36511269211769104\n",
            "Step 9000, Loss 0.9074970483779907\n",
            "Step 9010, Loss 0.8305268287658691\n",
            "Step 9020, Loss 0.837280809879303\n",
            "Step 9030, Loss 0.030491650104522705\n",
            "Step 9040, Loss 0.22139883041381836\n",
            "Step 9050, Loss 0.4414680600166321\n",
            "Step 9060, Loss 0.7697491645812988\n",
            "Step 9070, Loss 0.542633593082428\n",
            "Step 9080, Loss 0.36044201254844666\n",
            "Step 9090, Loss 0.5865333080291748\n",
            "Step 9100, Loss 0.36173635721206665\n",
            "Step 9110, Loss 0.3686355650424957\n",
            "Step 9120, Loss 0.8894740343093872\n",
            "Step 9130, Loss 0.7193509340286255\n",
            "Step 9140, Loss 1.2717599868774414\n",
            "Step 9150, Loss 0.391954630613327\n",
            "Step 9160, Loss 0.6049768328666687\n",
            "Step 9170, Loss 0.3486422598361969\n",
            "Step 9180, Loss 0.38167354464530945\n",
            "Step 9190, Loss 0.15200267732143402\n",
            "Step 9200, Loss 0.22106024622917175\n",
            "Step 9210, Loss 0.8984464406967163\n",
            "Step 9220, Loss 0.13398060202598572\n",
            "Step 9230, Loss 0.3277151584625244\n",
            "Step 9240, Loss 0.633955180644989\n",
            "Step 9250, Loss 0.4505491554737091\n",
            "Step 9260, Loss 0.6418098211288452\n",
            "Step 9270, Loss 0.43437913060188293\n",
            "Step 9280, Loss 0.9558195471763611\n",
            "Step 9290, Loss 0.8645703792572021\n",
            "Step 9300, Loss 0.9128583073616028\n",
            "Step 9310, Loss 0.9926486015319824\n",
            "Step 9320, Loss 0.5286019444465637\n",
            "Step 9330, Loss 0.8769018650054932\n",
            "Step 9340, Loss 0.8220054507255554\n",
            "Step 9350, Loss 0.3836711347103119\n",
            "Step 9360, Loss 0.8308026194572449\n",
            "Step 9370, Loss 0.8255337476730347\n",
            "Step 9380, Loss 0.07598304748535156\n",
            "Step 9390, Loss 1.420244574546814\n",
            "Step 9400, Loss 1.0143927335739136\n",
            "Step 9410, Loss 0.16296792030334473\n",
            "Step 9420, Loss 0.3341255784034729\n",
            "Step 9430, Loss 0.10616642236709595\n",
            "Step 9440, Loss 0.9465906620025635\n",
            "Step 9450, Loss 0.12496709823608398\n",
            "Step 9460, Loss 0.34330588579177856\n",
            "Step 9470, Loss 0.08307251334190369\n",
            "Step 9480, Loss 0.5245463848114014\n",
            "Step 9490, Loss 0.47914746403694153\n",
            "Step 9500, Loss 0.16250082850456238\n",
            "Step 9510, Loss 0.15573188662528992\n",
            "Step 9520, Loss 0.25387629866600037\n",
            "Step 9530, Loss 0.6609730124473572\n",
            "Step 9540, Loss 0.9768154621124268\n",
            "Step 9550, Loss 0.175514817237854\n",
            "Step 9560, Loss 0.17922034859657288\n",
            "Step 9570, Loss 0.765184760093689\n",
            "Step 9580, Loss 0.4530008137226105\n",
            "Step 9590, Loss 0.497883141040802\n",
            "Step 9600, Loss 0.5046592354774475\n",
            "Step 9610, Loss 1.347130537033081\n",
            "Step 9620, Loss 1.1147762537002563\n",
            "Step 9630, Loss 0.2775053381919861\n",
            "Step 9640, Loss 0.2692205011844635\n",
            "Step 9650, Loss 0.651117742061615\n",
            "Step 9660, Loss 0.5382702946662903\n",
            "Step 9670, Loss 0.5748004913330078\n",
            "Step 9680, Loss 0.05948108434677124\n",
            "Step 9690, Loss 0.1870216727256775\n",
            "Step 9700, Loss 1.0342130661010742\n",
            "Step 9710, Loss 0.747390627861023\n",
            "Step 9720, Loss 0.5581532120704651\n",
            "Step 9730, Loss 0.13981500267982483\n",
            "Step 9740, Loss 0.44538557529449463\n",
            "Step 9750, Loss 0.5848249197006226\n",
            "Step 9760, Loss 0.9229179620742798\n",
            "Step 9770, Loss 0.07064428925514221\n",
            "Step 9780, Loss 0.5450255870819092\n",
            "Step 9790, Loss 0.41521164774894714\n",
            "Step 9800, Loss 0.7980753183364868\n",
            "Step 9810, Loss 0.09924742579460144\n",
            "Step 9820, Loss 0.5457054376602173\n",
            "Step 9830, Loss 0.8819849491119385\n",
            "Step 9840, Loss 0.544114351272583\n",
            "Step 9850, Loss 0.4045691192150116\n",
            "Step 9860, Loss 0.268149197101593\n",
            "Step 9870, Loss 1.1507920026779175\n",
            "Step 9880, Loss 0.8955762386322021\n",
            "Step 9890, Loss 0.1338515281677246\n",
            "Step 9900, Loss 0.44995760917663574\n",
            "Step 9910, Loss 1.1735283136367798\n",
            "Step 9920, Loss 1.0688642263412476\n",
            "Step 9930, Loss 0.2910407483577728\n",
            "Step 9940, Loss 0.5222145318984985\n",
            "Step 9950, Loss 0.19526027143001556\n",
            "Step 9960, Loss 0.6562603712081909\n",
            "Step 9970, Loss 0.18923857808113098\n",
            "Step 9980, Loss 0.5524027943611145\n",
            "Step 9990, Loss 0.4959277808666229\n",
            "Step 10000, Loss 0.18092158436775208\n",
            "Step 10010, Loss 0.5014716386795044\n",
            "Step 10020, Loss 0.5923975110054016\n",
            "Step 10030, Loss 0.3797977566719055\n",
            "Step 10040, Loss 0.3090696632862091\n",
            "Step 10050, Loss 0.3133607506752014\n",
            "Step 10060, Loss 0.5892125368118286\n",
            "Step 10070, Loss 0.5062790513038635\n",
            "Step 10080, Loss 0.21114081144332886\n",
            "Step 10090, Loss 0.806060791015625\n",
            "Step 10100, Loss 0.359107106924057\n",
            "Step 10110, Loss 0.2514954209327698\n",
            "Step 10120, Loss 0.2494395673274994\n",
            "Step 10130, Loss 0.14325550198554993\n",
            "Step 10140, Loss 0.31020572781562805\n",
            "Step 10150, Loss 1.819227933883667\n",
            "Step 10160, Loss 0.12312811613082886\n",
            "Step 10170, Loss 0.7620015144348145\n",
            "Step 10180, Loss 0.5542398691177368\n",
            "Step 10190, Loss 0.6833038330078125\n",
            "Step 10200, Loss 0.49171775579452515\n",
            "Step 10210, Loss 0.11179009079933167\n",
            "Step 10220, Loss 0.21187980473041534\n",
            "Step 10230, Loss 0.4038761258125305\n",
            "Step 10240, Loss 0.8022511601448059\n",
            "Step 10250, Loss 0.09035667777061462\n",
            "Step 10260, Loss 0.05197259783744812\n",
            "Step 10270, Loss 0.43514057993888855\n",
            "Starting epoch 4/5, LR = [0.001]\n",
            "Step 10280, Loss 0.6121272444725037\n",
            "Step 10290, Loss 0.3765679597854614\n",
            "Step 10300, Loss 0.13351020216941833\n",
            "Step 10310, Loss 0.42530930042266846\n",
            "Step 10320, Loss 0.2945249676704407\n",
            "Step 10330, Loss 0.3170306384563446\n",
            "Step 10340, Loss 0.5708479285240173\n",
            "Step 10350, Loss 0.12950429320335388\n",
            "Step 10360, Loss 0.22097384929656982\n",
            "Step 10370, Loss 0.5087549090385437\n",
            "Step 10380, Loss 0.502689003944397\n",
            "Step 10390, Loss 0.5574679374694824\n",
            "Step 10400, Loss 0.3944551646709442\n",
            "Step 10410, Loss 0.18732744455337524\n",
            "Step 10420, Loss 0.6101057529449463\n",
            "Step 10430, Loss 0.24997252225875854\n",
            "Step 10440, Loss 0.23021292686462402\n",
            "Step 10450, Loss 0.2847624719142914\n",
            "Step 10460, Loss 0.25963684916496277\n",
            "Step 10470, Loss 0.11574044823646545\n",
            "Step 10480, Loss 0.019728362560272217\n",
            "Step 10490, Loss 0.3365097939968109\n",
            "Step 10500, Loss 0.2262144684791565\n",
            "Step 10510, Loss 0.12230280041694641\n",
            "Step 10520, Loss 0.3631928265094757\n",
            "Step 10530, Loss 0.5616921782493591\n",
            "Step 10540, Loss 0.4666021168231964\n",
            "Step 10550, Loss 1.102622151374817\n",
            "Step 10560, Loss 0.3988732397556305\n",
            "Step 10570, Loss 0.6772469282150269\n",
            "Step 10580, Loss 0.15834084153175354\n",
            "Step 10590, Loss 0.2675829529762268\n",
            "Step 10600, Loss 0.4484252631664276\n",
            "Step 10610, Loss 0.2945619523525238\n",
            "Step 10620, Loss 0.2845611274242401\n",
            "Step 10630, Loss 0.09261611104011536\n",
            "Step 10640, Loss 0.2527817487716675\n",
            "Step 10650, Loss 0.20596075057983398\n",
            "Step 10660, Loss 0.5318527221679688\n",
            "Step 10670, Loss 0.35884860157966614\n",
            "Step 10680, Loss 0.06333094835281372\n",
            "Step 10690, Loss 0.22684705257415771\n",
            "Step 10700, Loss 0.39256608486175537\n",
            "Step 10710, Loss 0.3626590371131897\n",
            "Step 10720, Loss 0.19656139612197876\n",
            "Step 10730, Loss 0.03443834185600281\n",
            "Step 10740, Loss 0.1827968955039978\n",
            "Step 10750, Loss 0.32510945200920105\n",
            "Step 10760, Loss 0.5445413589477539\n",
            "Step 10770, Loss 0.05566081404685974\n",
            "Step 10780, Loss 0.48276105523109436\n",
            "Step 10790, Loss 0.2358885109424591\n",
            "Step 10800, Loss 0.03567236661911011\n",
            "Step 10810, Loss 0.6033238172531128\n",
            "Step 10820, Loss 0.7645789384841919\n",
            "Step 10830, Loss 0.4424007534980774\n",
            "Step 10840, Loss 0.11817470192909241\n",
            "Step 10850, Loss 0.3461771607398987\n",
            "Step 10860, Loss 0.7612472772598267\n",
            "Step 10870, Loss 0.41294047236442566\n",
            "Step 10880, Loss 0.07388263940811157\n",
            "Step 10890, Loss 0.6687325239181519\n",
            "Step 10900, Loss 0.5443807244300842\n",
            "Step 10910, Loss 0.6667273044586182\n",
            "Step 10920, Loss 0.4753376245498657\n",
            "Step 10930, Loss 0.30184900760650635\n",
            "Step 10940, Loss 0.34971022605895996\n",
            "Step 10950, Loss 0.2981031537055969\n",
            "Step 10960, Loss 0.6356931328773499\n",
            "Step 10970, Loss 0.08210590481758118\n",
            "Step 10980, Loss 0.1615133285522461\n",
            "Step 10990, Loss 0.11073559522628784\n",
            "Step 11000, Loss 0.014052510261535645\n",
            "Step 11010, Loss 0.41669711470603943\n",
            "Step 11020, Loss 0.7447699904441833\n",
            "Step 11030, Loss 0.01323920488357544\n",
            "Step 11040, Loss 0.3652242422103882\n",
            "Step 11050, Loss 0.40508386492729187\n",
            "Step 11060, Loss 0.5274964570999146\n",
            "Step 11070, Loss 0.35470065474510193\n",
            "Step 11080, Loss 0.11060920357704163\n",
            "Step 11090, Loss 0.13637059926986694\n",
            "Step 11100, Loss 0.2623063325881958\n",
            "Step 11110, Loss 0.5594022870063782\n",
            "Step 11120, Loss 0.3234911262989044\n",
            "Step 11130, Loss 0.10129767656326294\n",
            "Step 11140, Loss 0.9506463408470154\n",
            "Step 11150, Loss 0.5471341609954834\n",
            "Step 11160, Loss 0.39906179904937744\n",
            "Step 11170, Loss 0.5207844972610474\n",
            "Step 11180, Loss 0.12104707956314087\n",
            "Step 11190, Loss 0.6448405981063843\n",
            "Step 11200, Loss 0.09841611981391907\n",
            "Step 11210, Loss 0.47738245129585266\n",
            "Step 11220, Loss 0.1365755796432495\n",
            "Step 11230, Loss 0.03916612267494202\n",
            "Step 11240, Loss 0.15031632781028748\n",
            "Step 11250, Loss 0.45543599128723145\n",
            "Step 11260, Loss 0.010162830352783203\n",
            "Step 11270, Loss 0.5236058831214905\n",
            "Step 11280, Loss 0.2496536374092102\n",
            "Step 11290, Loss 0.12101668119430542\n",
            "Step 11300, Loss 0.30636489391326904\n",
            "Step 11310, Loss 0.344118595123291\n",
            "Step 11320, Loss 0.20959115028381348\n",
            "Step 11330, Loss 0.47437793016433716\n",
            "Step 11340, Loss 0.06391975283622742\n",
            "Step 11350, Loss 0.006378054618835449\n",
            "Step 11360, Loss 0.1942591667175293\n",
            "Step 11370, Loss 0.769231379032135\n",
            "Step 11380, Loss 0.39540567994117737\n",
            "Step 11390, Loss 0.43926873803138733\n",
            "Step 11400, Loss 0.5672896504402161\n",
            "Step 11410, Loss 0.5506287813186646\n",
            "Step 11420, Loss 0.3287259042263031\n",
            "Step 11430, Loss 0.14180594682693481\n",
            "Step 11440, Loss 0.031838059425354004\n",
            "Step 11450, Loss 0.0788058340549469\n",
            "Step 11460, Loss 0.4969661235809326\n",
            "Step 11470, Loss 0.23946493864059448\n",
            "Step 11480, Loss 1.004319190979004\n",
            "Step 11490, Loss 0.26500606536865234\n",
            "Step 11500, Loss 0.302706778049469\n",
            "Step 11510, Loss 0.6793678402900696\n",
            "Step 11520, Loss 0.26524901390075684\n",
            "Step 11530, Loss 0.6646317839622498\n",
            "Step 11540, Loss 0.4640337824821472\n",
            "Step 11550, Loss 0.1876276135444641\n",
            "Step 11560, Loss 0.0870787501335144\n",
            "Step 11570, Loss 0.21866726875305176\n",
            "Step 11580, Loss 0.08322864770889282\n",
            "Step 11590, Loss 0.31679657101631165\n",
            "Step 11600, Loss 0.5524566769599915\n",
            "Step 11610, Loss 0.30873316526412964\n",
            "Step 11620, Loss 0.05956277251243591\n",
            "Step 11630, Loss 0.2362060248851776\n",
            "Step 11640, Loss 0.04887238144874573\n",
            "Step 11650, Loss 0.3092772960662842\n",
            "Step 11660, Loss 0.3054671287536621\n",
            "Step 11670, Loss 0.7208106517791748\n",
            "Step 11680, Loss 0.6039946675300598\n",
            "Step 11690, Loss 1.2951985597610474\n",
            "Step 11700, Loss 0.18532918393611908\n",
            "Step 11710, Loss 0.32303494215011597\n",
            "Step 11720, Loss 0.16277727484703064\n",
            "Step 11730, Loss 0.060816407203674316\n",
            "Step 11740, Loss 0.4256085157394409\n",
            "Step 11750, Loss 0.5740224123001099\n",
            "Step 11760, Loss 0.2267920970916748\n",
            "Step 11770, Loss 0.16712117195129395\n",
            "Step 11780, Loss 0.31998151540756226\n",
            "Step 11790, Loss 0.08094403147697449\n",
            "Step 11800, Loss 0.7957677841186523\n",
            "Step 11810, Loss 0.4581845700740814\n",
            "Step 11820, Loss 0.29608190059661865\n",
            "Step 11830, Loss 0.8536977171897888\n",
            "Step 11840, Loss 0.23328447341918945\n",
            "Step 11850, Loss 0.7045392990112305\n",
            "Step 11860, Loss 0.3144720196723938\n",
            "Step 11870, Loss 0.17782878875732422\n",
            "Step 11880, Loss 0.020896852016448975\n",
            "Step 11890, Loss 0.1771450936794281\n",
            "Step 11900, Loss 0.32984495162963867\n",
            "Step 11910, Loss 0.22363007068634033\n",
            "Step 11920, Loss 0.11661815643310547\n",
            "Step 11930, Loss 0.2609871029853821\n",
            "Step 11940, Loss 0.07142433524131775\n",
            "Step 11950, Loss 0.3119889199733734\n",
            "Step 11960, Loss 0.2281227558851242\n",
            "Step 11970, Loss 0.888115406036377\n",
            "Step 11980, Loss 0.3377543091773987\n",
            "Step 11990, Loss 0.4627503752708435\n",
            "Step 12000, Loss 0.41097772121429443\n",
            "Step 12010, Loss 0.5030444264411926\n",
            "Step 12020, Loss 0.6459794044494629\n",
            "Step 12030, Loss 0.12871667742729187\n",
            "Step 12040, Loss 0.33864450454711914\n",
            "Step 12050, Loss 0.3535141348838806\n",
            "Step 12060, Loss 0.17997464537620544\n",
            "Step 12070, Loss 0.055999577045440674\n",
            "Step 12080, Loss 0.08318191766738892\n",
            "Step 12090, Loss 0.44668444991111755\n",
            "Step 12100, Loss 0.3582126498222351\n",
            "Step 12110, Loss 0.3292520046234131\n",
            "Step 12120, Loss 0.31405600905418396\n",
            "Step 12130, Loss 0.19503921270370483\n",
            "Step 12140, Loss 0.013233542442321777\n",
            "Step 12150, Loss 0.6339414119720459\n",
            "Step 12160, Loss 0.44455474615097046\n",
            "Step 12170, Loss 0.05334341526031494\n",
            "Step 12180, Loss 0.1902945637702942\n",
            "Step 12190, Loss 0.3603134751319885\n",
            "Step 12200, Loss 0.5758713483810425\n",
            "Step 12210, Loss 0.2519904375076294\n",
            "Step 12220, Loss 1.02241051197052\n",
            "Step 12230, Loss 0.4088451862335205\n",
            "Step 12240, Loss 0.08376607298851013\n",
            "Step 12250, Loss 0.4931902587413788\n",
            "Step 12260, Loss 0.27053582668304443\n",
            "Step 12270, Loss 1.2911605834960938\n",
            "Step 12280, Loss 0.19049754738807678\n",
            "Step 12290, Loss 0.22616127133369446\n",
            "Step 12300, Loss 0.07164338231086731\n",
            "Step 12310, Loss 0.19508880376815796\n",
            "Step 12320, Loss 0.02588355541229248\n",
            "Step 12330, Loss 0.6855263113975525\n",
            "Step 12340, Loss 0.1371592879295349\n",
            "Step 12350, Loss 0.06845438480377197\n",
            "Step 12360, Loss 0.2767399251461029\n",
            "Step 12370, Loss 0.5470432043075562\n",
            "Step 12380, Loss 0.22779953479766846\n",
            "Step 12390, Loss 0.144239604473114\n",
            "Step 12400, Loss 0.18891531229019165\n",
            "Step 12410, Loss 0.4953683614730835\n",
            "Step 12420, Loss 0.3747013211250305\n",
            "Step 12430, Loss 0.3940207362174988\n",
            "Step 12440, Loss 0.6697781085968018\n",
            "Step 12450, Loss 0.016393542289733887\n",
            "Step 12460, Loss 0.8051710724830627\n",
            "Step 12470, Loss 0.5688561201095581\n",
            "Step 12480, Loss 0.19820153713226318\n",
            "Step 12490, Loss 0.009629905223846436\n",
            "Step 12500, Loss 0.3682553470134735\n",
            "Step 12510, Loss 0.27371737360954285\n",
            "Step 12520, Loss 0.32376131415367126\n",
            "Step 12530, Loss 0.12170043587684631\n",
            "Step 12540, Loss 0.20704451203346252\n",
            "Step 12550, Loss 0.3509975075721741\n",
            "Step 12560, Loss 0.027107834815979004\n",
            "Step 12570, Loss 0.8468257784843445\n",
            "Step 12580, Loss 0.48084497451782227\n",
            "Step 12590, Loss 1.051466941833496\n",
            "Step 12600, Loss 0.2461303174495697\n",
            "Step 12610, Loss 0.2951147258281708\n",
            "Step 12620, Loss 0.7203428149223328\n",
            "Step 12630, Loss 0.20129325985908508\n",
            "Step 12640, Loss 0.6232978701591492\n",
            "Step 12650, Loss 0.543677806854248\n",
            "Step 12660, Loss 0.33247822523117065\n",
            "Step 12670, Loss 0.12550568580627441\n",
            "Step 12680, Loss 0.38165464997291565\n",
            "Step 12690, Loss 0.2594022750854492\n",
            "Step 12700, Loss 0.0960988998413086\n",
            "Step 12710, Loss 0.2528873682022095\n",
            "Step 12720, Loss 0.9282447099685669\n",
            "Step 12730, Loss 0.052701592445373535\n",
            "Step 12740, Loss 0.5667216181755066\n",
            "Step 12750, Loss 0.623894989490509\n",
            "Step 12760, Loss 0.2288532555103302\n",
            "Step 12770, Loss 0.7897161841392517\n",
            "Step 12780, Loss 0.5692740678787231\n",
            "Step 12790, Loss 0.058684706687927246\n",
            "Step 12800, Loss 0.42936062812805176\n",
            "Step 12810, Loss 0.08359086513519287\n",
            "Step 12820, Loss 0.22303253412246704\n",
            "Step 12830, Loss 0.4784926474094391\n",
            "Step 12840, Loss 0.3092138171195984\n",
            "Step 12850, Loss 0.36707547307014465\n",
            "Step 12860, Loss 0.2402217984199524\n",
            "Step 12870, Loss 0.05577796697616577\n",
            "Step 12880, Loss 0.47322797775268555\n",
            "Step 12890, Loss 1.0288275480270386\n",
            "Step 12900, Loss 0.20642846822738647\n",
            "Step 12910, Loss 0.4356692135334015\n",
            "Step 12920, Loss 0.37852615118026733\n",
            "Step 12930, Loss 0.31417784094810486\n",
            "Step 12940, Loss 0.3815961480140686\n",
            "Step 12950, Loss 0.39969536662101746\n",
            "Step 12960, Loss 1.2336548566818237\n",
            "Step 12970, Loss 0.2713041305541992\n",
            "Step 12980, Loss 0.3759869337081909\n",
            "Step 12990, Loss 0.5533254146575928\n",
            "Step 13000, Loss 0.4489265978336334\n",
            "Step 13010, Loss 0.18297171592712402\n",
            "Step 13020, Loss 0.33769363164901733\n",
            "Step 13030, Loss 0.13802304863929749\n",
            "Step 13040, Loss 0.34375879168510437\n",
            "Step 13050, Loss 0.2695655822753906\n",
            "Step 13060, Loss 0.2579820454120636\n",
            "Step 13070, Loss 0.4728615880012512\n",
            "Step 13080, Loss 0.11633768677711487\n",
            "Step 13090, Loss 0.3265649080276489\n",
            "Step 13100, Loss 0.24501478672027588\n",
            "Step 13110, Loss 0.11992454528808594\n",
            "Step 13120, Loss 0.43135130405426025\n",
            "Step 13130, Loss 0.7398392558097839\n",
            "Step 13140, Loss 0.5630660057067871\n",
            "Step 13150, Loss 0.16604790091514587\n",
            "Step 13160, Loss 0.18662863969802856\n",
            "Step 13170, Loss 0.20033660531044006\n",
            "Step 13180, Loss 0.20013141632080078\n",
            "Step 13190, Loss 0.4811318516731262\n",
            "Step 13200, Loss 0.1790119707584381\n",
            "Step 13210, Loss 0.4670194387435913\n",
            "Step 13220, Loss 0.10664048790931702\n",
            "Step 13230, Loss 0.43278446793556213\n",
            "Step 13240, Loss 0.5110721588134766\n",
            "Step 13250, Loss 0.4062493145465851\n",
            "Step 13260, Loss 0.8448989391326904\n",
            "Step 13270, Loss 0.09586608409881592\n",
            "Step 13280, Loss 0.09682875871658325\n",
            "Step 13290, Loss 0.035611510276794434\n",
            "Step 13300, Loss 0.06920257210731506\n",
            "Step 13310, Loss 0.2621954679489136\n",
            "Step 13320, Loss 0.5368717312812805\n",
            "Step 13330, Loss 0.6033796072006226\n",
            "Step 13340, Loss 0.00931769609451294\n",
            "Step 13350, Loss 0.16143745183944702\n",
            "Step 13360, Loss 0.41129204630851746\n",
            "Step 13370, Loss 0.4673314094543457\n",
            "Step 13380, Loss 0.38301047682762146\n",
            "Step 13390, Loss 0.027347028255462646\n",
            "Step 13400, Loss 0.252685546875\n",
            "Step 13410, Loss 0.8375436067581177\n",
            "Step 13420, Loss 0.33292585611343384\n",
            "Step 13430, Loss 0.248141348361969\n",
            "Step 13440, Loss 0.8658061623573303\n",
            "Step 13450, Loss 1.731410026550293\n",
            "Step 13460, Loss 0.45866936445236206\n",
            "Step 13470, Loss 0.2861998379230499\n",
            "Step 13480, Loss 0.019261300563812256\n",
            "Step 13490, Loss 0.1793336272239685\n",
            "Step 13500, Loss 1.1380796432495117\n",
            "Step 13510, Loss 0.16625991463661194\n",
            "Step 13520, Loss 0.23973999917507172\n",
            "Step 13530, Loss 0.65190190076828\n",
            "Step 13540, Loss 0.679501473903656\n",
            "Step 13550, Loss 0.9216223359107971\n",
            "Step 13560, Loss 0.3895282745361328\n",
            "Step 13570, Loss 0.20933771133422852\n",
            "Step 13580, Loss 0.5151086449623108\n",
            "Step 13590, Loss 0.19840863347053528\n",
            "Step 13600, Loss 0.6410592794418335\n",
            "Step 13610, Loss 0.8501023054122925\n",
            "Step 13620, Loss 0.09458616375923157\n",
            "Step 13630, Loss 0.3291335105895996\n",
            "Step 13640, Loss 0.782692551612854\n",
            "Step 13650, Loss 0.5186872482299805\n",
            "Step 13660, Loss 0.12978321313858032\n",
            "Step 13670, Loss 0.1513417363166809\n",
            "Step 13680, Loss 0.16976609826087952\n",
            "Step 13690, Loss 0.10498207807540894\n",
            "Starting epoch 5/5, LR = [0.001]\n",
            "Step 13700, Loss 0.17993247509002686\n",
            "Step 13710, Loss 0.19175958633422852\n",
            "Step 13720, Loss 0.4804038107395172\n",
            "Step 13730, Loss 0.4297637641429901\n",
            "Step 13740, Loss 0.19490152597427368\n",
            "Step 13750, Loss 0.22608116269111633\n",
            "Step 13760, Loss 0.7375705242156982\n",
            "Step 13770, Loss 0.02570962905883789\n",
            "Step 13780, Loss 0.6135318279266357\n",
            "Step 13790, Loss 0.4817492961883545\n",
            "Step 13800, Loss 0.12314081192016602\n",
            "Step 13810, Loss 0.4224908649921417\n",
            "Step 13820, Loss 0.38625746965408325\n",
            "Step 13830, Loss 0.3314252495765686\n",
            "Step 13840, Loss 0.0801892876625061\n",
            "Step 13850, Loss 0.23969581723213196\n",
            "Step 13860, Loss 0.1375422477722168\n",
            "Step 13870, Loss 0.4803752899169922\n",
            "Step 13880, Loss 0.023845553398132324\n",
            "Step 13890, Loss 0.2500007748603821\n",
            "Step 13900, Loss 0.1572779417037964\n",
            "Step 13910, Loss 0.045272886753082275\n",
            "Step 13920, Loss 0.18362504243850708\n",
            "Step 13930, Loss 0.27909180521965027\n",
            "Step 13940, Loss 0.3799733817577362\n",
            "Step 13950, Loss 0.18146321177482605\n",
            "Step 13960, Loss 0.16294485330581665\n",
            "Step 13970, Loss 0.1103067696094513\n",
            "Step 13980, Loss 0.021351993083953857\n",
            "Step 13990, Loss 0.012184858322143555\n",
            "Step 14000, Loss 0.35477179288864136\n",
            "Step 14010, Loss 0.27472537755966187\n",
            "Step 14020, Loss 0.7458438873291016\n",
            "Step 14030, Loss 0.04336068034172058\n",
            "Step 14040, Loss 0.3270050883293152\n",
            "Step 14050, Loss 1.0372283458709717\n",
            "Step 14060, Loss 0.45203450322151184\n",
            "Step 14070, Loss 0.0032080411911010742\n",
            "Step 14080, Loss 1.235292673110962\n",
            "Step 14090, Loss 0.6895455121994019\n",
            "Step 14100, Loss 1.1310384273529053\n",
            "Step 14110, Loss 0.20832663774490356\n",
            "Step 14120, Loss 0.35239896178245544\n",
            "Step 14130, Loss 0.09585553407669067\n",
            "Step 14140, Loss 0.40753722190856934\n",
            "Step 14150, Loss 0.6381822824478149\n",
            "Step 14160, Loss 0.4600793123245239\n",
            "Step 14170, Loss 0.7109328508377075\n",
            "Step 14180, Loss 0.7249818444252014\n",
            "Step 14190, Loss 0.24939404428005219\n",
            "Step 14200, Loss 0.4394761323928833\n",
            "Step 14210, Loss 0.02093803882598877\n",
            "Step 14220, Loss 0.48080652952194214\n",
            "Step 14230, Loss 0.13618773221969604\n",
            "Step 14240, Loss 0.024393022060394287\n",
            "Step 14250, Loss 0.30473563075065613\n",
            "Step 14260, Loss 0.12430155277252197\n",
            "Step 14270, Loss 0.10409966111183167\n",
            "Step 14280, Loss 0.0022272467613220215\n",
            "Step 14290, Loss 0.24507048726081848\n",
            "Step 14300, Loss 0.38033485412597656\n",
            "Step 14310, Loss 0.13482323288917542\n",
            "Step 14320, Loss 0.3801855742931366\n",
            "Step 14330, Loss 0.2854471802711487\n",
            "Step 14340, Loss 0.4238046705722809\n",
            "Step 14350, Loss 0.2247069776058197\n",
            "Step 14360, Loss 0.16612508893013\n",
            "Step 14370, Loss 0.09979122877120972\n",
            "Step 14380, Loss 0.31384891271591187\n",
            "Step 14390, Loss 0.07071506977081299\n",
            "Step 14400, Loss 0.05421826243400574\n",
            "Step 14410, Loss 0.0023431777954101562\n",
            "Step 14420, Loss 0.1550528109073639\n",
            "Step 14430, Loss 0.09544527530670166\n",
            "Step 14440, Loss 0.8317021131515503\n",
            "Step 14450, Loss 0.39186015725135803\n",
            "Step 14460, Loss 0.05247250199317932\n",
            "Step 14470, Loss 0.3245258927345276\n",
            "Step 14480, Loss 0.3624171018600464\n",
            "Step 14490, Loss 0.46326732635498047\n",
            "Step 14500, Loss 0.2078702747821808\n",
            "Step 14510, Loss 0.15396061539649963\n",
            "Step 14520, Loss 0.07447394728660583\n",
            "Step 14530, Loss 0.4991888403892517\n",
            "Step 14540, Loss 0.32808181643486023\n",
            "Step 14550, Loss 0.1784406304359436\n",
            "Step 14560, Loss 0.1414976418018341\n",
            "Step 14570, Loss 0.15471160411834717\n",
            "Step 14580, Loss 0.19802594184875488\n",
            "Step 14590, Loss 0.29824626445770264\n",
            "Step 14600, Loss 0.025903701782226562\n",
            "Step 14610, Loss 0.19728165864944458\n",
            "Step 14620, Loss 0.39562171697616577\n",
            "Step 14630, Loss 0.032045185565948486\n",
            "Step 14640, Loss 0.2618976831436157\n",
            "Step 14650, Loss 0.1367085576057434\n",
            "Step 14660, Loss 0.29807454347610474\n",
            "Step 14670, Loss 0.1383669674396515\n",
            "Step 14680, Loss 0.5484937429428101\n",
            "Step 14690, Loss 0.10560190677642822\n",
            "Step 14700, Loss 0.012758731842041016\n",
            "Step 14710, Loss 0.02112632989883423\n",
            "Step 14720, Loss 0.2496911883354187\n",
            "Step 14730, Loss 0.09105837345123291\n",
            "Step 14740, Loss 0.208621084690094\n",
            "Step 14750, Loss 0.07171684503555298\n",
            "Step 14760, Loss 0.024819612503051758\n",
            "Step 14770, Loss 0.0025344491004943848\n",
            "Step 14780, Loss 0.16669604182243347\n",
            "Step 14790, Loss 0.8113477826118469\n",
            "Step 14800, Loss 1.0405523777008057\n",
            "Step 14810, Loss 0.282751202583313\n",
            "Step 14820, Loss 0.008682966232299805\n",
            "Step 14830, Loss 0.05484628677368164\n",
            "Step 14840, Loss 0.2879309058189392\n",
            "Step 14850, Loss 0.44938308000564575\n",
            "Step 14860, Loss 0.17400968074798584\n",
            "Step 14870, Loss 0.2615147829055786\n",
            "Step 14880, Loss 0.06594163179397583\n",
            "Step 14890, Loss 0.11941349506378174\n",
            "Step 14900, Loss 0.02058511972427368\n",
            "Step 14910, Loss 0.036663711071014404\n",
            "Step 14920, Loss 0.4179304242134094\n",
            "Step 14930, Loss 0.19845038652420044\n",
            "Step 14940, Loss 0.2830715477466583\n",
            "Step 14950, Loss 0.24332484602928162\n",
            "Step 14960, Loss 0.040485501289367676\n",
            "Step 14970, Loss 0.1553390622138977\n",
            "Step 14980, Loss 0.11050742864608765\n",
            "Step 14990, Loss 0.037491559982299805\n",
            "Step 15000, Loss 0.5812788009643555\n",
            "Step 15010, Loss 0.08741065859794617\n",
            "Step 15020, Loss 0.5604432821273804\n",
            "Step 15030, Loss 0.23201557993888855\n",
            "Step 15040, Loss 0.07999351620674133\n",
            "Step 15050, Loss 0.7177850604057312\n",
            "Step 15060, Loss 0.03063756227493286\n",
            "Step 15070, Loss 0.1394624412059784\n",
            "Step 15080, Loss 0.44630634784698486\n",
            "Step 15090, Loss 0.18373504281044006\n",
            "Step 15100, Loss 0.2137938141822815\n",
            "Step 15110, Loss 0.11091971397399902\n",
            "Step 15120, Loss 0.05412942171096802\n",
            "Step 15130, Loss 0.2935885190963745\n",
            "Step 15140, Loss 0.25478097796440125\n",
            "Step 15150, Loss 0.5415668487548828\n",
            "Step 15160, Loss 0.06232321262359619\n",
            "Step 15170, Loss 0.41858533024787903\n",
            "Step 15180, Loss 0.005032777786254883\n",
            "Step 15190, Loss 0.07395565509796143\n",
            "Step 15200, Loss 0.4457016587257385\n",
            "Step 15210, Loss 0.23518258333206177\n",
            "Step 15220, Loss 0.36222851276397705\n",
            "Step 15230, Loss 0.319608598947525\n",
            "Step 15240, Loss 0.6207420229911804\n",
            "Step 15250, Loss 0.7923493981361389\n",
            "Step 15260, Loss 0.3536750078201294\n",
            "Step 15270, Loss 0.19514644145965576\n",
            "Step 15280, Loss 0.22231754660606384\n",
            "Step 15290, Loss 0.03463554382324219\n",
            "Step 15300, Loss 0.5127604603767395\n",
            "Step 15310, Loss 0.19515159726142883\n",
            "Step 15320, Loss 0.27959394454956055\n",
            "Step 15330, Loss 0.2270982265472412\n",
            "Step 15340, Loss 0.1841711699962616\n",
            "Step 15350, Loss 0.26722240447998047\n",
            "Step 15360, Loss 0.07809415459632874\n",
            "Step 15370, Loss 0.18827630579471588\n",
            "Step 15380, Loss 0.3869140148162842\n",
            "Step 15390, Loss 0.0036701560020446777\n",
            "Step 15400, Loss 0.06232637166976929\n",
            "Step 15410, Loss 0.2764975428581238\n",
            "Step 15420, Loss 0.03576689958572388\n",
            "Step 15430, Loss 0.5037270188331604\n",
            "Step 15440, Loss 0.08331853151321411\n",
            "Step 15450, Loss 0.24574822187423706\n",
            "Step 15460, Loss 0.04011738300323486\n",
            "Step 15470, Loss 0.053738534450531006\n",
            "Step 15480, Loss 0.060314446687698364\n",
            "Step 15490, Loss 0.2000196874141693\n",
            "Step 15500, Loss 0.4447307586669922\n",
            "Step 15510, Loss 0.15361490845680237\n",
            "Step 15520, Loss 0.22742296755313873\n",
            "Step 15530, Loss 0.4256207346916199\n",
            "Step 15540, Loss 0.058555543422698975\n",
            "Step 15550, Loss 0.3841489553451538\n",
            "Step 15560, Loss 0.04016387462615967\n",
            "Step 15570, Loss 0.2586821913719177\n",
            "Step 15580, Loss 0.07820022106170654\n",
            "Step 15590, Loss 0.9269956350326538\n",
            "Step 15600, Loss 0.4557763338088989\n",
            "Step 15610, Loss 0.3987649977207184\n",
            "Step 15620, Loss 0.2977444529533386\n",
            "Step 15630, Loss 0.32466092705726624\n",
            "Step 15640, Loss 0.45022180676460266\n",
            "Step 15650, Loss 0.04060161113739014\n",
            "Step 15660, Loss 0.29754337668418884\n",
            "Step 15670, Loss 0.3810065984725952\n",
            "Step 15680, Loss 0.543258547782898\n",
            "Step 15690, Loss 0.046574294567108154\n",
            "Step 15700, Loss 0.5112792253494263\n",
            "Step 15710, Loss 0.06858789920806885\n",
            "Step 15720, Loss 0.04428154230117798\n",
            "Step 15730, Loss 0.27760186791419983\n",
            "Step 15740, Loss 0.17962530255317688\n",
            "Step 15750, Loss 0.2092823088169098\n",
            "Step 15760, Loss 1.1480001211166382\n",
            "Step 15770, Loss 0.24747693538665771\n",
            "Step 15780, Loss 0.5113792419433594\n",
            "Step 15790, Loss 0.010833144187927246\n",
            "Step 15800, Loss 0.41401034593582153\n",
            "Step 15810, Loss 0.7825320959091187\n",
            "Step 15820, Loss 0.15212416648864746\n",
            "Step 15830, Loss 0.3125220835208893\n",
            "Step 15840, Loss 0.31399357318878174\n",
            "Step 15850, Loss 0.10957172513008118\n",
            "Step 15860, Loss 0.504163384437561\n",
            "Step 15870, Loss 0.06989365816116333\n",
            "Step 15880, Loss 0.06749579310417175\n",
            "Step 15890, Loss 0.039106786251068115\n",
            "Step 15900, Loss 0.28365635871887207\n",
            "Step 15910, Loss 0.0016170144081115723\n",
            "Step 15920, Loss 0.009027957916259766\n",
            "Step 15930, Loss 0.26188260316848755\n",
            "Step 15940, Loss 0.47717684507369995\n",
            "Step 15950, Loss 0.19457003474235535\n",
            "Step 15960, Loss 0.210139662027359\n",
            "Step 15970, Loss 0.3546944260597229\n",
            "Step 15980, Loss 0.18015703558921814\n",
            "Step 15990, Loss 0.0844673216342926\n",
            "Step 16000, Loss 0.7689155340194702\n",
            "Step 16010, Loss 0.2597731649875641\n",
            "Step 16020, Loss 0.17447328567504883\n",
            "Step 16030, Loss 0.18961721658706665\n",
            "Step 16040, Loss 0.04971843957901001\n",
            "Step 16050, Loss 0.30140504240989685\n",
            "Step 16060, Loss 0.1310766041278839\n",
            "Step 16070, Loss 0.4906117618083954\n",
            "Step 16080, Loss 0.21093755960464478\n",
            "Step 16090, Loss 0.07071489095687866\n",
            "Step 16100, Loss 0.019169628620147705\n",
            "Step 16110, Loss 0.23481711745262146\n",
            "Step 16120, Loss 0.14195004105567932\n",
            "Step 16130, Loss 0.3638664186000824\n",
            "Step 16140, Loss 0.01486063003540039\n",
            "Step 16150, Loss 0.1113629937171936\n",
            "Step 16160, Loss 0.1717955470085144\n",
            "Step 16170, Loss 0.05189782381057739\n",
            "Step 16180, Loss 0.5698126554489136\n",
            "Step 16190, Loss 0.07844924926757812\n",
            "Step 16200, Loss 0.07594674825668335\n",
            "Step 16210, Loss 0.23728835582733154\n",
            "Step 16220, Loss 0.06365275382995605\n",
            "Step 16230, Loss 0.3978796601295471\n",
            "Step 16240, Loss 0.18433207273483276\n",
            "Step 16250, Loss 0.059699833393096924\n",
            "Step 16260, Loss 0.3535125255584717\n",
            "Step 16270, Loss 0.11280059814453125\n",
            "Step 16280, Loss 0.03225114941596985\n",
            "Step 16290, Loss 0.08794978260993958\n",
            "Step 16300, Loss 0.0798090398311615\n",
            "Step 16310, Loss 0.23196330666542053\n",
            "Step 16320, Loss 0.12348818778991699\n",
            "Step 16330, Loss 0.5124368071556091\n",
            "Step 16340, Loss 0.29161423444747925\n",
            "Step 16350, Loss 0.11576950550079346\n",
            "Step 16360, Loss 0.024601340293884277\n",
            "Step 16370, Loss 0.1081390380859375\n",
            "Step 16380, Loss 0.35455259680747986\n",
            "Step 16390, Loss 0.44054704904556274\n",
            "Step 16400, Loss 0.20282596349716187\n",
            "Step 16410, Loss 0.2132701277732849\n",
            "Step 16420, Loss 0.40742698311805725\n",
            "Step 16430, Loss 0.030330121517181396\n",
            "Step 16440, Loss 0.7721215486526489\n",
            "Step 16450, Loss 0.4181796908378601\n",
            "Step 16460, Loss 0.06566721200942993\n",
            "Step 16470, Loss 0.14584344625473022\n",
            "Step 16480, Loss 0.6779886484146118\n",
            "Step 16490, Loss 0.08419477939605713\n",
            "Step 16500, Loss 0.11869695782661438\n",
            "Step 16510, Loss 0.05662578344345093\n",
            "Step 16520, Loss 0.13055408000946045\n",
            "Step 16530, Loss 0.564752459526062\n",
            "Step 16540, Loss 0.44738709926605225\n",
            "Step 16550, Loss 1.0949385166168213\n",
            "Step 16560, Loss 0.129746675491333\n",
            "Step 16570, Loss 0.41593241691589355\n",
            "Step 16580, Loss 0.1614779531955719\n",
            "Step 16590, Loss 0.019653379917144775\n",
            "Step 16600, Loss 0.3541814088821411\n",
            "Step 16610, Loss 0.4284150004386902\n",
            "Step 16620, Loss 0.1682775765657425\n",
            "Step 16630, Loss 1.2728400230407715\n",
            "Step 16640, Loss 0.10311675071716309\n",
            "Step 16650, Loss 0.22586718201637268\n",
            "Step 16660, Loss 0.05413377285003662\n",
            "Step 16670, Loss 0.20182950794696808\n",
            "Step 16680, Loss 0.26500946283340454\n",
            "Step 16690, Loss 0.3034447431564331\n",
            "Step 16700, Loss 0.007402360439300537\n",
            "Step 16710, Loss 0.9134567379951477\n",
            "Step 16720, Loss 0.2405014932155609\n",
            "Step 16730, Loss 0.2051803171634674\n",
            "Step 16740, Loss 0.6274862289428711\n",
            "Step 16750, Loss 0.40248703956604004\n",
            "Step 16760, Loss 0.1316339671611786\n",
            "Step 16770, Loss 0.2774447798728943\n",
            "Step 16780, Loss 0.2518390119075775\n",
            "Step 16790, Loss 0.2755737006664276\n",
            "Step 16800, Loss 0.6607246398925781\n",
            "Step 16810, Loss 0.32344573736190796\n",
            "Step 16820, Loss 0.06581449508666992\n",
            "Step 16830, Loss 0.7194472551345825\n",
            "Step 16840, Loss 0.0030733346939086914\n",
            "Step 16850, Loss 0.2879784405231476\n",
            "Step 16860, Loss 0.6417268514633179\n",
            "Step 16870, Loss 0.0189608633518219\n",
            "Step 16880, Loss 0.015763580799102783\n",
            "Step 16890, Loss 0.293298602104187\n",
            "Step 16900, Loss 0.2514019310474396\n",
            "Step 16910, Loss 2.2585902214050293\n",
            "Step 16920, Loss 0.9728338122367859\n",
            "Step 16930, Loss 0.16678747534751892\n",
            "Step 16940, Loss 0.05874913930892944\n",
            "Step 16950, Loss 0.22612978518009186\n",
            "Step 16960, Loss 0.48188453912734985\n",
            "Step 16970, Loss 0.19356417655944824\n",
            "Step 16980, Loss 0.2096707820892334\n",
            "Step 16990, Loss 0.7717655897140503\n",
            "Step 17000, Loss 0.195267915725708\n",
            "Step 17010, Loss 0.36526912450790405\n",
            "Step 17020, Loss 0.629653811454773\n",
            "Step 17030, Loss 0.9585431814193726\n",
            "Step 17040, Loss 0.7923657298088074\n",
            "Step 17050, Loss 0.0256863534450531\n",
            "Step 17060, Loss 0.05393451452255249\n",
            "Step 17070, Loss 0.1572970300912857\n",
            "Step 17080, Loss 0.43041539192199707\n",
            "Step 17090, Loss 0.6956875324249268\n",
            "Step 17100, Loss 0.6867047548294067\n",
            "Step 17110, Loss 0.09488624334335327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD4CAYAAAAaT9YAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdqznIakxb6j",
        "colab_type": "code",
        "outputId": "77625451-e2df-4935-8158-b61c2fe3f433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#Test on source\n",
        "src_encoder = src_encoder.to(DEVICE)\n",
        "src_encoder.train(False)\n",
        "\n",
        "src_classifier = src_classifier.to(DEVICE)\n",
        "src_classifier.train(False)\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "  \n",
        "  outputs = src_classifier(src_encoder(images))\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "  \n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "src_accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy on Target Domain before Adaptation: {}'.format(src_accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1610/1610 [47:17<00:00,  1.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy on Target Domain before Adaptation: 0.5218555900621118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2qjhetXgOm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0wF3HiVhDq5",
        "colab_type": "code",
        "outputId": "da22bed7-e3e7-4437-9b1b-fad3cae9f3c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Train discriminator\n",
        "critic = Discriminator()\n",
        "loss_critic_vector = []\n",
        "target_loss_vector = []\n",
        "tgt_encoder = source_model\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "tgt_parameters_to_optimize = tgt_encoder.parameters() \n",
        "target_optimizer = optim.SGD(tgt_parameters_to_optimize, lr=LR_tgt, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "critic_parameters_to_optimize = critic.parameters()\n",
        "critic_optimizer = optim.SGD(critic_parameters_to_optimize, lr=LR_discr, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(target_optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "cudnn.benchmark\n",
        "\n",
        "current_step = 0\n",
        "\n",
        "tgt_encoder = tgt_encoder.to(DEVICE)\n",
        "src_encoder = src_encoder.to(DEVICE)\n",
        "critic = critic.to(DEVICE)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "  data_zip = enumerate(zip(train_dataloader, test_dataloader))\n",
        "  for step, ((images_src, _), (images_tgt, _)) in data_zip:\n",
        "    \n",
        "    #######################\n",
        "    # Train Discriminator #\n",
        "    #######################\n",
        "\n",
        "    images_src = images_src.to(DEVICE)\n",
        "    images_tgt = images_tgt.to(DEVICE)   \n",
        "\n",
        "    tgt_encoder.train()\n",
        "    \n",
        "    critic.train()\n",
        "\n",
        "    target_optimizer.zero_grad()\n",
        "\n",
        "    # extract and concat features  outputs\n",
        "    feat_src = src_encoder(images_src)\n",
        "    feat_tgt = tgt_encoder(images_tgt)\n",
        "    feat_concat = torch.cat((feat_src, feat_tgt), 0)\n",
        "\n",
        "    # predict on discriminator\n",
        "    pred_concat = critic(feat_concat.detach())\n",
        "\n",
        "    # prepare real and fake label\n",
        "    label_src = torch.ones(feat_src.size(0)).long()\n",
        "    label_tgt = torch.zeros(feat_tgt.size(0)).long()\n",
        "\n",
        "    label_src = label_src.to(DEVICE)\n",
        "    label_tgt = label_tgt.to(DEVICE)\n",
        "\n",
        "    label_concat = torch.cat((label_src, label_tgt), 0)\n",
        "\n",
        "    # compute loss for critic  \n",
        "    loss_critic = criterion(pred_concat, label_concat)\n",
        "    loss_critic_vector.append(loss_critic)\n",
        "    loss_critic.backward()\n",
        "\n",
        "    # optimize critic\n",
        "    critic_optimizer.step()\n",
        "\n",
        "    # domain accuracy\n",
        "    pred_cls = torch.squeeze(pred_concat.max(1)[1])\n",
        "    acc = (pred_cls == label_concat).float().mean()\n",
        "\n",
        "    ########################\n",
        "    # Train Target Encoder #\n",
        "    ########################\n",
        "\n",
        "    critic_optimizer.zero_grad()\n",
        "    target_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    # extract and target features outputs\n",
        "    feat_tgt = tgt_encoder(images_tgt)\n",
        "    # predict on discriminator outputs\n",
        "    pred_tgt = critic(feat_tgt)\n",
        "    # prepare fake labels \n",
        "    label_tgt = torch.ones(feat_tgt.size(0)).long()\n",
        "    label_tgt = label_tgt.to(DEVICE)\n",
        " \n",
        "    # compute loss for target encoder\n",
        "    loss_tgt = criterion(pred_tgt, label_tgt)\n",
        "    target_loss_vector.append(loss_tgt)\n",
        "    #loss_tgt = criterion(pred_concat, label_tgt)\n",
        "    loss_tgt.backward()\n",
        "\n",
        "    # Log loss\n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      #print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "      print(\"Epoch [{}/{}] Step [{}/{}]: \"\n",
        "                        \"discriminator_loss = {:.5f}, target_loss = {:.5f}, acc = {:.5f}\"\n",
        "                        .format(epoch + 1,\n",
        "                                NUM_EPOCHS,\n",
        "                                step + 1,\n",
        "                                len(test_dataloader),\n",
        "                                loss_critic.item(),\n",
        "                                loss_tgt.item(),\n",
        "                                acc.item()))\n",
        "\n",
        "    # optimize target encoder\n",
        "    target_optimizer.step()\n",
        "\n",
        "tgt_model = copy.deepcopy(tgt_encoder)\n",
        "final_discriminator = copy.deepcopy(critic)\n",
        "plt.plot(loss_critic_vector)\n",
        "plt.show()\n",
        "plt.plot(target_loss_vector)\n",
        "plt.show()\n",
        "#torch.save(tgt_encoder, tgt_encoder_path+'tgt_encoder.pt')\n",
        "#torch.save(src_classifier, src_classifier_path)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [2/5] Step [1442/1610]: discriminator_loss = 0.60262, target_loss = 0.44441, acc = 0.50000\n",
            "Epoch [2/5] Step [1443/1610]: discriminator_loss = 0.52290, target_loss = 0.41897, acc = 0.50000\n",
            "Epoch [2/5] Step [1444/1610]: discriminator_loss = 0.59347, target_loss = 0.40432, acc = 0.50000\n",
            "Epoch [2/5] Step [1445/1610]: discriminator_loss = 0.59118, target_loss = 0.41199, acc = 0.50000\n",
            "Epoch [2/5] Step [1446/1610]: discriminator_loss = 0.53077, target_loss = 0.42382, acc = 0.50000\n",
            "Epoch [2/5] Step [1447/1610]: discriminator_loss = 0.53538, target_loss = 0.41605, acc = 0.50000\n",
            "Epoch [2/5] Step [1448/1610]: discriminator_loss = 0.58546, target_loss = 0.39689, acc = 0.50000\n",
            "Epoch [2/5] Step [1449/1610]: discriminator_loss = 0.55919, target_loss = 0.38253, acc = 0.50000\n",
            "Epoch [2/5] Step [1450/1610]: discriminator_loss = 0.56883, target_loss = 0.40750, acc = 0.50000\n",
            "Epoch [2/5] Step [1451/1610]: discriminator_loss = 0.57295, target_loss = 0.41104, acc = 0.50000\n",
            "Epoch [2/5] Step [1452/1610]: discriminator_loss = 0.59642, target_loss = 0.39096, acc = 0.50000\n",
            "Epoch [2/5] Step [1453/1610]: discriminator_loss = 0.57041, target_loss = 0.36528, acc = 0.50000\n",
            "Epoch [2/5] Step [1454/1610]: discriminator_loss = 0.61167, target_loss = 0.36310, acc = 0.50000\n",
            "Epoch [2/5] Step [1455/1610]: discriminator_loss = 0.60299, target_loss = 0.35447, acc = 0.50000\n",
            "Epoch [2/5] Step [1456/1610]: discriminator_loss = 0.62876, target_loss = 0.38698, acc = 0.50000\n",
            "Epoch [2/5] Step [1457/1610]: discriminator_loss = 0.60164, target_loss = 0.38979, acc = 0.50000\n",
            "Epoch [2/5] Step [1458/1610]: discriminator_loss = 0.58372, target_loss = 0.41017, acc = 0.50000\n",
            "Epoch [2/5] Step [1459/1610]: discriminator_loss = 0.57778, target_loss = 0.40694, acc = 0.50000\n",
            "Epoch [2/5] Step [1460/1610]: discriminator_loss = 0.60629, target_loss = 0.39160, acc = 0.50000\n",
            "Epoch [2/5] Step [1461/1610]: discriminator_loss = 0.60247, target_loss = 0.39496, acc = 0.53125\n",
            "Epoch [2/5] Step [1462/1610]: discriminator_loss = 0.57678, target_loss = 0.36206, acc = 0.50000\n",
            "Epoch [2/5] Step [1463/1610]: discriminator_loss = 0.52073, target_loss = 0.40184, acc = 0.50000\n",
            "Epoch [2/5] Step [1464/1610]: discriminator_loss = 0.59861, target_loss = 0.39810, acc = 0.50000\n",
            "Epoch [2/5] Step [1465/1610]: discriminator_loss = 0.63252, target_loss = 0.40926, acc = 0.50000\n",
            "Epoch [2/5] Step [1466/1610]: discriminator_loss = 0.58162, target_loss = 0.42424, acc = 0.50000\n",
            "Epoch [2/5] Step [1467/1610]: discriminator_loss = 0.60659, target_loss = 0.39233, acc = 0.50000\n",
            "Epoch [2/5] Step [1468/1610]: discriminator_loss = 0.56904, target_loss = 0.40732, acc = 0.50000\n",
            "Epoch [2/5] Step [1469/1610]: discriminator_loss = 0.67165, target_loss = 0.34307, acc = 0.50000\n",
            "Epoch [2/5] Step [1470/1610]: discriminator_loss = 0.58522, target_loss = 0.40801, acc = 0.50000\n",
            "Epoch [2/5] Step [1471/1610]: discriminator_loss = 0.60173, target_loss = 0.38964, acc = 0.50000\n",
            "Epoch [2/5] Step [1472/1610]: discriminator_loss = 0.63215, target_loss = 0.38522, acc = 0.50000\n",
            "Epoch [2/5] Step [1473/1610]: discriminator_loss = 0.57581, target_loss = 0.45479, acc = 0.50000\n",
            "Epoch [2/5] Step [1474/1610]: discriminator_loss = 0.51320, target_loss = 0.43849, acc = 0.50000\n",
            "Epoch [2/5] Step [1475/1610]: discriminator_loss = 0.56260, target_loss = 0.44773, acc = 0.50000\n",
            "Epoch [2/5] Step [1476/1610]: discriminator_loss = 0.51797, target_loss = 0.40481, acc = 0.53125\n",
            "Epoch [2/5] Step [1477/1610]: discriminator_loss = 0.58037, target_loss = 0.41935, acc = 0.50000\n",
            "Epoch [2/5] Step [1478/1610]: discriminator_loss = 0.52817, target_loss = 0.43433, acc = 0.50000\n",
            "Epoch [2/5] Step [1479/1610]: discriminator_loss = 0.61743, target_loss = 0.40424, acc = 0.50000\n",
            "Epoch [2/5] Step [1480/1610]: discriminator_loss = 0.58029, target_loss = 0.39775, acc = 0.50000\n",
            "Epoch [2/5] Step [1481/1610]: discriminator_loss = 0.56576, target_loss = 0.38253, acc = 0.50000\n",
            "Epoch [2/5] Step [1482/1610]: discriminator_loss = 0.55178, target_loss = 0.42259, acc = 0.50000\n",
            "Epoch [2/5] Step [1483/1610]: discriminator_loss = 0.56097, target_loss = 0.40439, acc = 0.50000\n",
            "Epoch [2/5] Step [1484/1610]: discriminator_loss = 0.58170, target_loss = 0.39862, acc = 0.50000\n",
            "Epoch [2/5] Step [1485/1610]: discriminator_loss = 0.57753, target_loss = 0.38454, acc = 0.50000\n",
            "Epoch [2/5] Step [1486/1610]: discriminator_loss = 0.63018, target_loss = 0.37273, acc = 0.50000\n",
            "Epoch [2/5] Step [1487/1610]: discriminator_loss = 0.61192, target_loss = 0.35994, acc = 0.50000\n",
            "Epoch [2/5] Step [1488/1610]: discriminator_loss = 0.58619, target_loss = 0.38126, acc = 0.50000\n",
            "Epoch [2/5] Step [1489/1610]: discriminator_loss = 0.60586, target_loss = 0.37953, acc = 0.50000\n",
            "Epoch [2/5] Step [1490/1610]: discriminator_loss = 0.64160, target_loss = 0.34263, acc = 0.50000\n",
            "Epoch [2/5] Step [1491/1610]: discriminator_loss = 0.59923, target_loss = 0.39481, acc = 0.50000\n",
            "Epoch [2/5] Step [1492/1610]: discriminator_loss = 0.59651, target_loss = 0.35208, acc = 0.50000\n",
            "Epoch [2/5] Step [1493/1610]: discriminator_loss = 0.62845, target_loss = 0.38073, acc = 0.50000\n",
            "Epoch [2/5] Step [1494/1610]: discriminator_loss = 0.60614, target_loss = 0.35937, acc = 0.50000\n",
            "Epoch [2/5] Step [1495/1610]: discriminator_loss = 0.59320, target_loss = 0.37721, acc = 0.50000\n",
            "Epoch [2/5] Step [1496/1610]: discriminator_loss = 0.59279, target_loss = 0.37768, acc = 0.50000\n",
            "Epoch [2/5] Step [1497/1610]: discriminator_loss = 0.57227, target_loss = 0.37777, acc = 0.50000\n",
            "Epoch [2/5] Step [1498/1610]: discriminator_loss = 0.61730, target_loss = 0.38707, acc = 0.50000\n",
            "Epoch [2/5] Step [1499/1610]: discriminator_loss = 0.68590, target_loss = 0.37492, acc = 0.50000\n",
            "Epoch [2/5] Step [1500/1610]: discriminator_loss = 0.58161, target_loss = 0.36659, acc = 0.50000\n",
            "Epoch [2/5] Step [1501/1610]: discriminator_loss = 0.54590, target_loss = 0.43476, acc = 0.50000\n",
            "Epoch [2/5] Step [1502/1610]: discriminator_loss = 0.57489, target_loss = 0.42805, acc = 0.50000\n",
            "Epoch [2/5] Step [1503/1610]: discriminator_loss = 0.57148, target_loss = 0.41570, acc = 0.50000\n",
            "Epoch [2/5] Step [1504/1610]: discriminator_loss = 0.53375, target_loss = 0.42370, acc = 0.50000\n",
            "Epoch [2/5] Step [1505/1610]: discriminator_loss = 0.56058, target_loss = 0.39713, acc = 0.50000\n",
            "Epoch [2/5] Step [1506/1610]: discriminator_loss = 0.56903, target_loss = 0.39515, acc = 0.50000\n",
            "Epoch [2/5] Step [1507/1610]: discriminator_loss = 0.56169, target_loss = 0.40330, acc = 0.50000\n",
            "Epoch [2/5] Step [1508/1610]: discriminator_loss = 0.56666, target_loss = 0.42560, acc = 0.50000\n",
            "Epoch [2/5] Step [1509/1610]: discriminator_loss = 0.62827, target_loss = 0.40918, acc = 0.50000\n",
            "Epoch [2/5] Step [1510/1610]: discriminator_loss = 0.57416, target_loss = 0.38741, acc = 0.50000\n",
            "Epoch [2/5] Step [1511/1610]: discriminator_loss = 0.63148, target_loss = 0.39343, acc = 0.50000\n",
            "Epoch [2/5] Step [1512/1610]: discriminator_loss = 0.59523, target_loss = 0.41577, acc = 0.50000\n",
            "Epoch [2/5] Step [1513/1610]: discriminator_loss = 0.58079, target_loss = 0.36905, acc = 0.50000\n",
            "Epoch [2/5] Step [1514/1610]: discriminator_loss = 0.55300, target_loss = 0.41100, acc = 0.50000\n",
            "Epoch [2/5] Step [1515/1610]: discriminator_loss = 0.56352, target_loss = 0.42095, acc = 0.50000\n",
            "Epoch [2/5] Step [1516/1610]: discriminator_loss = 0.53617, target_loss = 0.40010, acc = 0.53125\n",
            "Epoch [2/5] Step [1517/1610]: discriminator_loss = 0.61209, target_loss = 0.37732, acc = 0.50000\n",
            "Epoch [2/5] Step [1518/1610]: discriminator_loss = 0.61489, target_loss = 0.39362, acc = 0.50000\n",
            "Epoch [2/5] Step [1519/1610]: discriminator_loss = 0.60153, target_loss = 0.40004, acc = 0.50000\n",
            "Epoch [2/5] Step [1520/1610]: discriminator_loss = 0.56484, target_loss = 0.41553, acc = 0.50000\n",
            "Epoch [2/5] Step [1521/1610]: discriminator_loss = 0.61111, target_loss = 0.36697, acc = 0.50000\n",
            "Epoch [2/5] Step [1522/1610]: discriminator_loss = 0.60964, target_loss = 0.38998, acc = 0.50000\n",
            "Epoch [2/5] Step [1523/1610]: discriminator_loss = 0.57183, target_loss = 0.39198, acc = 0.50000\n",
            "Epoch [2/5] Step [1524/1610]: discriminator_loss = 0.60475, target_loss = 0.40644, acc = 0.50000\n",
            "Epoch [2/5] Step [1525/1610]: discriminator_loss = 0.58895, target_loss = 0.42321, acc = 0.50000\n",
            "Epoch [2/5] Step [1526/1610]: discriminator_loss = 0.57895, target_loss = 0.41591, acc = 0.50000\n",
            "Epoch [2/5] Step [1527/1610]: discriminator_loss = 0.56838, target_loss = 0.39258, acc = 0.50000\n",
            "Epoch [2/5] Step [1528/1610]: discriminator_loss = 0.54039, target_loss = 0.41495, acc = 0.50000\n",
            "Epoch [2/5] Step [1529/1610]: discriminator_loss = 0.56360, target_loss = 0.43683, acc = 0.50000\n",
            "Epoch [2/5] Step [1530/1610]: discriminator_loss = 0.58602, target_loss = 0.41331, acc = 0.50000\n",
            "Epoch [2/5] Step [1531/1610]: discriminator_loss = 0.60199, target_loss = 0.40126, acc = 0.50000\n",
            "Epoch [2/5] Step [1532/1610]: discriminator_loss = 0.63840, target_loss = 0.40365, acc = 0.50000\n",
            "Epoch [2/5] Step [1533/1610]: discriminator_loss = 0.60373, target_loss = 0.38597, acc = 0.50000\n",
            "Epoch [2/5] Step [1534/1610]: discriminator_loss = 0.58427, target_loss = 0.40276, acc = 0.50000\n",
            "Epoch [2/5] Step [1535/1610]: discriminator_loss = 0.66652, target_loss = 0.39794, acc = 0.50000\n",
            "Epoch [2/5] Step [1536/1610]: discriminator_loss = 0.60103, target_loss = 0.38710, acc = 0.50000\n",
            "Epoch [2/5] Step [1537/1610]: discriminator_loss = 0.57314, target_loss = 0.40075, acc = 0.50000\n",
            "Epoch [2/5] Step [1538/1610]: discriminator_loss = 0.63987, target_loss = 0.38850, acc = 0.50000\n",
            "Epoch [2/5] Step [1539/1610]: discriminator_loss = 0.61102, target_loss = 0.43723, acc = 0.50000\n",
            "Epoch [2/5] Step [1540/1610]: discriminator_loss = 0.58371, target_loss = 0.38634, acc = 0.50000\n",
            "Epoch [2/5] Step [1541/1610]: discriminator_loss = 0.59235, target_loss = 0.37819, acc = 0.50000\n",
            "Epoch [2/5] Step [1542/1610]: discriminator_loss = 0.58677, target_loss = 0.40437, acc = 0.50000\n",
            "Epoch [2/5] Step [1543/1610]: discriminator_loss = 0.60003, target_loss = 0.36920, acc = 0.50000\n",
            "Epoch [2/5] Step [1544/1610]: discriminator_loss = 0.59193, target_loss = 0.37861, acc = 0.50000\n",
            "Epoch [2/5] Step [1545/1610]: discriminator_loss = 0.60798, target_loss = 0.37022, acc = 0.50000\n",
            "Epoch [2/5] Step [1546/1610]: discriminator_loss = 0.56984, target_loss = 0.39314, acc = 0.50000\n",
            "Epoch [2/5] Step [1547/1610]: discriminator_loss = 0.60406, target_loss = 0.37910, acc = 0.50000\n",
            "Epoch [2/5] Step [1548/1610]: discriminator_loss = 0.65708, target_loss = 0.37085, acc = 0.50000\n",
            "Epoch [2/5] Step [1549/1610]: discriminator_loss = 0.61347, target_loss = 0.37501, acc = 0.50000\n",
            "Epoch [2/5] Step [1550/1610]: discriminator_loss = 0.61063, target_loss = 0.38853, acc = 0.50000\n",
            "Epoch [2/5] Step [1551/1610]: discriminator_loss = 0.54425, target_loss = 0.40593, acc = 0.50000\n",
            "Epoch [2/5] Step [1552/1610]: discriminator_loss = 0.59811, target_loss = 0.37625, acc = 0.50000\n",
            "Epoch [2/5] Step [1553/1610]: discriminator_loss = 0.57577, target_loss = 0.40488, acc = 0.50000\n",
            "Epoch [2/5] Step [1554/1610]: discriminator_loss = 0.60454, target_loss = 0.37306, acc = 0.50000\n",
            "Epoch [2/5] Step [1555/1610]: discriminator_loss = 0.64174, target_loss = 0.40388, acc = 0.50000\n",
            "Epoch [2/5] Step [1556/1610]: discriminator_loss = 0.56858, target_loss = 0.39964, acc = 0.50000\n",
            "Epoch [2/5] Step [1557/1610]: discriminator_loss = 0.60600, target_loss = 0.38744, acc = 0.50000\n",
            "Epoch [2/5] Step [1558/1610]: discriminator_loss = 0.67088, target_loss = 0.39975, acc = 0.50000\n",
            "Epoch [2/5] Step [1559/1610]: discriminator_loss = 0.58653, target_loss = 0.40903, acc = 0.50000\n",
            "Epoch [2/5] Step [1560/1610]: discriminator_loss = 0.58988, target_loss = 0.40811, acc = 0.50000\n",
            "Epoch [2/5] Step [1561/1610]: discriminator_loss = 0.53720, target_loss = 0.44123, acc = 0.50000\n",
            "Epoch [2/5] Step [1562/1610]: discriminator_loss = 0.56788, target_loss = 0.41528, acc = 0.50000\n",
            "Epoch [2/5] Step [1563/1610]: discriminator_loss = 0.55075, target_loss = 0.42792, acc = 0.50000\n",
            "Epoch [2/5] Step [1564/1610]: discriminator_loss = 0.52928, target_loss = 0.42352, acc = 0.50000\n",
            "Epoch [2/5] Step [1565/1610]: discriminator_loss = 0.59683, target_loss = 0.40607, acc = 0.50000\n",
            "Epoch [2/5] Step [1566/1610]: discriminator_loss = 0.55420, target_loss = 0.40405, acc = 0.50000\n",
            "Epoch [2/5] Step [1567/1610]: discriminator_loss = 0.59865, target_loss = 0.40081, acc = 0.50000\n",
            "Epoch [2/5] Step [1568/1610]: discriminator_loss = 0.54704, target_loss = 0.40246, acc = 0.50000\n",
            "Epoch [2/5] Step [1569/1610]: discriminator_loss = 0.54177, target_loss = 0.41747, acc = 0.50000\n",
            "Epoch [2/5] Step [1570/1610]: discriminator_loss = 0.55067, target_loss = 0.41143, acc = 0.50000\n",
            "Epoch [2/5] Step [1571/1610]: discriminator_loss = 0.62456, target_loss = 0.39592, acc = 0.50000\n",
            "Epoch [2/5] Step [1572/1610]: discriminator_loss = 0.60021, target_loss = 0.40238, acc = 0.50000\n",
            "Epoch [2/5] Step [1573/1610]: discriminator_loss = 0.56390, target_loss = 0.40661, acc = 0.50000\n",
            "Epoch [2/5] Step [1574/1610]: discriminator_loss = 0.55827, target_loss = 0.33707, acc = 0.50000\n",
            "Epoch [2/5] Step [1575/1610]: discriminator_loss = 0.58691, target_loss = 0.37654, acc = 0.50000\n",
            "Epoch [2/5] Step [1576/1610]: discriminator_loss = 0.60608, target_loss = 0.36935, acc = 0.50000\n",
            "Epoch [2/5] Step [1577/1610]: discriminator_loss = 0.64873, target_loss = 0.37728, acc = 0.50000\n",
            "Epoch [2/5] Step [1578/1610]: discriminator_loss = 0.62039, target_loss = 0.34475, acc = 0.50000\n",
            "Epoch [2/5] Step [1579/1610]: discriminator_loss = 0.57940, target_loss = 0.38305, acc = 0.50000\n",
            "Epoch [2/5] Step [1580/1610]: discriminator_loss = 0.57766, target_loss = 0.38698, acc = 0.50000\n",
            "Epoch [2/5] Step [1581/1610]: discriminator_loss = 0.58669, target_loss = 0.34914, acc = 0.50000\n",
            "Epoch [2/5] Step [1582/1610]: discriminator_loss = 0.57037, target_loss = 0.36584, acc = 0.50000\n",
            "Epoch [2/5] Step [1583/1610]: discriminator_loss = 0.64617, target_loss = 0.36941, acc = 0.50000\n",
            "Epoch [2/5] Step [1584/1610]: discriminator_loss = 0.60191, target_loss = 0.42356, acc = 0.50000\n",
            "Epoch [2/5] Step [1585/1610]: discriminator_loss = 0.59915, target_loss = 0.40137, acc = 0.50000\n",
            "Epoch [2/5] Step [1586/1610]: discriminator_loss = 0.63015, target_loss = 0.42459, acc = 0.50000\n",
            "Epoch [2/5] Step [1587/1610]: discriminator_loss = 0.56857, target_loss = 0.45144, acc = 0.50000\n",
            "Epoch [2/5] Step [1588/1610]: discriminator_loss = 0.53630, target_loss = 0.45004, acc = 0.50000\n",
            "Epoch [2/5] Step [1589/1610]: discriminator_loss = 0.53429, target_loss = 0.47523, acc = 0.50000\n",
            "Epoch [2/5] Step [1590/1610]: discriminator_loss = 0.53505, target_loss = 0.42596, acc = 0.50000\n",
            "Epoch [2/5] Step [1591/1610]: discriminator_loss = 0.54807, target_loss = 0.43866, acc = 0.50000\n",
            "Epoch [2/5] Step [1592/1610]: discriminator_loss = 0.53153, target_loss = 0.44068, acc = 0.53125\n",
            "Epoch [2/5] Step [1593/1610]: discriminator_loss = 0.55608, target_loss = 0.39105, acc = 0.50000\n",
            "Epoch [2/5] Step [1594/1610]: discriminator_loss = 0.55937, target_loss = 0.41937, acc = 0.50000\n",
            "Epoch [2/5] Step [1595/1610]: discriminator_loss = 0.58968, target_loss = 0.37955, acc = 0.50000\n",
            "Epoch [2/5] Step [1596/1610]: discriminator_loss = 0.65305, target_loss = 0.36743, acc = 0.50000\n",
            "Epoch [2/5] Step [1597/1610]: discriminator_loss = 0.56676, target_loss = 0.39599, acc = 0.50000\n",
            "Epoch [2/5] Step [1598/1610]: discriminator_loss = 0.58379, target_loss = 0.35813, acc = 0.50000\n",
            "Epoch [2/5] Step [1599/1610]: discriminator_loss = 0.60152, target_loss = 0.35978, acc = 0.50000\n",
            "Epoch [2/5] Step [1600/1610]: discriminator_loss = 0.57721, target_loss = 0.36531, acc = 0.50000\n",
            "Epoch [2/5] Step [1601/1610]: discriminator_loss = 0.64253, target_loss = 0.37922, acc = 0.50000\n",
            "Epoch [2/5] Step [1602/1610]: discriminator_loss = 0.59245, target_loss = 0.33522, acc = 0.50000\n",
            "Epoch [2/5] Step [1603/1610]: discriminator_loss = 0.62816, target_loss = 0.34791, acc = 0.50000\n",
            "Epoch [2/5] Step [1604/1610]: discriminator_loss = 0.63570, target_loss = 0.38670, acc = 0.50000\n",
            "Epoch [2/5] Step [1605/1610]: discriminator_loss = 0.71501, target_loss = 0.40106, acc = 0.50000\n",
            "Epoch [2/5] Step [1606/1610]: discriminator_loss = 0.60438, target_loss = 0.37931, acc = 0.50000\n",
            "Epoch [2/5] Step [1607/1610]: discriminator_loss = 0.62684, target_loss = 0.40864, acc = 0.50000\n",
            "Epoch [2/5] Step [1608/1610]: discriminator_loss = 0.57637, target_loss = 0.39873, acc = 0.50000\n",
            "Epoch [2/5] Step [1609/1610]: discriminator_loss = 0.59288, target_loss = 0.38242, acc = 0.50000\n",
            "Epoch [2/5] Step [1610/1610]: discriminator_loss = 0.52022, target_loss = 0.43210, acc = 0.50000\n",
            "Epoch [3/5] Step [1/1610]: discriminator_loss = 0.52692, target_loss = 0.42193, acc = 0.50000\n",
            "Epoch [3/5] Step [2/1610]: discriminator_loss = 0.54172, target_loss = 0.42870, acc = 0.50000\n",
            "Epoch [3/5] Step [3/1610]: discriminator_loss = 0.63606, target_loss = 0.46875, acc = 0.50000\n",
            "Epoch [3/5] Step [4/1610]: discriminator_loss = 0.54490, target_loss = 0.43076, acc = 0.50000\n",
            "Epoch [3/5] Step [5/1610]: discriminator_loss = 0.58169, target_loss = 0.40990, acc = 0.50000\n",
            "Epoch [3/5] Step [6/1610]: discriminator_loss = 0.55396, target_loss = 0.41315, acc = 0.50000\n",
            "Epoch [3/5] Step [7/1610]: discriminator_loss = 0.55322, target_loss = 0.42571, acc = 0.50000\n",
            "Epoch [3/5] Step [8/1610]: discriminator_loss = 0.56160, target_loss = 0.40286, acc = 0.50000\n",
            "Epoch [3/5] Step [9/1610]: discriminator_loss = 0.56439, target_loss = 0.43359, acc = 0.50000\n",
            "Epoch [3/5] Step [10/1610]: discriminator_loss = 0.55537, target_loss = 0.39023, acc = 0.50000\n",
            "Epoch [3/5] Step [11/1610]: discriminator_loss = 0.57410, target_loss = 0.39546, acc = 0.50000\n",
            "Epoch [3/5] Step [12/1610]: discriminator_loss = 0.59447, target_loss = 0.40392, acc = 0.50000\n",
            "Epoch [3/5] Step [13/1610]: discriminator_loss = 0.57178, target_loss = 0.37914, acc = 0.50000\n",
            "Epoch [3/5] Step [14/1610]: discriminator_loss = 0.56877, target_loss = 0.36209, acc = 0.50000\n",
            "Epoch [3/5] Step [15/1610]: discriminator_loss = 0.58455, target_loss = 0.35085, acc = 0.50000\n",
            "Epoch [3/5] Step [16/1610]: discriminator_loss = 0.57470, target_loss = 0.39050, acc = 0.50000\n",
            "Epoch [3/5] Step [17/1610]: discriminator_loss = 0.57022, target_loss = 0.36923, acc = 0.50000\n",
            "Epoch [3/5] Step [18/1610]: discriminator_loss = 0.63319, target_loss = 0.39302, acc = 0.50000\n",
            "Epoch [3/5] Step [19/1610]: discriminator_loss = 0.61655, target_loss = 0.36997, acc = 0.50000\n",
            "Epoch [3/5] Step [20/1610]: discriminator_loss = 0.59683, target_loss = 0.39651, acc = 0.50000\n",
            "Epoch [3/5] Step [21/1610]: discriminator_loss = 0.62801, target_loss = 0.38533, acc = 0.50000\n",
            "Epoch [3/5] Step [22/1610]: discriminator_loss = 0.58897, target_loss = 0.38021, acc = 0.50000\n",
            "Epoch [3/5] Step [23/1610]: discriminator_loss = 0.62186, target_loss = 0.41116, acc = 0.50000\n",
            "Epoch [3/5] Step [24/1610]: discriminator_loss = 0.61097, target_loss = 0.40194, acc = 0.50000\n",
            "Epoch [3/5] Step [25/1610]: discriminator_loss = 0.55036, target_loss = 0.39591, acc = 0.50000\n",
            "Epoch [3/5] Step [26/1610]: discriminator_loss = 0.62886, target_loss = 0.39687, acc = 0.50000\n",
            "Epoch [3/5] Step [27/1610]: discriminator_loss = 0.63380, target_loss = 0.45752, acc = 0.50000\n",
            "Epoch [3/5] Step [28/1610]: discriminator_loss = 0.55360, target_loss = 0.46895, acc = 0.50000\n",
            "Epoch [3/5] Step [29/1610]: discriminator_loss = 0.56640, target_loss = 0.44433, acc = 0.53125\n",
            "Epoch [3/5] Step [30/1610]: discriminator_loss = 0.56547, target_loss = 0.44416, acc = 0.50000\n",
            "Epoch [3/5] Step [31/1610]: discriminator_loss = 0.60383, target_loss = 0.42065, acc = 0.50000\n",
            "Epoch [3/5] Step [32/1610]: discriminator_loss = 0.53444, target_loss = 0.43841, acc = 0.50000\n",
            "Epoch [3/5] Step [33/1610]: discriminator_loss = 0.55607, target_loss = 0.45155, acc = 0.50000\n",
            "Epoch [3/5] Step [34/1610]: discriminator_loss = 0.57747, target_loss = 0.41095, acc = 0.50000\n",
            "Epoch [3/5] Step [35/1610]: discriminator_loss = 0.54128, target_loss = 0.39302, acc = 0.53125\n",
            "Epoch [3/5] Step [36/1610]: discriminator_loss = 0.58338, target_loss = 0.40739, acc = 0.50000\n",
            "Epoch [3/5] Step [37/1610]: discriminator_loss = 0.55985, target_loss = 0.39853, acc = 0.50000\n",
            "Epoch [3/5] Step [38/1610]: discriminator_loss = 0.61709, target_loss = 0.37901, acc = 0.50000\n",
            "Epoch [3/5] Step [39/1610]: discriminator_loss = 0.60673, target_loss = 0.39082, acc = 0.50000\n",
            "Epoch [3/5] Step [40/1610]: discriminator_loss = 0.60295, target_loss = 0.36582, acc = 0.50000\n",
            "Epoch [3/5] Step [41/1610]: discriminator_loss = 0.59699, target_loss = 0.32571, acc = 0.50000\n",
            "Epoch [3/5] Step [42/1610]: discriminator_loss = 0.66351, target_loss = 0.35102, acc = 0.50000\n",
            "Epoch [3/5] Step [43/1610]: discriminator_loss = 0.69846, target_loss = 0.33933, acc = 0.50000\n",
            "Epoch [3/5] Step [44/1610]: discriminator_loss = 0.59023, target_loss = 0.36405, acc = 0.50000\n",
            "Epoch [3/5] Step [45/1610]: discriminator_loss = 0.61638, target_loss = 0.34801, acc = 0.50000\n",
            "Epoch [3/5] Step [46/1610]: discriminator_loss = 0.63905, target_loss = 0.37932, acc = 0.50000\n",
            "Epoch [3/5] Step [47/1610]: discriminator_loss = 0.58727, target_loss = 0.39741, acc = 0.50000\n",
            "Epoch [3/5] Step [48/1610]: discriminator_loss = 0.65428, target_loss = 0.37698, acc = 0.50000\n",
            "Epoch [3/5] Step [49/1610]: discriminator_loss = 0.59009, target_loss = 0.40865, acc = 0.50000\n",
            "Epoch [3/5] Step [50/1610]: discriminator_loss = 0.54968, target_loss = 0.38953, acc = 0.50000\n",
            "Epoch [3/5] Step [51/1610]: discriminator_loss = 0.58125, target_loss = 0.40486, acc = 0.50000\n",
            "Epoch [3/5] Step [52/1610]: discriminator_loss = 0.59055, target_loss = 0.41244, acc = 0.50000\n",
            "Epoch [3/5] Step [53/1610]: discriminator_loss = 0.60887, target_loss = 0.36132, acc = 0.50000\n",
            "Epoch [3/5] Step [54/1610]: discriminator_loss = 0.56136, target_loss = 0.39039, acc = 0.50000\n",
            "Epoch [3/5] Step [55/1610]: discriminator_loss = 0.54900, target_loss = 0.40431, acc = 0.50000\n",
            "Epoch [3/5] Step [56/1610]: discriminator_loss = 0.58294, target_loss = 0.37666, acc = 0.50000\n",
            "Epoch [3/5] Step [57/1610]: discriminator_loss = 0.59142, target_loss = 0.38309, acc = 0.50000\n",
            "Epoch [3/5] Step [58/1610]: discriminator_loss = 0.59408, target_loss = 0.39436, acc = 0.50000\n",
            "Epoch [3/5] Step [59/1610]: discriminator_loss = 0.59199, target_loss = 0.40648, acc = 0.50000\n",
            "Epoch [3/5] Step [60/1610]: discriminator_loss = 0.57943, target_loss = 0.34362, acc = 0.50000\n",
            "Epoch [3/5] Step [61/1610]: discriminator_loss = 0.64048, target_loss = 0.39634, acc = 0.50000\n",
            "Epoch [3/5] Step [62/1610]: discriminator_loss = 0.60413, target_loss = 0.39267, acc = 0.53125\n",
            "Epoch [3/5] Step [63/1610]: discriminator_loss = 0.57930, target_loss = 0.41613, acc = 0.50000\n",
            "Epoch [3/5] Step [64/1610]: discriminator_loss = 0.53277, target_loss = 0.43785, acc = 0.50000\n",
            "Epoch [3/5] Step [65/1610]: discriminator_loss = 0.52255, target_loss = 0.45288, acc = 0.50000\n",
            "Epoch [3/5] Step [66/1610]: discriminator_loss = 0.53745, target_loss = 0.42505, acc = 0.50000\n",
            "Epoch [3/5] Step [67/1610]: discriminator_loss = 0.52741, target_loss = 0.36634, acc = 0.50000\n",
            "Epoch [3/5] Step [68/1610]: discriminator_loss = 0.55975, target_loss = 0.39859, acc = 0.50000\n",
            "Epoch [3/5] Step [69/1610]: discriminator_loss = 0.59782, target_loss = 0.35422, acc = 0.50000\n",
            "Epoch [3/5] Step [70/1610]: discriminator_loss = 0.65994, target_loss = 0.38788, acc = 0.50000\n",
            "Epoch [3/5] Step [71/1610]: discriminator_loss = 0.61244, target_loss = 0.39021, acc = 0.50000\n",
            "Epoch [3/5] Step [72/1610]: discriminator_loss = 0.50437, target_loss = 0.45698, acc = 0.50000\n",
            "Epoch [3/5] Step [73/1610]: discriminator_loss = 0.57823, target_loss = 0.40594, acc = 0.50000\n",
            "Epoch [3/5] Step [74/1610]: discriminator_loss = 0.51852, target_loss = 0.46706, acc = 0.50000\n",
            "Epoch [3/5] Step [75/1610]: discriminator_loss = 0.56342, target_loss = 0.43435, acc = 0.50000\n",
            "Epoch [3/5] Step [76/1610]: discriminator_loss = 0.59677, target_loss = 0.39343, acc = 0.50000\n",
            "Epoch [3/5] Step [77/1610]: discriminator_loss = 0.52700, target_loss = 0.44918, acc = 0.50000\n",
            "Epoch [3/5] Step [78/1610]: discriminator_loss = 0.60238, target_loss = 0.42791, acc = 0.50000\n",
            "Epoch [3/5] Step [79/1610]: discriminator_loss = 0.58370, target_loss = 0.40813, acc = 0.53125\n",
            "Epoch [3/5] Step [80/1610]: discriminator_loss = 0.57969, target_loss = 0.40127, acc = 0.50000\n",
            "Epoch [3/5] Step [81/1610]: discriminator_loss = 0.58267, target_loss = 0.41274, acc = 0.50000\n",
            "Epoch [3/5] Step [82/1610]: discriminator_loss = 0.55219, target_loss = 0.38560, acc = 0.50000\n",
            "Epoch [3/5] Step [83/1610]: discriminator_loss = 0.61071, target_loss = 0.38178, acc = 0.50000\n",
            "Epoch [3/5] Step [84/1610]: discriminator_loss = 0.59266, target_loss = 0.40152, acc = 0.50000\n",
            "Epoch [3/5] Step [85/1610]: discriminator_loss = 0.56971, target_loss = 0.39954, acc = 0.50000\n",
            "Epoch [3/5] Step [86/1610]: discriminator_loss = 0.63462, target_loss = 0.37940, acc = 0.50000\n",
            "Epoch [3/5] Step [87/1610]: discriminator_loss = 0.59729, target_loss = 0.34265, acc = 0.50000\n",
            "Epoch [3/5] Step [88/1610]: discriminator_loss = 0.59342, target_loss = 0.40004, acc = 0.50000\n",
            "Epoch [3/5] Step [89/1610]: discriminator_loss = 0.59445, target_loss = 0.38808, acc = 0.50000\n",
            "Epoch [3/5] Step [90/1610]: discriminator_loss = 0.58210, target_loss = 0.38740, acc = 0.50000\n",
            "Epoch [3/5] Step [91/1610]: discriminator_loss = 0.59985, target_loss = 0.36944, acc = 0.50000\n",
            "Epoch [3/5] Step [92/1610]: discriminator_loss = 0.61848, target_loss = 0.36885, acc = 0.50000\n",
            "Epoch [3/5] Step [93/1610]: discriminator_loss = 0.58352, target_loss = 0.36287, acc = 0.50000\n",
            "Epoch [3/5] Step [94/1610]: discriminator_loss = 0.59297, target_loss = 0.36179, acc = 0.50000\n",
            "Epoch [3/5] Step [95/1610]: discriminator_loss = 0.61914, target_loss = 0.36547, acc = 0.50000\n",
            "Epoch [3/5] Step [96/1610]: discriminator_loss = 0.60354, target_loss = 0.35537, acc = 0.50000\n",
            "Epoch [3/5] Step [97/1610]: discriminator_loss = 0.61470, target_loss = 0.35672, acc = 0.50000\n",
            "Epoch [3/5] Step [98/1610]: discriminator_loss = 0.59858, target_loss = 0.38369, acc = 0.50000\n",
            "Epoch [3/5] Step [99/1610]: discriminator_loss = 0.63196, target_loss = 0.37414, acc = 0.50000\n",
            "Epoch [3/5] Step [100/1610]: discriminator_loss = 0.64982, target_loss = 0.36736, acc = 0.50000\n",
            "Epoch [3/5] Step [101/1610]: discriminator_loss = 0.59599, target_loss = 0.37656, acc = 0.50000\n",
            "Epoch [3/5] Step [102/1610]: discriminator_loss = 0.57012, target_loss = 0.43877, acc = 0.50000\n",
            "Epoch [3/5] Step [103/1610]: discriminator_loss = 0.55399, target_loss = 0.43896, acc = 0.50000\n",
            "Epoch [3/5] Step [104/1610]: discriminator_loss = 0.58787, target_loss = 0.41566, acc = 0.50000\n",
            "Epoch [3/5] Step [105/1610]: discriminator_loss = 0.54457, target_loss = 0.41788, acc = 0.50000\n",
            "Epoch [3/5] Step [106/1610]: discriminator_loss = 0.55430, target_loss = 0.43276, acc = 0.50000\n",
            "Epoch [3/5] Step [107/1610]: discriminator_loss = 0.57075, target_loss = 0.40592, acc = 0.50000\n",
            "Epoch [3/5] Step [108/1610]: discriminator_loss = 0.54128, target_loss = 0.41868, acc = 0.50000\n",
            "Epoch [3/5] Step [109/1610]: discriminator_loss = 0.51922, target_loss = 0.44282, acc = 0.53125\n",
            "Epoch [3/5] Step [110/1610]: discriminator_loss = 0.51584, target_loss = 0.42311, acc = 0.50000\n",
            "Epoch [3/5] Step [111/1610]: discriminator_loss = 0.61527, target_loss = 0.42624, acc = 0.50000\n",
            "Epoch [3/5] Step [112/1610]: discriminator_loss = 0.61836, target_loss = 0.42517, acc = 0.50000\n",
            "Epoch [3/5] Step [113/1610]: discriminator_loss = 0.56297, target_loss = 0.40000, acc = 0.50000\n",
            "Epoch [3/5] Step [114/1610]: discriminator_loss = 0.59492, target_loss = 0.40007, acc = 0.50000\n",
            "Epoch [3/5] Step [115/1610]: discriminator_loss = 0.59057, target_loss = 0.37784, acc = 0.50000\n",
            "Epoch [3/5] Step [116/1610]: discriminator_loss = 0.54273, target_loss = 0.40421, acc = 0.50000\n",
            "Epoch [3/5] Step [117/1610]: discriminator_loss = 0.55858, target_loss = 0.41520, acc = 0.50000\n",
            "Epoch [3/5] Step [118/1610]: discriminator_loss = 0.59335, target_loss = 0.38090, acc = 0.50000\n",
            "Epoch [3/5] Step [119/1610]: discriminator_loss = 0.57023, target_loss = 0.39195, acc = 0.50000\n",
            "Epoch [3/5] Step [120/1610]: discriminator_loss = 0.62668, target_loss = 0.37817, acc = 0.50000\n",
            "Epoch [3/5] Step [121/1610]: discriminator_loss = 0.58782, target_loss = 0.38645, acc = 0.50000\n",
            "Epoch [3/5] Step [122/1610]: discriminator_loss = 0.57134, target_loss = 0.40344, acc = 0.50000\n",
            "Epoch [3/5] Step [123/1610]: discriminator_loss = 0.58526, target_loss = 0.36708, acc = 0.50000\n",
            "Epoch [3/5] Step [124/1610]: discriminator_loss = 0.65208, target_loss = 0.38631, acc = 0.50000\n",
            "Epoch [3/5] Step [125/1610]: discriminator_loss = 0.57840, target_loss = 0.39480, acc = 0.50000\n",
            "Epoch [3/5] Step [126/1610]: discriminator_loss = 0.62841, target_loss = 0.40881, acc = 0.50000\n",
            "Epoch [3/5] Step [127/1610]: discriminator_loss = 0.57547, target_loss = 0.41057, acc = 0.50000\n",
            "Epoch [3/5] Step [128/1610]: discriminator_loss = 0.58234, target_loss = 0.37645, acc = 0.50000\n",
            "Epoch [3/5] Step [129/1610]: discriminator_loss = 0.58593, target_loss = 0.40572, acc = 0.50000\n",
            "Epoch [3/5] Step [130/1610]: discriminator_loss = 0.55735, target_loss = 0.40481, acc = 0.50000\n",
            "Epoch [3/5] Step [131/1610]: discriminator_loss = 0.57294, target_loss = 0.40377, acc = 0.50000\n",
            "Epoch [3/5] Step [132/1610]: discriminator_loss = 0.58908, target_loss = 0.41702, acc = 0.50000\n",
            "Epoch [3/5] Step [133/1610]: discriminator_loss = 0.58042, target_loss = 0.40467, acc = 0.50000\n",
            "Epoch [3/5] Step [134/1610]: discriminator_loss = 0.57056, target_loss = 0.39489, acc = 0.50000\n",
            "Epoch [3/5] Step [135/1610]: discriminator_loss = 0.62777, target_loss = 0.39410, acc = 0.50000\n",
            "Epoch [3/5] Step [136/1610]: discriminator_loss = 0.54948, target_loss = 0.40132, acc = 0.50000\n",
            "Epoch [3/5] Step [137/1610]: discriminator_loss = 0.56618, target_loss = 0.40642, acc = 0.50000\n",
            "Epoch [3/5] Step [138/1610]: discriminator_loss = 0.58301, target_loss = 0.37878, acc = 0.50000\n",
            "Epoch [3/5] Step [139/1610]: discriminator_loss = 0.58797, target_loss = 0.36907, acc = 0.50000\n",
            "Epoch [3/5] Step [140/1610]: discriminator_loss = 0.57960, target_loss = 0.36413, acc = 0.50000\n",
            "Epoch [3/5] Step [141/1610]: discriminator_loss = 0.58016, target_loss = 0.41054, acc = 0.50000\n",
            "Epoch [3/5] Step [142/1610]: discriminator_loss = 0.62383, target_loss = 0.35476, acc = 0.50000\n",
            "Epoch [3/5] Step [143/1610]: discriminator_loss = 0.57732, target_loss = 0.37063, acc = 0.50000\n",
            "Epoch [3/5] Step [144/1610]: discriminator_loss = 0.59392, target_loss = 0.34177, acc = 0.50000\n",
            "Epoch [3/5] Step [145/1610]: discriminator_loss = 0.63515, target_loss = 0.36450, acc = 0.50000\n",
            "Epoch [3/5] Step [146/1610]: discriminator_loss = 0.63407, target_loss = 0.36121, acc = 0.50000\n",
            "Epoch [3/5] Step [147/1610]: discriminator_loss = 0.60981, target_loss = 0.39827, acc = 0.50000\n",
            "Epoch [3/5] Step [148/1610]: discriminator_loss = 0.56874, target_loss = 0.45406, acc = 0.50000\n",
            "Epoch [3/5] Step [149/1610]: discriminator_loss = 0.55657, target_loss = 0.39321, acc = 0.50000\n",
            "Epoch [3/5] Step [150/1610]: discriminator_loss = 0.57466, target_loss = 0.40111, acc = 0.50000\n",
            "Epoch [3/5] Step [151/1610]: discriminator_loss = 0.56714, target_loss = 0.41709, acc = 0.50000\n",
            "Epoch [3/5] Step [152/1610]: discriminator_loss = 0.57058, target_loss = 0.41632, acc = 0.50000\n",
            "Epoch [3/5] Step [153/1610]: discriminator_loss = 0.61546, target_loss = 0.41396, acc = 0.50000\n",
            "Epoch [3/5] Step [154/1610]: discriminator_loss = 0.57864, target_loss = 0.41814, acc = 0.50000\n",
            "Epoch [3/5] Step [155/1610]: discriminator_loss = 0.54905, target_loss = 0.45582, acc = 0.53125\n",
            "Epoch [3/5] Step [156/1610]: discriminator_loss = 0.54137, target_loss = 0.42075, acc = 0.50000\n",
            "Epoch [3/5] Step [157/1610]: discriminator_loss = 0.53870, target_loss = 0.41813, acc = 0.50000\n",
            "Epoch [3/5] Step [158/1610]: discriminator_loss = 0.56894, target_loss = 0.40377, acc = 0.50000\n",
            "Epoch [3/5] Step [159/1610]: discriminator_loss = 0.54985, target_loss = 0.44812, acc = 0.50000\n",
            "Epoch [3/5] Step [160/1610]: discriminator_loss = 0.60322, target_loss = 0.41491, acc = 0.50000\n",
            "Epoch [3/5] Step [161/1610]: discriminator_loss = 0.56652, target_loss = 0.41000, acc = 0.50000\n",
            "Epoch [3/5] Step [162/1610]: discriminator_loss = 0.56376, target_loss = 0.39548, acc = 0.50000\n",
            "Epoch [3/5] Step [163/1610]: discriminator_loss = 0.57714, target_loss = 0.39637, acc = 0.50000\n",
            "Epoch [3/5] Step [164/1610]: discriminator_loss = 0.57590, target_loss = 0.40351, acc = 0.50000\n",
            "Epoch [3/5] Step [165/1610]: discriminator_loss = 0.58946, target_loss = 0.33853, acc = 0.50000\n",
            "Epoch [3/5] Step [166/1610]: discriminator_loss = 0.55865, target_loss = 0.39227, acc = 0.50000\n",
            "Epoch [3/5] Step [167/1610]: discriminator_loss = 0.58282, target_loss = 0.40592, acc = 0.50000\n",
            "Epoch [3/5] Step [168/1610]: discriminator_loss = 0.60862, target_loss = 0.35285, acc = 0.50000\n",
            "Epoch [3/5] Step [169/1610]: discriminator_loss = 0.61046, target_loss = 0.37946, acc = 0.50000\n",
            "Epoch [3/5] Step [170/1610]: discriminator_loss = 0.59267, target_loss = 0.37288, acc = 0.50000\n",
            "Epoch [3/5] Step [171/1610]: discriminator_loss = 0.59028, target_loss = 0.42175, acc = 0.53125\n",
            "Epoch [3/5] Step [172/1610]: discriminator_loss = 0.53883, target_loss = 0.37780, acc = 0.53125\n",
            "Epoch [3/5] Step [173/1610]: discriminator_loss = 0.58342, target_loss = 0.39246, acc = 0.50000\n",
            "Epoch [3/5] Step [174/1610]: discriminator_loss = 0.55872, target_loss = 0.39967, acc = 0.50000\n",
            "Epoch [3/5] Step [175/1610]: discriminator_loss = 0.58520, target_loss = 0.38554, acc = 0.50000\n",
            "Epoch [3/5] Step [176/1610]: discriminator_loss = 0.58608, target_loss = 0.40740, acc = 0.53125\n",
            "Epoch [3/5] Step [177/1610]: discriminator_loss = 0.59738, target_loss = 0.39301, acc = 0.50000\n",
            "Epoch [3/5] Step [178/1610]: discriminator_loss = 0.56627, target_loss = 0.41385, acc = 0.50000\n",
            "Epoch [3/5] Step [179/1610]: discriminator_loss = 0.57888, target_loss = 0.36830, acc = 0.50000\n",
            "Epoch [3/5] Step [180/1610]: discriminator_loss = 0.58374, target_loss = 0.37515, acc = 0.50000\n",
            "Epoch [3/5] Step [181/1610]: discriminator_loss = 0.55766, target_loss = 0.39630, acc = 0.50000\n",
            "Epoch [3/5] Step [182/1610]: discriminator_loss = 0.54664, target_loss = 0.40862, acc = 0.50000\n",
            "Epoch [3/5] Step [183/1610]: discriminator_loss = 0.59082, target_loss = 0.41330, acc = 0.50000\n",
            "Epoch [3/5] Step [184/1610]: discriminator_loss = 0.58791, target_loss = 0.39621, acc = 0.50000\n",
            "Epoch [3/5] Step [185/1610]: discriminator_loss = 0.60274, target_loss = 0.40591, acc = 0.50000\n",
            "Epoch [3/5] Step [186/1610]: discriminator_loss = 0.57405, target_loss = 0.42141, acc = 0.50000\n",
            "Epoch [3/5] Step [187/1610]: discriminator_loss = 0.59256, target_loss = 0.40814, acc = 0.50000\n",
            "Epoch [3/5] Step [188/1610]: discriminator_loss = 0.58779, target_loss = 0.37831, acc = 0.50000\n",
            "Epoch [3/5] Step [189/1610]: discriminator_loss = 0.57837, target_loss = 0.41026, acc = 0.50000\n",
            "Epoch [3/5] Step [190/1610]: discriminator_loss = 0.57260, target_loss = 0.44697, acc = 0.50000\n",
            "Epoch [3/5] Step [191/1610]: discriminator_loss = 0.56807, target_loss = 0.40269, acc = 0.50000\n",
            "Epoch [3/5] Step [192/1610]: discriminator_loss = 0.56928, target_loss = 0.39864, acc = 0.50000\n",
            "Epoch [3/5] Step [193/1610]: discriminator_loss = 0.53664, target_loss = 0.41761, acc = 0.50000\n",
            "Epoch [3/5] Step [194/1610]: discriminator_loss = 0.60812, target_loss = 0.39252, acc = 0.50000\n",
            "Epoch [3/5] Step [195/1610]: discriminator_loss = 0.57217, target_loss = 0.41626, acc = 0.50000\n",
            "Epoch [3/5] Step [196/1610]: discriminator_loss = 0.59098, target_loss = 0.36741, acc = 0.50000\n",
            "Epoch [3/5] Step [197/1610]: discriminator_loss = 0.61240, target_loss = 0.39310, acc = 0.50000\n",
            "Epoch [3/5] Step [198/1610]: discriminator_loss = 0.56292, target_loss = 0.40384, acc = 0.50000\n",
            "Epoch [3/5] Step [199/1610]: discriminator_loss = 0.59478, target_loss = 0.37065, acc = 0.50000\n",
            "Epoch [3/5] Step [200/1610]: discriminator_loss = 0.57793, target_loss = 0.37905, acc = 0.50000\n",
            "Epoch [3/5] Step [201/1610]: discriminator_loss = 0.60730, target_loss = 0.34997, acc = 0.50000\n",
            "Epoch [3/5] Step [202/1610]: discriminator_loss = 0.57547, target_loss = 0.37102, acc = 0.50000\n",
            "Epoch [3/5] Step [203/1610]: discriminator_loss = 0.61961, target_loss = 0.37021, acc = 0.50000\n",
            "Epoch [3/5] Step [204/1610]: discriminator_loss = 0.62311, target_loss = 0.36074, acc = 0.50000\n",
            "Epoch [3/5] Step [205/1610]: discriminator_loss = 0.63147, target_loss = 0.36207, acc = 0.50000\n",
            "Epoch [3/5] Step [206/1610]: discriminator_loss = 0.60870, target_loss = 0.34765, acc = 0.50000\n",
            "Epoch [3/5] Step [207/1610]: discriminator_loss = 0.60488, target_loss = 0.36087, acc = 0.50000\n",
            "Epoch [3/5] Step [208/1610]: discriminator_loss = 0.57645, target_loss = 0.35827, acc = 0.50000\n",
            "Epoch [3/5] Step [209/1610]: discriminator_loss = 0.59700, target_loss = 0.35463, acc = 0.50000\n",
            "Epoch [3/5] Step [210/1610]: discriminator_loss = 0.58708, target_loss = 0.40186, acc = 0.50000\n",
            "Epoch [3/5] Step [211/1610]: discriminator_loss = 0.59764, target_loss = 0.42603, acc = 0.50000\n",
            "Epoch [3/5] Step [212/1610]: discriminator_loss = 0.56490, target_loss = 0.41898, acc = 0.50000\n",
            "Epoch [3/5] Step [213/1610]: discriminator_loss = 0.61022, target_loss = 0.38979, acc = 0.50000\n",
            "Epoch [3/5] Step [214/1610]: discriminator_loss = 0.54099, target_loss = 0.38069, acc = 0.50000\n",
            "Epoch [3/5] Step [215/1610]: discriminator_loss = 0.57823, target_loss = 0.39048, acc = 0.50000\n",
            "Epoch [3/5] Step [216/1610]: discriminator_loss = 0.62072, target_loss = 0.43793, acc = 0.50000\n",
            "Epoch [3/5] Step [217/1610]: discriminator_loss = 0.59593, target_loss = 0.42363, acc = 0.50000\n",
            "Epoch [3/5] Step [218/1610]: discriminator_loss = 0.54830, target_loss = 0.47239, acc = 0.50000\n",
            "Epoch [3/5] Step [219/1610]: discriminator_loss = 0.54005, target_loss = 0.45869, acc = 0.50000\n",
            "Epoch [3/5] Step [220/1610]: discriminator_loss = 0.52123, target_loss = 0.44142, acc = 0.50000\n",
            "Epoch [3/5] Step [221/1610]: discriminator_loss = 0.52592, target_loss = 0.44037, acc = 0.50000\n",
            "Epoch [3/5] Step [222/1610]: discriminator_loss = 0.51937, target_loss = 0.43426, acc = 0.50000\n",
            "Epoch [3/5] Step [223/1610]: discriminator_loss = 0.53611, target_loss = 0.41740, acc = 0.50000\n",
            "Epoch [3/5] Step [224/1610]: discriminator_loss = 0.52479, target_loss = 0.41613, acc = 0.53125\n",
            "Epoch [3/5] Step [225/1610]: discriminator_loss = 0.57359, target_loss = 0.39947, acc = 0.50000\n",
            "Epoch [3/5] Step [226/1610]: discriminator_loss = 0.53758, target_loss = 0.41401, acc = 0.50000\n",
            "Epoch [3/5] Step [227/1610]: discriminator_loss = 0.52762, target_loss = 0.36397, acc = 0.50000\n",
            "Epoch [3/5] Step [228/1610]: discriminator_loss = 0.64537, target_loss = 0.39875, acc = 0.50000\n",
            "Epoch [3/5] Step [229/1610]: discriminator_loss = 0.63007, target_loss = 0.35717, acc = 0.50000\n",
            "Epoch [3/5] Step [230/1610]: discriminator_loss = 0.59925, target_loss = 0.37326, acc = 0.50000\n",
            "Epoch [3/5] Step [231/1610]: discriminator_loss = 0.58605, target_loss = 0.38920, acc = 0.50000\n",
            "Epoch [3/5] Step [232/1610]: discriminator_loss = 0.64026, target_loss = 0.36835, acc = 0.50000\n",
            "Epoch [3/5] Step [233/1610]: discriminator_loss = 0.61788, target_loss = 0.38098, acc = 0.50000\n",
            "Epoch [3/5] Step [234/1610]: discriminator_loss = 0.60202, target_loss = 0.38398, acc = 0.50000\n",
            "Epoch [3/5] Step [235/1610]: discriminator_loss = 0.57599, target_loss = 0.37960, acc = 0.50000\n",
            "Epoch [3/5] Step [236/1610]: discriminator_loss = 0.57736, target_loss = 0.41617, acc = 0.50000\n",
            "Epoch [3/5] Step [237/1610]: discriminator_loss = 0.58159, target_loss = 0.37458, acc = 0.50000\n",
            "Epoch [3/5] Step [238/1610]: discriminator_loss = 0.62572, target_loss = 0.40054, acc = 0.50000\n",
            "Epoch [3/5] Step [239/1610]: discriminator_loss = 0.59565, target_loss = 0.39368, acc = 0.50000\n",
            "Epoch [3/5] Step [240/1610]: discriminator_loss = 0.59439, target_loss = 0.39643, acc = 0.50000\n",
            "Epoch [3/5] Step [241/1610]: discriminator_loss = 0.56338, target_loss = 0.39849, acc = 0.50000\n",
            "Epoch [3/5] Step [242/1610]: discriminator_loss = 0.59021, target_loss = 0.40200, acc = 0.50000\n",
            "Epoch [3/5] Step [243/1610]: discriminator_loss = 0.56903, target_loss = 0.39426, acc = 0.50000\n",
            "Epoch [3/5] Step [244/1610]: discriminator_loss = 0.59398, target_loss = 0.36925, acc = 0.50000\n",
            "Epoch [3/5] Step [245/1610]: discriminator_loss = 0.59096, target_loss = 0.38500, acc = 0.50000\n",
            "Epoch [3/5] Step [246/1610]: discriminator_loss = 0.59358, target_loss = 0.39850, acc = 0.50000\n",
            "Epoch [3/5] Step [247/1610]: discriminator_loss = 0.59323, target_loss = 0.39800, acc = 0.50000\n",
            "Epoch [3/5] Step [248/1610]: discriminator_loss = 0.58572, target_loss = 0.38308, acc = 0.50000\n",
            "Epoch [3/5] Step [249/1610]: discriminator_loss = 0.55165, target_loss = 0.42133, acc = 0.50000\n",
            "Epoch [3/5] Step [250/1610]: discriminator_loss = 0.57181, target_loss = 0.37313, acc = 0.50000\n",
            "Epoch [3/5] Step [251/1610]: discriminator_loss = 0.57940, target_loss = 0.41533, acc = 0.50000\n",
            "Epoch [3/5] Step [252/1610]: discriminator_loss = 0.59658, target_loss = 0.44112, acc = 0.53125\n",
            "Epoch [3/5] Step [253/1610]: discriminator_loss = 0.54582, target_loss = 0.38497, acc = 0.50000\n",
            "Epoch [3/5] Step [254/1610]: discriminator_loss = 0.60516, target_loss = 0.38789, acc = 0.50000\n",
            "Epoch [3/5] Step [255/1610]: discriminator_loss = 0.58966, target_loss = 0.36363, acc = 0.50000\n",
            "Epoch [3/5] Step [256/1610]: discriminator_loss = 0.57766, target_loss = 0.39390, acc = 0.50000\n",
            "Epoch [3/5] Step [257/1610]: discriminator_loss = 0.64539, target_loss = 0.38936, acc = 0.50000\n",
            "Epoch [3/5] Step [258/1610]: discriminator_loss = 0.74213, target_loss = 0.41559, acc = 0.50000\n",
            "Epoch [3/5] Step [259/1610]: discriminator_loss = 0.58267, target_loss = 0.37779, acc = 0.50000\n",
            "Epoch [3/5] Step [260/1610]: discriminator_loss = 0.60265, target_loss = 0.40366, acc = 0.50000\n",
            "Epoch [3/5] Step [261/1610]: discriminator_loss = 0.55972, target_loss = 0.40816, acc = 0.50000\n",
            "Epoch [3/5] Step [262/1610]: discriminator_loss = 0.55945, target_loss = 0.42218, acc = 0.50000\n",
            "Epoch [3/5] Step [263/1610]: discriminator_loss = 0.53168, target_loss = 0.45234, acc = 0.50000\n",
            "Epoch [3/5] Step [264/1610]: discriminator_loss = 0.54805, target_loss = 0.40727, acc = 0.50000\n",
            "Epoch [3/5] Step [265/1610]: discriminator_loss = 0.56735, target_loss = 0.41299, acc = 0.50000\n",
            "Epoch [3/5] Step [266/1610]: discriminator_loss = 0.57864, target_loss = 0.41675, acc = 0.50000\n",
            "Epoch [3/5] Step [267/1610]: discriminator_loss = 0.58227, target_loss = 0.40594, acc = 0.50000\n",
            "Epoch [3/5] Step [268/1610]: discriminator_loss = 0.60402, target_loss = 0.41505, acc = 0.50000\n",
            "Epoch [3/5] Step [269/1610]: discriminator_loss = 0.61107, target_loss = 0.36683, acc = 0.50000\n",
            "Epoch [3/5] Step [270/1610]: discriminator_loss = 0.56750, target_loss = 0.42840, acc = 0.50000\n",
            "Epoch [3/5] Step [271/1610]: discriminator_loss = 0.57614, target_loss = 0.38601, acc = 0.50000\n",
            "Epoch [3/5] Step [272/1610]: discriminator_loss = 0.59503, target_loss = 0.38628, acc = 0.50000\n",
            "Epoch [3/5] Step [273/1610]: discriminator_loss = 0.58725, target_loss = 0.39664, acc = 0.50000\n",
            "Epoch [3/5] Step [274/1610]: discriminator_loss = 0.59971, target_loss = 0.36458, acc = 0.50000\n",
            "Epoch [3/5] Step [275/1610]: discriminator_loss = 0.61111, target_loss = 0.36591, acc = 0.50000\n",
            "Epoch [3/5] Step [276/1610]: discriminator_loss = 0.58862, target_loss = 0.35301, acc = 0.50000\n",
            "Epoch [3/5] Step [277/1610]: discriminator_loss = 0.63219, target_loss = 0.37018, acc = 0.50000\n",
            "Epoch [3/5] Step [278/1610]: discriminator_loss = 0.67223, target_loss = 0.39270, acc = 0.50000\n",
            "Epoch [3/5] Step [279/1610]: discriminator_loss = 0.61353, target_loss = 0.36452, acc = 0.50000\n",
            "Epoch [3/5] Step [280/1610]: discriminator_loss = 0.61114, target_loss = 0.36709, acc = 0.50000\n",
            "Epoch [3/5] Step [281/1610]: discriminator_loss = 0.61391, target_loss = 0.37018, acc = 0.50000\n",
            "Epoch [3/5] Step [282/1610]: discriminator_loss = 0.55853, target_loss = 0.39552, acc = 0.50000\n",
            "Epoch [3/5] Step [283/1610]: discriminator_loss = 0.56828, target_loss = 0.37163, acc = 0.50000\n",
            "Epoch [3/5] Step [284/1610]: discriminator_loss = 0.56611, target_loss = 0.39895, acc = 0.50000\n",
            "Epoch [3/5] Step [285/1610]: discriminator_loss = 0.61153, target_loss = 0.39753, acc = 0.50000\n",
            "Epoch [3/5] Step [286/1610]: discriminator_loss = 0.57958, target_loss = 0.39132, acc = 0.50000\n",
            "Epoch [3/5] Step [287/1610]: discriminator_loss = 0.58779, target_loss = 0.41783, acc = 0.50000\n",
            "Epoch [3/5] Step [288/1610]: discriminator_loss = 0.55284, target_loss = 0.40167, acc = 0.50000\n",
            "Epoch [3/5] Step [289/1610]: discriminator_loss = 0.56294, target_loss = 0.43810, acc = 0.50000\n",
            "Epoch [3/5] Step [290/1610]: discriminator_loss = 0.60835, target_loss = 0.38132, acc = 0.50000\n",
            "Epoch [3/5] Step [291/1610]: discriminator_loss = 0.54017, target_loss = 0.38279, acc = 0.50000\n",
            "Epoch [3/5] Step [292/1610]: discriminator_loss = 0.59733, target_loss = 0.38573, acc = 0.50000\n",
            "Epoch [3/5] Step [293/1610]: discriminator_loss = 0.54625, target_loss = 0.44203, acc = 0.50000\n",
            "Epoch [3/5] Step [294/1610]: discriminator_loss = 0.58119, target_loss = 0.42957, acc = 0.50000\n",
            "Epoch [3/5] Step [295/1610]: discriminator_loss = 0.56099, target_loss = 0.44612, acc = 0.50000\n",
            "Epoch [3/5] Step [296/1610]: discriminator_loss = 0.57706, target_loss = 0.41094, acc = 0.50000\n",
            "Epoch [3/5] Step [297/1610]: discriminator_loss = 0.56788, target_loss = 0.40671, acc = 0.50000\n",
            "Epoch [3/5] Step [298/1610]: discriminator_loss = 0.53746, target_loss = 0.41697, acc = 0.50000\n",
            "Epoch [3/5] Step [299/1610]: discriminator_loss = 0.57523, target_loss = 0.41019, acc = 0.50000\n",
            "Epoch [3/5] Step [300/1610]: discriminator_loss = 0.56943, target_loss = 0.42569, acc = 0.50000\n",
            "Epoch [3/5] Step [301/1610]: discriminator_loss = 0.57249, target_loss = 0.39807, acc = 0.50000\n",
            "Epoch [3/5] Step [302/1610]: discriminator_loss = 0.57586, target_loss = 0.41233, acc = 0.50000\n",
            "Epoch [3/5] Step [303/1610]: discriminator_loss = 0.58442, target_loss = 0.41721, acc = 0.50000\n",
            "Epoch [3/5] Step [304/1610]: discriminator_loss = 0.58935, target_loss = 0.38197, acc = 0.50000\n",
            "Epoch [3/5] Step [305/1610]: discriminator_loss = 0.57280, target_loss = 0.35479, acc = 0.50000\n",
            "Epoch [3/5] Step [306/1610]: discriminator_loss = 0.58932, target_loss = 0.39117, acc = 0.50000\n",
            "Epoch [3/5] Step [307/1610]: discriminator_loss = 0.63098, target_loss = 0.38238, acc = 0.50000\n",
            "Epoch [3/5] Step [308/1610]: discriminator_loss = 0.55838, target_loss = 0.39481, acc = 0.50000\n",
            "Epoch [3/5] Step [309/1610]: discriminator_loss = 0.59378, target_loss = 0.38870, acc = 0.50000\n",
            "Epoch [3/5] Step [310/1610]: discriminator_loss = 0.57291, target_loss = 0.39212, acc = 0.50000\n",
            "Epoch [3/5] Step [311/1610]: discriminator_loss = 0.61348, target_loss = 0.37711, acc = 0.50000\n",
            "Epoch [3/5] Step [312/1610]: discriminator_loss = 0.61991, target_loss = 0.39999, acc = 0.50000\n",
            "Epoch [3/5] Step [313/1610]: discriminator_loss = 0.58734, target_loss = 0.36160, acc = 0.50000\n",
            "Epoch [3/5] Step [314/1610]: discriminator_loss = 0.63635, target_loss = 0.36758, acc = 0.50000\n",
            "Epoch [3/5] Step [315/1610]: discriminator_loss = 0.57426, target_loss = 0.35555, acc = 0.50000\n",
            "Epoch [3/5] Step [316/1610]: discriminator_loss = 0.61440, target_loss = 0.42165, acc = 0.50000\n",
            "Epoch [3/5] Step [317/1610]: discriminator_loss = 0.61670, target_loss = 0.33551, acc = 0.50000\n",
            "Epoch [3/5] Step [318/1610]: discriminator_loss = 0.59890, target_loss = 0.39456, acc = 0.50000\n",
            "Epoch [3/5] Step [319/1610]: discriminator_loss = 0.59852, target_loss = 0.37371, acc = 0.50000\n",
            "Epoch [3/5] Step [320/1610]: discriminator_loss = 0.54620, target_loss = 0.40764, acc = 0.50000\n",
            "Epoch [3/5] Step [321/1610]: discriminator_loss = 0.55728, target_loss = 0.39638, acc = 0.50000\n",
            "Epoch [3/5] Step [322/1610]: discriminator_loss = 0.53394, target_loss = 0.42523, acc = 0.50000\n",
            "Epoch [3/5] Step [323/1610]: discriminator_loss = 0.57367, target_loss = 0.45328, acc = 0.50000\n",
            "Epoch [3/5] Step [324/1610]: discriminator_loss = 0.50781, target_loss = 0.43769, acc = 0.50000\n",
            "Epoch [3/5] Step [325/1610]: discriminator_loss = 0.55063, target_loss = 0.38957, acc = 0.50000\n",
            "Epoch [3/5] Step [326/1610]: discriminator_loss = 0.53634, target_loss = 0.39961, acc = 0.50000\n",
            "Epoch [3/5] Step [327/1610]: discriminator_loss = 0.55881, target_loss = 0.41847, acc = 0.50000\n",
            "Epoch [3/5] Step [328/1610]: discriminator_loss = 0.56575, target_loss = 0.37972, acc = 0.50000\n",
            "Epoch [3/5] Step [329/1610]: discriminator_loss = 0.54570, target_loss = 0.41844, acc = 0.50000\n",
            "Epoch [3/5] Step [330/1610]: discriminator_loss = 0.56960, target_loss = 0.39028, acc = 0.50000\n",
            "Epoch [3/5] Step [331/1610]: discriminator_loss = 0.56898, target_loss = 0.37227, acc = 0.50000\n",
            "Epoch [3/5] Step [332/1610]: discriminator_loss = 0.56809, target_loss = 0.39360, acc = 0.50000\n",
            "Epoch [3/5] Step [333/1610]: discriminator_loss = 0.59076, target_loss = 0.38063, acc = 0.50000\n",
            "Epoch [3/5] Step [334/1610]: discriminator_loss = 0.60593, target_loss = 0.37044, acc = 0.50000\n",
            "Epoch [3/5] Step [335/1610]: discriminator_loss = 0.56326, target_loss = 0.40190, acc = 0.50000\n",
            "Epoch [3/5] Step [336/1610]: discriminator_loss = 0.59611, target_loss = 0.37675, acc = 0.50000\n",
            "Epoch [3/5] Step [337/1610]: discriminator_loss = 0.57448, target_loss = 0.39662, acc = 0.50000\n",
            "Epoch [3/5] Step [338/1610]: discriminator_loss = 0.60987, target_loss = 0.39835, acc = 0.50000\n",
            "Epoch [3/5] Step [339/1610]: discriminator_loss = 0.57430, target_loss = 0.38938, acc = 0.50000\n",
            "Epoch [3/5] Step [340/1610]: discriminator_loss = 0.54082, target_loss = 0.37332, acc = 0.53125\n",
            "Epoch [3/5] Step [341/1610]: discriminator_loss = 0.58986, target_loss = 0.40653, acc = 0.50000\n",
            "Epoch [3/5] Step [342/1610]: discriminator_loss = 0.59921, target_loss = 0.41809, acc = 0.50000\n",
            "Epoch [3/5] Step [343/1610]: discriminator_loss = 0.57475, target_loss = 0.39268, acc = 0.50000\n",
            "Epoch [3/5] Step [344/1610]: discriminator_loss = 0.55793, target_loss = 0.42057, acc = 0.50000\n",
            "Epoch [3/5] Step [345/1610]: discriminator_loss = 0.59005, target_loss = 0.38590, acc = 0.50000\n",
            "Epoch [3/5] Step [346/1610]: discriminator_loss = 0.55129, target_loss = 0.41115, acc = 0.50000\n",
            "Epoch [3/5] Step [347/1610]: discriminator_loss = 0.55941, target_loss = 0.39473, acc = 0.50000\n",
            "Epoch [3/5] Step [348/1610]: discriminator_loss = 0.61534, target_loss = 0.40996, acc = 0.50000\n",
            "Epoch [3/5] Step [349/1610]: discriminator_loss = 0.57334, target_loss = 0.42024, acc = 0.50000\n",
            "Epoch [3/5] Step [350/1610]: discriminator_loss = 0.55525, target_loss = 0.39027, acc = 0.50000\n",
            "Epoch [3/5] Step [351/1610]: discriminator_loss = 0.57769, target_loss = 0.39428, acc = 0.50000\n",
            "Epoch [3/5] Step [352/1610]: discriminator_loss = 0.54126, target_loss = 0.39637, acc = 0.50000\n",
            "Epoch [3/5] Step [353/1610]: discriminator_loss = 0.57268, target_loss = 0.32419, acc = 0.50000\n",
            "Epoch [3/5] Step [354/1610]: discriminator_loss = 0.62622, target_loss = 0.37282, acc = 0.50000\n",
            "Epoch [3/5] Step [355/1610]: discriminator_loss = 0.61540, target_loss = 0.39188, acc = 0.50000\n",
            "Epoch [3/5] Step [356/1610]: discriminator_loss = 0.59455, target_loss = 0.35226, acc = 0.50000\n",
            "Epoch [3/5] Step [357/1610]: discriminator_loss = 0.61789, target_loss = 0.40094, acc = 0.50000\n",
            "Epoch [3/5] Step [358/1610]: discriminator_loss = 0.64430, target_loss = 0.36998, acc = 0.50000\n",
            "Epoch [3/5] Step [359/1610]: discriminator_loss = 0.59060, target_loss = 0.38479, acc = 0.50000\n",
            "Epoch [3/5] Step [360/1610]: discriminator_loss = 0.59881, target_loss = 0.41310, acc = 0.50000\n",
            "Epoch [3/5] Step [361/1610]: discriminator_loss = 0.62017, target_loss = 0.38864, acc = 0.50000\n",
            "Epoch [3/5] Step [362/1610]: discriminator_loss = 0.57825, target_loss = 0.43456, acc = 0.50000\n",
            "Epoch [3/5] Step [363/1610]: discriminator_loss = 0.55108, target_loss = 0.39371, acc = 0.50000\n",
            "Epoch [3/5] Step [364/1610]: discriminator_loss = 0.54890, target_loss = 0.40694, acc = 0.50000\n",
            "Epoch [3/5] Step [365/1610]: discriminator_loss = 0.54650, target_loss = 0.42237, acc = 0.50000\n",
            "Epoch [3/5] Step [366/1610]: discriminator_loss = 0.57133, target_loss = 0.44602, acc = 0.50000\n",
            "Epoch [3/5] Step [367/1610]: discriminator_loss = 0.51635, target_loss = 0.47459, acc = 0.50000\n",
            "Epoch [3/5] Step [368/1610]: discriminator_loss = 0.54015, target_loss = 0.46337, acc = 0.53125\n",
            "Epoch [3/5] Step [369/1610]: discriminator_loss = 0.55891, target_loss = 0.43209, acc = 0.50000\n",
            "Epoch [3/5] Step [370/1610]: discriminator_loss = 0.54920, target_loss = 0.42076, acc = 0.50000\n",
            "Epoch [3/5] Step [371/1610]: discriminator_loss = 0.58391, target_loss = 0.44519, acc = 0.50000\n",
            "Epoch [3/5] Step [372/1610]: discriminator_loss = 0.54190, target_loss = 0.42322, acc = 0.53125\n",
            "Epoch [3/5] Step [373/1610]: discriminator_loss = 0.54412, target_loss = 0.43125, acc = 0.50000\n",
            "Epoch [3/5] Step [374/1610]: discriminator_loss = 0.56947, target_loss = 0.39285, acc = 0.50000\n",
            "Epoch [3/5] Step [375/1610]: discriminator_loss = 0.59081, target_loss = 0.40073, acc = 0.50000\n",
            "Epoch [3/5] Step [376/1610]: discriminator_loss = 0.57462, target_loss = 0.40717, acc = 0.50000\n",
            "Epoch [3/5] Step [377/1610]: discriminator_loss = 0.58219, target_loss = 0.37578, acc = 0.50000\n",
            "Epoch [3/5] Step [378/1610]: discriminator_loss = 0.59182, target_loss = 0.38566, acc = 0.50000\n",
            "Epoch [3/5] Step [379/1610]: discriminator_loss = 0.60827, target_loss = 0.37726, acc = 0.50000\n",
            "Epoch [3/5] Step [380/1610]: discriminator_loss = 0.58998, target_loss = 0.35976, acc = 0.50000\n",
            "Epoch [3/5] Step [381/1610]: discriminator_loss = 0.60446, target_loss = 0.36149, acc = 0.50000\n",
            "Epoch [3/5] Step [382/1610]: discriminator_loss = 0.61126, target_loss = 0.35905, acc = 0.50000\n",
            "Epoch [3/5] Step [383/1610]: discriminator_loss = 0.60132, target_loss = 0.34084, acc = 0.50000\n",
            "Epoch [3/5] Step [384/1610]: discriminator_loss = 0.64581, target_loss = 0.33473, acc = 0.50000\n",
            "Epoch [3/5] Step [385/1610]: discriminator_loss = 0.68189, target_loss = 0.31603, acc = 0.50000\n",
            "Epoch [3/5] Step [386/1610]: discriminator_loss = 0.62773, target_loss = 0.34102, acc = 0.50000\n",
            "Epoch [3/5] Step [387/1610]: discriminator_loss = 0.57600, target_loss = 0.35166, acc = 0.50000\n",
            "Epoch [3/5] Step [388/1610]: discriminator_loss = 0.60922, target_loss = 0.38115, acc = 0.50000\n",
            "Epoch [3/5] Step [389/1610]: discriminator_loss = 0.56063, target_loss = 0.38381, acc = 0.50000\n",
            "Epoch [3/5] Step [390/1610]: discriminator_loss = 0.53133, target_loss = 0.37201, acc = 0.50000\n",
            "Epoch [3/5] Step [391/1610]: discriminator_loss = 0.60809, target_loss = 0.37878, acc = 0.50000\n",
            "Epoch [3/5] Step [392/1610]: discriminator_loss = 0.56226, target_loss = 0.41235, acc = 0.50000\n",
            "Epoch [3/5] Step [393/1610]: discriminator_loss = 0.57268, target_loss = 0.39191, acc = 0.50000\n",
            "Epoch [3/5] Step [394/1610]: discriminator_loss = 0.54795, target_loss = 0.44378, acc = 0.50000\n",
            "Epoch [3/5] Step [395/1610]: discriminator_loss = 0.57896, target_loss = 0.42477, acc = 0.50000\n",
            "Epoch [3/5] Step [396/1610]: discriminator_loss = 0.52287, target_loss = 0.44209, acc = 0.50000\n",
            "Epoch [3/5] Step [397/1610]: discriminator_loss = 0.52846, target_loss = 0.43348, acc = 0.50000\n",
            "Epoch [3/5] Step [398/1610]: discriminator_loss = 0.54562, target_loss = 0.42033, acc = 0.50000\n",
            "Epoch [3/5] Step [399/1610]: discriminator_loss = 0.50789, target_loss = 0.41230, acc = 0.50000\n",
            "Epoch [3/5] Step [400/1610]: discriminator_loss = 0.53057, target_loss = 0.37734, acc = 0.50000\n",
            "Epoch [3/5] Step [401/1610]: discriminator_loss = 0.59177, target_loss = 0.40557, acc = 0.50000\n",
            "Epoch [3/5] Step [402/1610]: discriminator_loss = 0.57866, target_loss = 0.40130, acc = 0.50000\n",
            "Epoch [3/5] Step [403/1610]: discriminator_loss = 0.61523, target_loss = 0.39572, acc = 0.50000\n",
            "Epoch [3/5] Step [404/1610]: discriminator_loss = 0.58989, target_loss = 0.43877, acc = 0.50000\n",
            "Epoch [3/5] Step [405/1610]: discriminator_loss = 0.59312, target_loss = 0.43237, acc = 0.50000\n",
            "Epoch [3/5] Step [406/1610]: discriminator_loss = 0.50225, target_loss = 0.40384, acc = 0.50000\n",
            "Epoch [3/5] Step [407/1610]: discriminator_loss = 0.56363, target_loss = 0.46348, acc = 0.50000\n",
            "Epoch [3/5] Step [408/1610]: discriminator_loss = 0.52482, target_loss = 0.40814, acc = 0.50000\n",
            "Epoch [3/5] Step [409/1610]: discriminator_loss = 0.59392, target_loss = 0.39311, acc = 0.50000\n",
            "Epoch [3/5] Step [410/1610]: discriminator_loss = 0.54507, target_loss = 0.42592, acc = 0.50000\n",
            "Epoch [3/5] Step [411/1610]: discriminator_loss = 0.59203, target_loss = 0.44444, acc = 0.50000\n",
            "Epoch [3/5] Step [412/1610]: discriminator_loss = 0.55725, target_loss = 0.36984, acc = 0.50000\n",
            "Epoch [3/5] Step [413/1610]: discriminator_loss = 0.53256, target_loss = 0.42411, acc = 0.50000\n",
            "Epoch [3/5] Step [414/1610]: discriminator_loss = 0.54558, target_loss = 0.39531, acc = 0.50000\n",
            "Epoch [3/5] Step [415/1610]: discriminator_loss = 0.55460, target_loss = 0.41120, acc = 0.50000\n",
            "Epoch [3/5] Step [416/1610]: discriminator_loss = 0.62602, target_loss = 0.40977, acc = 0.50000\n",
            "Epoch [3/5] Step [417/1610]: discriminator_loss = 0.57751, target_loss = 0.43792, acc = 0.50000\n",
            "Epoch [3/5] Step [418/1610]: discriminator_loss = 0.58432, target_loss = 0.38879, acc = 0.50000\n",
            "Epoch [3/5] Step [419/1610]: discriminator_loss = 0.58347, target_loss = 0.38363, acc = 0.50000\n",
            "Epoch [3/5] Step [420/1610]: discriminator_loss = 0.60422, target_loss = 0.39733, acc = 0.50000\n",
            "Epoch [3/5] Step [421/1610]: discriminator_loss = 0.61128, target_loss = 0.37460, acc = 0.50000\n",
            "Epoch [3/5] Step [422/1610]: discriminator_loss = 0.58225, target_loss = 0.39909, acc = 0.50000\n",
            "Epoch [3/5] Step [423/1610]: discriminator_loss = 0.61259, target_loss = 0.40316, acc = 0.50000\n",
            "Epoch [3/5] Step [424/1610]: discriminator_loss = 0.58953, target_loss = 0.36691, acc = 0.50000\n",
            "Epoch [3/5] Step [425/1610]: discriminator_loss = 0.69172, target_loss = 0.38679, acc = 0.50000\n",
            "Epoch [3/5] Step [426/1610]: discriminator_loss = 0.61602, target_loss = 0.39853, acc = 0.50000\n",
            "Epoch [3/5] Step [427/1610]: discriminator_loss = 0.58434, target_loss = 0.38481, acc = 0.50000\n",
            "Epoch [3/5] Step [428/1610]: discriminator_loss = 0.56827, target_loss = 0.39445, acc = 0.50000\n",
            "Epoch [3/5] Step [429/1610]: discriminator_loss = 0.58365, target_loss = 0.38843, acc = 0.50000\n",
            "Epoch [3/5] Step [430/1610]: discriminator_loss = 0.58023, target_loss = 0.41907, acc = 0.50000\n",
            "Epoch [3/5] Step [431/1610]: discriminator_loss = 0.56897, target_loss = 0.43795, acc = 0.50000\n",
            "Epoch [3/5] Step [432/1610]: discriminator_loss = 0.54488, target_loss = 0.40002, acc = 0.53125\n",
            "Epoch [3/5] Step [433/1610]: discriminator_loss = 0.55631, target_loss = 0.44429, acc = 0.50000\n",
            "Epoch [3/5] Step [434/1610]: discriminator_loss = 0.51915, target_loss = 0.43677, acc = 0.50000\n",
            "Epoch [3/5] Step [435/1610]: discriminator_loss = 0.55512, target_loss = 0.42568, acc = 0.50000\n",
            "Epoch [3/5] Step [436/1610]: discriminator_loss = 0.57772, target_loss = 0.41094, acc = 0.50000\n",
            "Epoch [3/5] Step [437/1610]: discriminator_loss = 0.55620, target_loss = 0.38257, acc = 0.53125\n",
            "Epoch [3/5] Step [438/1610]: discriminator_loss = 0.55436, target_loss = 0.36925, acc = 0.50000\n",
            "Epoch [3/5] Step [439/1610]: discriminator_loss = 0.56735, target_loss = 0.37030, acc = 0.50000\n",
            "Epoch [3/5] Step [440/1610]: discriminator_loss = 0.56038, target_loss = 0.37916, acc = 0.50000\n",
            "Epoch [3/5] Step [441/1610]: discriminator_loss = 0.67455, target_loss = 0.36676, acc = 0.50000\n",
            "Epoch [3/5] Step [442/1610]: discriminator_loss = 0.59392, target_loss = 0.36990, acc = 0.50000\n",
            "Epoch [3/5] Step [443/1610]: discriminator_loss = 0.63986, target_loss = 0.36465, acc = 0.50000\n",
            "Epoch [3/5] Step [444/1610]: discriminator_loss = 0.65920, target_loss = 0.38992, acc = 0.50000\n",
            "Epoch [3/5] Step [445/1610]: discriminator_loss = 0.60424, target_loss = 0.39139, acc = 0.50000\n",
            "Epoch [3/5] Step [446/1610]: discriminator_loss = 0.57815, target_loss = 0.39160, acc = 0.50000\n",
            "Epoch [3/5] Step [447/1610]: discriminator_loss = 0.57371, target_loss = 0.40570, acc = 0.50000\n",
            "Epoch [3/5] Step [448/1610]: discriminator_loss = 0.54477, target_loss = 0.39418, acc = 0.50000\n",
            "Epoch [3/5] Step [449/1610]: discriminator_loss = 0.57010, target_loss = 0.40007, acc = 0.50000\n",
            "Epoch [3/5] Step [450/1610]: discriminator_loss = 0.56122, target_loss = 0.44897, acc = 0.50000\n",
            "Epoch [3/5] Step [451/1610]: discriminator_loss = 0.55685, target_loss = 0.44143, acc = 0.50000\n",
            "Epoch [3/5] Step [452/1610]: discriminator_loss = 0.54543, target_loss = 0.42457, acc = 0.50000\n",
            "Epoch [3/5] Step [453/1610]: discriminator_loss = 0.55744, target_loss = 0.40273, acc = 0.50000\n",
            "Epoch [3/5] Step [454/1610]: discriminator_loss = 0.54505, target_loss = 0.41985, acc = 0.50000\n",
            "Epoch [3/5] Step [455/1610]: discriminator_loss = 0.58997, target_loss = 0.36644, acc = 0.50000\n",
            "Epoch [3/5] Step [456/1610]: discriminator_loss = 0.58877, target_loss = 0.40175, acc = 0.50000\n",
            "Epoch [3/5] Step [457/1610]: discriminator_loss = 0.56977, target_loss = 0.39283, acc = 0.50000\n",
            "Epoch [3/5] Step [458/1610]: discriminator_loss = 0.57399, target_loss = 0.36959, acc = 0.50000\n",
            "Epoch [3/5] Step [459/1610]: discriminator_loss = 0.54943, target_loss = 0.40178, acc = 0.50000\n",
            "Epoch [3/5] Step [460/1610]: discriminator_loss = 0.56583, target_loss = 0.37991, acc = 0.50000\n",
            "Epoch [3/5] Step [461/1610]: discriminator_loss = 0.58537, target_loss = 0.37665, acc = 0.50000\n",
            "Epoch [3/5] Step [462/1610]: discriminator_loss = 0.55343, target_loss = 0.37574, acc = 0.50000\n",
            "Epoch [3/5] Step [463/1610]: discriminator_loss = 0.57571, target_loss = 0.38697, acc = 0.50000\n",
            "Epoch [3/5] Step [464/1610]: discriminator_loss = 0.59428, target_loss = 0.38164, acc = 0.50000\n",
            "Epoch [3/5] Step [465/1610]: discriminator_loss = 0.59164, target_loss = 0.38974, acc = 0.50000\n",
            "Epoch [3/5] Step [466/1610]: discriminator_loss = 0.57361, target_loss = 0.40568, acc = 0.50000\n",
            "Epoch [3/5] Step [467/1610]: discriminator_loss = 0.58977, target_loss = 0.36309, acc = 0.50000\n",
            "Epoch [3/5] Step [468/1610]: discriminator_loss = 0.66054, target_loss = 0.40789, acc = 0.50000\n",
            "Epoch [3/5] Step [469/1610]: discriminator_loss = 0.60096, target_loss = 0.39205, acc = 0.50000\n",
            "Epoch [3/5] Step [470/1610]: discriminator_loss = 0.55496, target_loss = 0.38663, acc = 0.50000\n",
            "Epoch [3/5] Step [471/1610]: discriminator_loss = 0.58814, target_loss = 0.39087, acc = 0.50000\n",
            "Epoch [3/5] Step [472/1610]: discriminator_loss = 0.65054, target_loss = 0.39236, acc = 0.50000\n",
            "Epoch [3/5] Step [473/1610]: discriminator_loss = 0.54165, target_loss = 0.40556, acc = 0.50000\n",
            "Epoch [3/5] Step [474/1610]: discriminator_loss = 0.61063, target_loss = 0.41940, acc = 0.50000\n",
            "Epoch [3/5] Step [475/1610]: discriminator_loss = 0.60417, target_loss = 0.39268, acc = 0.50000\n",
            "Epoch [3/5] Step [476/1610]: discriminator_loss = 0.60266, target_loss = 0.37855, acc = 0.50000\n",
            "Epoch [3/5] Step [477/1610]: discriminator_loss = 0.57782, target_loss = 0.42302, acc = 0.50000\n",
            "Epoch [3/5] Step [478/1610]: discriminator_loss = 0.55224, target_loss = 0.42410, acc = 0.50000\n",
            "Epoch [3/5] Step [479/1610]: discriminator_loss = 0.53661, target_loss = 0.41905, acc = 0.50000\n",
            "Epoch [3/5] Step [480/1610]: discriminator_loss = 0.57763, target_loss = 0.40918, acc = 0.50000\n",
            "Epoch [3/5] Step [481/1610]: discriminator_loss = 0.58370, target_loss = 0.42073, acc = 0.50000\n",
            "Epoch [3/5] Step [482/1610]: discriminator_loss = 0.54189, target_loss = 0.40781, acc = 0.50000\n",
            "Epoch [3/5] Step [483/1610]: discriminator_loss = 0.54414, target_loss = 0.41034, acc = 0.50000\n",
            "Epoch [3/5] Step [484/1610]: discriminator_loss = 0.63968, target_loss = 0.40402, acc = 0.50000\n",
            "Epoch [3/5] Step [485/1610]: discriminator_loss = 0.55612, target_loss = 0.41539, acc = 0.50000\n",
            "Epoch [3/5] Step [486/1610]: discriminator_loss = 0.56657, target_loss = 0.38509, acc = 0.50000\n",
            "Epoch [3/5] Step [487/1610]: discriminator_loss = 0.56773, target_loss = 0.40949, acc = 0.50000\n",
            "Epoch [3/5] Step [488/1610]: discriminator_loss = 0.56139, target_loss = 0.42156, acc = 0.50000\n",
            "Epoch [3/5] Step [489/1610]: discriminator_loss = 0.54926, target_loss = 0.41269, acc = 0.50000\n",
            "Epoch [3/5] Step [490/1610]: discriminator_loss = 0.60838, target_loss = 0.40495, acc = 0.50000\n",
            "Epoch [3/5] Step [491/1610]: discriminator_loss = 0.55289, target_loss = 0.40246, acc = 0.50000\n",
            "Epoch [3/5] Step [492/1610]: discriminator_loss = 0.55097, target_loss = 0.41742, acc = 0.50000\n",
            "Epoch [3/5] Step [493/1610]: discriminator_loss = 0.58036, target_loss = 0.38729, acc = 0.50000\n",
            "Epoch [3/5] Step [494/1610]: discriminator_loss = 0.55580, target_loss = 0.41292, acc = 0.50000\n",
            "Epoch [3/5] Step [495/1610]: discriminator_loss = 0.53965, target_loss = 0.35897, acc = 0.50000\n",
            "Epoch [3/5] Step [496/1610]: discriminator_loss = 0.59449, target_loss = 0.40079, acc = 0.50000\n",
            "Epoch [3/5] Step [497/1610]: discriminator_loss = 0.56916, target_loss = 0.38612, acc = 0.50000\n",
            "Epoch [3/5] Step [498/1610]: discriminator_loss = 0.55720, target_loss = 0.41641, acc = 0.50000\n",
            "Epoch [3/5] Step [499/1610]: discriminator_loss = 0.61242, target_loss = 0.39664, acc = 0.50000\n",
            "Epoch [3/5] Step [500/1610]: discriminator_loss = 0.58529, target_loss = 0.38411, acc = 0.50000\n",
            "Epoch [3/5] Step [501/1610]: discriminator_loss = 0.59267, target_loss = 0.38480, acc = 0.50000\n",
            "Epoch [3/5] Step [502/1610]: discriminator_loss = 0.57997, target_loss = 0.36982, acc = 0.50000\n",
            "Epoch [3/5] Step [503/1610]: discriminator_loss = 0.59564, target_loss = 0.40889, acc = 0.50000\n",
            "Epoch [3/5] Step [504/1610]: discriminator_loss = 0.58640, target_loss = 0.39907, acc = 0.50000\n",
            "Epoch [3/5] Step [505/1610]: discriminator_loss = 0.60042, target_loss = 0.36999, acc = 0.50000\n",
            "Epoch [3/5] Step [506/1610]: discriminator_loss = 0.57878, target_loss = 0.39624, acc = 0.50000\n",
            "Epoch [3/5] Step [507/1610]: discriminator_loss = 0.58126, target_loss = 0.37959, acc = 0.50000\n",
            "Epoch [3/5] Step [508/1610]: discriminator_loss = 0.57635, target_loss = 0.41022, acc = 0.50000\n",
            "Epoch [3/5] Step [509/1610]: discriminator_loss = 0.58966, target_loss = 0.39394, acc = 0.50000\n",
            "Epoch [3/5] Step [510/1610]: discriminator_loss = 0.61243, target_loss = 0.41280, acc = 0.50000\n",
            "Epoch [3/5] Step [511/1610]: discriminator_loss = 0.58623, target_loss = 0.42153, acc = 0.50000\n",
            "Epoch [3/5] Step [512/1610]: discriminator_loss = 0.55519, target_loss = 0.41772, acc = 0.50000\n",
            "Epoch [3/5] Step [513/1610]: discriminator_loss = 0.57777, target_loss = 0.39096, acc = 0.50000\n",
            "Epoch [3/5] Step [514/1610]: discriminator_loss = 0.54177, target_loss = 0.43661, acc = 0.50000\n",
            "Epoch [3/5] Step [515/1610]: discriminator_loss = 0.52561, target_loss = 0.41195, acc = 0.53125\n",
            "Epoch [3/5] Step [516/1610]: discriminator_loss = 0.57112, target_loss = 0.43394, acc = 0.50000\n",
            "Epoch [3/5] Step [517/1610]: discriminator_loss = 0.58855, target_loss = 0.42644, acc = 0.50000\n",
            "Epoch [3/5] Step [518/1610]: discriminator_loss = 0.57625, target_loss = 0.43012, acc = 0.50000\n",
            "Epoch [3/5] Step [519/1610]: discriminator_loss = 0.56857, target_loss = 0.42274, acc = 0.50000\n",
            "Epoch [3/5] Step [520/1610]: discriminator_loss = 0.55597, target_loss = 0.42341, acc = 0.50000\n",
            "Epoch [3/5] Step [521/1610]: discriminator_loss = 0.54307, target_loss = 0.41414, acc = 0.50000\n",
            "Epoch [3/5] Step [522/1610]: discriminator_loss = 0.52644, target_loss = 0.39015, acc = 0.50000\n",
            "Epoch [3/5] Step [523/1610]: discriminator_loss = 0.55317, target_loss = 0.43206, acc = 0.50000\n",
            "Epoch [3/5] Step [524/1610]: discriminator_loss = 0.58697, target_loss = 0.38564, acc = 0.50000\n",
            "Epoch [3/5] Step [525/1610]: discriminator_loss = 0.56223, target_loss = 0.38772, acc = 0.50000\n",
            "Epoch [3/5] Step [526/1610]: discriminator_loss = 0.54411, target_loss = 0.43137, acc = 0.50000\n",
            "Epoch [3/5] Step [527/1610]: discriminator_loss = 0.60769, target_loss = 0.40650, acc = 0.50000\n",
            "Epoch [3/5] Step [528/1610]: discriminator_loss = 0.57382, target_loss = 0.37510, acc = 0.50000\n",
            "Epoch [3/5] Step [529/1610]: discriminator_loss = 0.58273, target_loss = 0.36803, acc = 0.50000\n",
            "Epoch [3/5] Step [530/1610]: discriminator_loss = 0.65363, target_loss = 0.40063, acc = 0.50000\n",
            "Epoch [3/5] Step [531/1610]: discriminator_loss = 0.56396, target_loss = 0.40731, acc = 0.50000\n",
            "Epoch [3/5] Step [532/1610]: discriminator_loss = 0.59825, target_loss = 0.40070, acc = 0.50000\n",
            "Epoch [3/5] Step [533/1610]: discriminator_loss = 0.57766, target_loss = 0.37187, acc = 0.50000\n",
            "Epoch [3/5] Step [534/1610]: discriminator_loss = 0.57154, target_loss = 0.38383, acc = 0.50000\n",
            "Epoch [3/5] Step [535/1610]: discriminator_loss = 0.57081, target_loss = 0.38584, acc = 0.50000\n",
            "Epoch [3/5] Step [536/1610]: discriminator_loss = 0.57954, target_loss = 0.36055, acc = 0.50000\n",
            "Epoch [3/5] Step [537/1610]: discriminator_loss = 0.57855, target_loss = 0.35967, acc = 0.50000\n",
            "Epoch [3/5] Step [538/1610]: discriminator_loss = 0.56088, target_loss = 0.38321, acc = 0.50000\n",
            "Epoch [3/5] Step [539/1610]: discriminator_loss = 0.60284, target_loss = 0.38481, acc = 0.50000\n",
            "Epoch [3/5] Step [540/1610]: discriminator_loss = 0.58463, target_loss = 0.34852, acc = 0.50000\n",
            "Epoch [3/5] Step [541/1610]: discriminator_loss = 0.57853, target_loss = 0.37085, acc = 0.50000\n",
            "Epoch [3/5] Step [542/1610]: discriminator_loss = 0.58500, target_loss = 0.36809, acc = 0.50000\n",
            "Epoch [3/5] Step [543/1610]: discriminator_loss = 0.59564, target_loss = 0.34699, acc = 0.50000\n",
            "Epoch [3/5] Step [544/1610]: discriminator_loss = 0.56417, target_loss = 0.37581, acc = 0.50000\n",
            "Epoch [3/5] Step [545/1610]: discriminator_loss = 0.71350, target_loss = 0.34289, acc = 0.50000\n",
            "Epoch [3/5] Step [546/1610]: discriminator_loss = 0.59318, target_loss = 0.41749, acc = 0.50000\n",
            "Epoch [3/5] Step [547/1610]: discriminator_loss = 0.63144, target_loss = 0.36011, acc = 0.50000\n",
            "Epoch [3/5] Step [548/1610]: discriminator_loss = 0.55600, target_loss = 0.35315, acc = 0.50000\n",
            "Epoch [3/5] Step [549/1610]: discriminator_loss = 0.58006, target_loss = 0.42325, acc = 0.50000\n",
            "Epoch [3/5] Step [550/1610]: discriminator_loss = 0.59251, target_loss = 0.42653, acc = 0.50000\n",
            "Epoch [3/5] Step [551/1610]: discriminator_loss = 0.53878, target_loss = 0.45497, acc = 0.50000\n",
            "Epoch [3/5] Step [552/1610]: discriminator_loss = 0.56264, target_loss = 0.47705, acc = 0.50000\n",
            "Epoch [3/5] Step [553/1610]: discriminator_loss = 0.49897, target_loss = 0.49030, acc = 0.50000\n",
            "Epoch [3/5] Step [554/1610]: discriminator_loss = 0.51983, target_loss = 0.43923, acc = 0.50000\n",
            "Epoch [3/5] Step [555/1610]: discriminator_loss = 0.51657, target_loss = 0.43818, acc = 0.50000\n",
            "Epoch [3/5] Step [556/1610]: discriminator_loss = 0.51453, target_loss = 0.43889, acc = 0.50000\n",
            "Epoch [3/5] Step [557/1610]: discriminator_loss = 0.50266, target_loss = 0.43231, acc = 0.50000\n",
            "Epoch [3/5] Step [558/1610]: discriminator_loss = 0.53633, target_loss = 0.42559, acc = 0.50000\n",
            "Epoch [3/5] Step [559/1610]: discriminator_loss = 0.59532, target_loss = 0.36545, acc = 0.50000\n",
            "Epoch [3/5] Step [560/1610]: discriminator_loss = 0.52343, target_loss = 0.44592, acc = 0.50000\n",
            "Epoch [3/5] Step [561/1610]: discriminator_loss = 0.53152, target_loss = 0.39856, acc = 0.50000\n",
            "Epoch [3/5] Step [562/1610]: discriminator_loss = 0.52727, target_loss = 0.41367, acc = 0.50000\n",
            "Epoch [3/5] Step [563/1610]: discriminator_loss = 0.62606, target_loss = 0.38248, acc = 0.50000\n",
            "Epoch [3/5] Step [564/1610]: discriminator_loss = 0.58334, target_loss = 0.33723, acc = 0.50000\n",
            "Epoch [3/5] Step [565/1610]: discriminator_loss = 0.57680, target_loss = 0.38717, acc = 0.50000\n",
            "Epoch [3/5] Step [566/1610]: discriminator_loss = 0.62240, target_loss = 0.34576, acc = 0.50000\n",
            "Epoch [3/5] Step [567/1610]: discriminator_loss = 0.58086, target_loss = 0.39564, acc = 0.50000\n",
            "Epoch [3/5] Step [568/1610]: discriminator_loss = 0.58383, target_loss = 0.37821, acc = 0.50000\n",
            "Epoch [3/5] Step [569/1610]: discriminator_loss = 0.59365, target_loss = 0.39787, acc = 0.50000\n",
            "Epoch [3/5] Step [570/1610]: discriminator_loss = 0.57641, target_loss = 0.38083, acc = 0.50000\n",
            "Epoch [3/5] Step [571/1610]: discriminator_loss = 0.56110, target_loss = 0.40215, acc = 0.50000\n",
            "Epoch [3/5] Step [572/1610]: discriminator_loss = 0.67007, target_loss = 0.37198, acc = 0.50000\n",
            "Epoch [3/5] Step [573/1610]: discriminator_loss = 0.58740, target_loss = 0.38191, acc = 0.50000\n",
            "Epoch [3/5] Step [574/1610]: discriminator_loss = 0.56494, target_loss = 0.38427, acc = 0.50000\n",
            "Epoch [3/5] Step [575/1610]: discriminator_loss = 0.55671, target_loss = 0.41039, acc = 0.50000\n",
            "Epoch [3/5] Step [576/1610]: discriminator_loss = 0.57365, target_loss = 0.37248, acc = 0.50000\n",
            "Epoch [3/5] Step [577/1610]: discriminator_loss = 0.57794, target_loss = 0.39546, acc = 0.50000\n",
            "Epoch [3/5] Step [578/1610]: discriminator_loss = 0.56974, target_loss = 0.42380, acc = 0.50000\n",
            "Epoch [3/5] Step [579/1610]: discriminator_loss = 0.55772, target_loss = 0.40781, acc = 0.50000\n",
            "Epoch [3/5] Step [580/1610]: discriminator_loss = 0.55661, target_loss = 0.41282, acc = 0.50000\n",
            "Epoch [3/5] Step [581/1610]: discriminator_loss = 0.57674, target_loss = 0.41735, acc = 0.50000\n",
            "Epoch [3/5] Step [582/1610]: discriminator_loss = 0.57449, target_loss = 0.45612, acc = 0.50000\n",
            "Epoch [3/5] Step [583/1610]: discriminator_loss = 0.59583, target_loss = 0.41269, acc = 0.50000\n",
            "Epoch [3/5] Step [584/1610]: discriminator_loss = 0.57505, target_loss = 0.40124, acc = 0.50000\n",
            "Epoch [3/5] Step [585/1610]: discriminator_loss = 0.55665, target_loss = 0.42608, acc = 0.50000\n",
            "Epoch [3/5] Step [586/1610]: discriminator_loss = 0.59121, target_loss = 0.42106, acc = 0.50000\n",
            "Epoch [3/5] Step [587/1610]: discriminator_loss = 0.58837, target_loss = 0.41974, acc = 0.50000\n",
            "Epoch [3/5] Step [588/1610]: discriminator_loss = 0.55872, target_loss = 0.38859, acc = 0.50000\n",
            "Epoch [3/5] Step [589/1610]: discriminator_loss = 0.58535, target_loss = 0.38809, acc = 0.50000\n",
            "Epoch [3/5] Step [590/1610]: discriminator_loss = 0.56775, target_loss = 0.39495, acc = 0.50000\n",
            "Epoch [3/5] Step [591/1610]: discriminator_loss = 0.57070, target_loss = 0.38996, acc = 0.50000\n",
            "Epoch [3/5] Step [592/1610]: discriminator_loss = 0.57153, target_loss = 0.39356, acc = 0.50000\n",
            "Epoch [3/5] Step [593/1610]: discriminator_loss = 0.60598, target_loss = 0.38323, acc = 0.50000\n",
            "Epoch [3/5] Step [594/1610]: discriminator_loss = 0.57469, target_loss = 0.35662, acc = 0.50000\n",
            "Epoch [3/5] Step [595/1610]: discriminator_loss = 0.56968, target_loss = 0.40460, acc = 0.50000\n",
            "Epoch [3/5] Step [596/1610]: discriminator_loss = 0.58098, target_loss = 0.39777, acc = 0.50000\n",
            "Epoch [3/5] Step [597/1610]: discriminator_loss = 0.60185, target_loss = 0.41050, acc = 0.50000\n",
            "Epoch [3/5] Step [598/1610]: discriminator_loss = 0.60463, target_loss = 0.38846, acc = 0.50000\n",
            "Epoch [3/5] Step [599/1610]: discriminator_loss = 0.57376, target_loss = 0.39885, acc = 0.50000\n",
            "Epoch [3/5] Step [600/1610]: discriminator_loss = 0.59161, target_loss = 0.38456, acc = 0.50000\n",
            "Epoch [3/5] Step [601/1610]: discriminator_loss = 0.59860, target_loss = 0.36896, acc = 0.50000\n",
            "Epoch [3/5] Step [602/1610]: discriminator_loss = 0.64077, target_loss = 0.38384, acc = 0.50000\n",
            "Epoch [3/5] Step [603/1610]: discriminator_loss = 0.61228, target_loss = 0.35537, acc = 0.50000\n",
            "Epoch [3/5] Step [604/1610]: discriminator_loss = 0.57255, target_loss = 0.39657, acc = 0.50000\n",
            "Epoch [3/5] Step [605/1610]: discriminator_loss = 0.58580, target_loss = 0.40237, acc = 0.50000\n",
            "Epoch [3/5] Step [606/1610]: discriminator_loss = 0.59195, target_loss = 0.41198, acc = 0.50000\n",
            "Epoch [3/5] Step [607/1610]: discriminator_loss = 0.55014, target_loss = 0.41859, acc = 0.50000\n",
            "Epoch [3/5] Step [608/1610]: discriminator_loss = 0.57232, target_loss = 0.38457, acc = 0.50000\n",
            "Epoch [3/5] Step [609/1610]: discriminator_loss = 0.57357, target_loss = 0.38865, acc = 0.50000\n",
            "Epoch [3/5] Step [610/1610]: discriminator_loss = 0.55173, target_loss = 0.41756, acc = 0.50000\n",
            "Epoch [3/5] Step [611/1610]: discriminator_loss = 0.54835, target_loss = 0.39622, acc = 0.53125\n",
            "Epoch [3/5] Step [612/1610]: discriminator_loss = 0.55764, target_loss = 0.39056, acc = 0.50000\n",
            "Epoch [3/5] Step [613/1610]: discriminator_loss = 0.55358, target_loss = 0.43680, acc = 0.50000\n",
            "Epoch [3/5] Step [614/1610]: discriminator_loss = 0.58547, target_loss = 0.39822, acc = 0.50000\n",
            "Epoch [3/5] Step [615/1610]: discriminator_loss = 0.59205, target_loss = 0.39991, acc = 0.50000\n",
            "Epoch [3/5] Step [616/1610]: discriminator_loss = 0.60578, target_loss = 0.38802, acc = 0.50000\n",
            "Epoch [3/5] Step [617/1610]: discriminator_loss = 0.55876, target_loss = 0.41032, acc = 0.50000\n",
            "Epoch [3/5] Step [618/1610]: discriminator_loss = 0.58625, target_loss = 0.37814, acc = 0.50000\n",
            "Epoch [3/5] Step [619/1610]: discriminator_loss = 0.56731, target_loss = 0.43324, acc = 0.50000\n",
            "Epoch [3/5] Step [620/1610]: discriminator_loss = 0.58261, target_loss = 0.39340, acc = 0.50000\n",
            "Epoch [3/5] Step [621/1610]: discriminator_loss = 0.56999, target_loss = 0.40275, acc = 0.50000\n",
            "Epoch [3/5] Step [622/1610]: discriminator_loss = 0.60084, target_loss = 0.39599, acc = 0.50000\n",
            "Epoch [3/5] Step [623/1610]: discriminator_loss = 0.56093, target_loss = 0.37636, acc = 0.50000\n",
            "Epoch [3/5] Step [624/1610]: discriminator_loss = 0.55614, target_loss = 0.41191, acc = 0.50000\n",
            "Epoch [3/5] Step [625/1610]: discriminator_loss = 0.57426, target_loss = 0.38400, acc = 0.50000\n",
            "Epoch [3/5] Step [626/1610]: discriminator_loss = 0.54382, target_loss = 0.40145, acc = 0.50000\n",
            "Epoch [3/5] Step [627/1610]: discriminator_loss = 0.57321, target_loss = 0.38320, acc = 0.50000\n",
            "Epoch [3/5] Step [628/1610]: discriminator_loss = 0.56605, target_loss = 0.42619, acc = 0.50000\n",
            "Epoch [3/5] Step [629/1610]: discriminator_loss = 0.57497, target_loss = 0.40406, acc = 0.50000\n",
            "Epoch [3/5] Step [630/1610]: discriminator_loss = 0.56041, target_loss = 0.40346, acc = 0.50000\n",
            "Epoch [3/5] Step [631/1610]: discriminator_loss = 0.57320, target_loss = 0.39877, acc = 0.50000\n",
            "Epoch [3/5] Step [632/1610]: discriminator_loss = 0.60103, target_loss = 0.41029, acc = 0.50000\n",
            "Epoch [3/5] Step [633/1610]: discriminator_loss = 0.58866, target_loss = 0.39282, acc = 0.50000\n",
            "Epoch [3/5] Step [634/1610]: discriminator_loss = 0.55694, target_loss = 0.41387, acc = 0.50000\n",
            "Epoch [3/5] Step [635/1610]: discriminator_loss = 0.56376, target_loss = 0.39937, acc = 0.50000\n",
            "Epoch [3/5] Step [636/1610]: discriminator_loss = 0.58452, target_loss = 0.37916, acc = 0.50000\n",
            "Epoch [3/5] Step [637/1610]: discriminator_loss = 0.60292, target_loss = 0.38497, acc = 0.50000\n",
            "Epoch [3/5] Step [638/1610]: discriminator_loss = 0.59471, target_loss = 0.40818, acc = 0.50000\n",
            "Epoch [3/5] Step [639/1610]: discriminator_loss = 0.56965, target_loss = 0.36342, acc = 0.50000\n",
            "Epoch [3/5] Step [640/1610]: discriminator_loss = 0.62330, target_loss = 0.38956, acc = 0.50000\n",
            "Epoch [3/5] Step [641/1610]: discriminator_loss = 0.57594, target_loss = 0.38852, acc = 0.50000\n",
            "Epoch [3/5] Step [642/1610]: discriminator_loss = 0.60887, target_loss = 0.36569, acc = 0.50000\n",
            "Epoch [3/5] Step [643/1610]: discriminator_loss = 0.58885, target_loss = 0.37758, acc = 0.50000\n",
            "Epoch [3/5] Step [644/1610]: discriminator_loss = 0.56764, target_loss = 0.39059, acc = 0.50000\n",
            "Epoch [3/5] Step [645/1610]: discriminator_loss = 0.57214, target_loss = 0.37615, acc = 0.50000\n",
            "Epoch [3/5] Step [646/1610]: discriminator_loss = 0.56582, target_loss = 0.41265, acc = 0.50000\n",
            "Epoch [3/5] Step [647/1610]: discriminator_loss = 0.58343, target_loss = 0.41451, acc = 0.50000\n",
            "Epoch [3/5] Step [648/1610]: discriminator_loss = 0.56014, target_loss = 0.40136, acc = 0.50000\n",
            "Epoch [3/5] Step [649/1610]: discriminator_loss = 0.54872, target_loss = 0.42656, acc = 0.50000\n",
            "Epoch [3/5] Step [650/1610]: discriminator_loss = 0.56364, target_loss = 0.41275, acc = 0.50000\n",
            "Epoch [3/5] Step [651/1610]: discriminator_loss = 0.60776, target_loss = 0.39248, acc = 0.50000\n",
            "Epoch [3/5] Step [652/1610]: discriminator_loss = 0.55049, target_loss = 0.43656, acc = 0.50000\n",
            "Epoch [3/5] Step [653/1610]: discriminator_loss = 0.53402, target_loss = 0.36769, acc = 0.50000\n",
            "Epoch [3/5] Step [654/1610]: discriminator_loss = 0.52734, target_loss = 0.39401, acc = 0.50000\n",
            "Epoch [3/5] Step [655/1610]: discriminator_loss = 0.59136, target_loss = 0.43231, acc = 0.50000\n",
            "Epoch [3/5] Step [656/1610]: discriminator_loss = 0.56944, target_loss = 0.41869, acc = 0.50000\n",
            "Epoch [3/5] Step [657/1610]: discriminator_loss = 0.65604, target_loss = 0.41540, acc = 0.50000\n",
            "Epoch [3/5] Step [658/1610]: discriminator_loss = 0.61541, target_loss = 0.38600, acc = 0.50000\n",
            "Epoch [3/5] Step [659/1610]: discriminator_loss = 0.57262, target_loss = 0.39650, acc = 0.50000\n",
            "Epoch [3/5] Step [660/1610]: discriminator_loss = 0.52761, target_loss = 0.43984, acc = 0.50000\n",
            "Epoch [3/5] Step [661/1610]: discriminator_loss = 0.57404, target_loss = 0.40597, acc = 0.50000\n",
            "Epoch [3/5] Step [662/1610]: discriminator_loss = 0.58575, target_loss = 0.40466, acc = 0.50000\n",
            "Epoch [3/5] Step [663/1610]: discriminator_loss = 0.52422, target_loss = 0.43657, acc = 0.50000\n",
            "Epoch [3/5] Step [664/1610]: discriminator_loss = 0.56481, target_loss = 0.39434, acc = 0.50000\n",
            "Epoch [3/5] Step [665/1610]: discriminator_loss = 0.55105, target_loss = 0.39383, acc = 0.50000\n",
            "Epoch [3/5] Step [666/1610]: discriminator_loss = 0.55303, target_loss = 0.42766, acc = 0.50000\n",
            "Epoch [3/5] Step [667/1610]: discriminator_loss = 0.54184, target_loss = 0.40906, acc = 0.50000\n",
            "Epoch [3/5] Step [668/1610]: discriminator_loss = 0.58880, target_loss = 0.39695, acc = 0.50000\n",
            "Epoch [3/5] Step [669/1610]: discriminator_loss = 0.57119, target_loss = 0.39887, acc = 0.50000\n",
            "Epoch [3/5] Step [670/1610]: discriminator_loss = 0.56406, target_loss = 0.40858, acc = 0.50000\n",
            "Epoch [3/5] Step [671/1610]: discriminator_loss = 0.60604, target_loss = 0.39803, acc = 0.50000\n",
            "Epoch [3/5] Step [672/1610]: discriminator_loss = 0.63060, target_loss = 0.41909, acc = 0.50000\n",
            "Epoch [3/5] Step [673/1610]: discriminator_loss = 0.62353, target_loss = 0.42147, acc = 0.50000\n",
            "Epoch [3/5] Step [674/1610]: discriminator_loss = 0.58142, target_loss = 0.38038, acc = 0.50000\n",
            "Epoch [3/5] Step [675/1610]: discriminator_loss = 0.57829, target_loss = 0.40004, acc = 0.50000\n",
            "Epoch [3/5] Step [676/1610]: discriminator_loss = 0.55829, target_loss = 0.42743, acc = 0.50000\n",
            "Epoch [3/5] Step [677/1610]: discriminator_loss = 0.55642, target_loss = 0.41710, acc = 0.50000\n",
            "Epoch [3/5] Step [678/1610]: discriminator_loss = 0.56287, target_loss = 0.40233, acc = 0.50000\n",
            "Epoch [3/5] Step [679/1610]: discriminator_loss = 0.57741, target_loss = 0.40121, acc = 0.50000\n",
            "Epoch [3/5] Step [680/1610]: discriminator_loss = 0.58738, target_loss = 0.41081, acc = 0.50000\n",
            "Epoch [3/5] Step [681/1610]: discriminator_loss = 0.59836, target_loss = 0.42328, acc = 0.50000\n",
            "Epoch [3/5] Step [682/1610]: discriminator_loss = 0.57906, target_loss = 0.40096, acc = 0.50000\n",
            "Epoch [3/5] Step [683/1610]: discriminator_loss = 0.57364, target_loss = 0.38524, acc = 0.50000\n",
            "Epoch [3/5] Step [684/1610]: discriminator_loss = 0.55656, target_loss = 0.42416, acc = 0.50000\n",
            "Epoch [3/5] Step [685/1610]: discriminator_loss = 0.56471, target_loss = 0.38415, acc = 0.50000\n",
            "Epoch [3/5] Step [686/1610]: discriminator_loss = 0.58110, target_loss = 0.38783, acc = 0.50000\n",
            "Epoch [3/5] Step [687/1610]: discriminator_loss = 0.59512, target_loss = 0.39383, acc = 0.50000\n",
            "Epoch [3/5] Step [688/1610]: discriminator_loss = 0.57606, target_loss = 0.39163, acc = 0.50000\n",
            "Epoch [3/5] Step [689/1610]: discriminator_loss = 0.60180, target_loss = 0.38058, acc = 0.50000\n",
            "Epoch [3/5] Step [690/1610]: discriminator_loss = 0.60087, target_loss = 0.36090, acc = 0.50000\n",
            "Epoch [3/5] Step [691/1610]: discriminator_loss = 0.61143, target_loss = 0.35749, acc = 0.50000\n",
            "Epoch [3/5] Step [692/1610]: discriminator_loss = 0.59405, target_loss = 0.39163, acc = 0.50000\n",
            "Epoch [3/5] Step [693/1610]: discriminator_loss = 0.65284, target_loss = 0.36534, acc = 0.50000\n",
            "Epoch [3/5] Step [694/1610]: discriminator_loss = 0.57412, target_loss = 0.37431, acc = 0.50000\n",
            "Epoch [3/5] Step [695/1610]: discriminator_loss = 0.57617, target_loss = 0.39802, acc = 0.50000\n",
            "Epoch [3/5] Step [696/1610]: discriminator_loss = 0.57494, target_loss = 0.39807, acc = 0.50000\n",
            "Epoch [3/5] Step [697/1610]: discriminator_loss = 0.56515, target_loss = 0.38499, acc = 0.50000\n",
            "Epoch [3/5] Step [698/1610]: discriminator_loss = 0.56994, target_loss = 0.39266, acc = 0.50000\n",
            "Epoch [3/5] Step [699/1610]: discriminator_loss = 0.61828, target_loss = 0.41755, acc = 0.50000\n",
            "Epoch [3/5] Step [700/1610]: discriminator_loss = 0.60319, target_loss = 0.40156, acc = 0.50000\n",
            "Epoch [3/5] Step [701/1610]: discriminator_loss = 0.55255, target_loss = 0.39487, acc = 0.50000\n",
            "Epoch [3/5] Step [702/1610]: discriminator_loss = 0.57596, target_loss = 0.40929, acc = 0.50000\n",
            "Epoch [3/5] Step [703/1610]: discriminator_loss = 0.57705, target_loss = 0.40433, acc = 0.50000\n",
            "Epoch [3/5] Step [704/1610]: discriminator_loss = 0.55698, target_loss = 0.42853, acc = 0.50000\n",
            "Epoch [3/5] Step [705/1610]: discriminator_loss = 0.58277, target_loss = 0.42547, acc = 0.50000\n",
            "Epoch [3/5] Step [706/1610]: discriminator_loss = 0.56071, target_loss = 0.42041, acc = 0.50000\n",
            "Epoch [3/5] Step [707/1610]: discriminator_loss = 0.55778, target_loss = 0.41917, acc = 0.50000\n",
            "Epoch [3/5] Step [708/1610]: discriminator_loss = 0.57518, target_loss = 0.42411, acc = 0.50000\n",
            "Epoch [3/5] Step [709/1610]: discriminator_loss = 0.57873, target_loss = 0.41589, acc = 0.50000\n",
            "Epoch [3/5] Step [710/1610]: discriminator_loss = 0.56365, target_loss = 0.41764, acc = 0.50000\n",
            "Epoch [3/5] Step [711/1610]: discriminator_loss = 0.59398, target_loss = 0.41825, acc = 0.50000\n",
            "Epoch [3/5] Step [712/1610]: discriminator_loss = 0.56682, target_loss = 0.41102, acc = 0.50000\n",
            "Epoch [3/5] Step [713/1610]: discriminator_loss = 0.55753, target_loss = 0.40056, acc = 0.50000\n",
            "Epoch [3/5] Step [714/1610]: discriminator_loss = 0.58042, target_loss = 0.40190, acc = 0.50000\n",
            "Epoch [3/5] Step [715/1610]: discriminator_loss = 0.58364, target_loss = 0.38419, acc = 0.50000\n",
            "Epoch [3/5] Step [716/1610]: discriminator_loss = 0.55869, target_loss = 0.40298, acc = 0.50000\n",
            "Epoch [3/5] Step [717/1610]: discriminator_loss = 0.61177, target_loss = 0.39869, acc = 0.50000\n",
            "Epoch [3/5] Step [718/1610]: discriminator_loss = 0.64105, target_loss = 0.35511, acc = 0.50000\n",
            "Epoch [3/5] Step [719/1610]: discriminator_loss = 0.59495, target_loss = 0.40644, acc = 0.50000\n",
            "Epoch [3/5] Step [720/1610]: discriminator_loss = 0.56761, target_loss = 0.37923, acc = 0.50000\n",
            "Epoch [3/5] Step [721/1610]: discriminator_loss = 0.56261, target_loss = 0.40418, acc = 0.50000\n",
            "Epoch [3/5] Step [722/1610]: discriminator_loss = 0.58384, target_loss = 0.37651, acc = 0.50000\n",
            "Epoch [3/5] Step [723/1610]: discriminator_loss = 0.59464, target_loss = 0.40437, acc = 0.50000\n",
            "Epoch [3/5] Step [724/1610]: discriminator_loss = 0.58778, target_loss = 0.39473, acc = 0.50000\n",
            "Epoch [3/5] Step [725/1610]: discriminator_loss = 0.57612, target_loss = 0.37340, acc = 0.50000\n",
            "Epoch [3/5] Step [726/1610]: discriminator_loss = 0.57797, target_loss = 0.38878, acc = 0.50000\n",
            "Epoch [3/5] Step [727/1610]: discriminator_loss = 0.60395, target_loss = 0.39645, acc = 0.50000\n",
            "Epoch [3/5] Step [728/1610]: discriminator_loss = 0.58655, target_loss = 0.39355, acc = 0.50000\n",
            "Epoch [3/5] Step [729/1610]: discriminator_loss = 0.59346, target_loss = 0.38054, acc = 0.50000\n",
            "Epoch [3/5] Step [730/1610]: discriminator_loss = 0.60331, target_loss = 0.36780, acc = 0.50000\n",
            "Epoch [3/5] Step [731/1610]: discriminator_loss = 0.58638, target_loss = 0.39098, acc = 0.50000\n",
            "Epoch [3/5] Step [732/1610]: discriminator_loss = 0.60684, target_loss = 0.38532, acc = 0.50000\n",
            "Epoch [3/5] Step [733/1610]: discriminator_loss = 0.56821, target_loss = 0.40032, acc = 0.50000\n",
            "Epoch [3/5] Step [734/1610]: discriminator_loss = 0.54857, target_loss = 0.40715, acc = 0.50000\n",
            "Epoch [3/5] Step [735/1610]: discriminator_loss = 0.55104, target_loss = 0.41048, acc = 0.53125\n",
            "Epoch [3/5] Step [736/1610]: discriminator_loss = 0.58878, target_loss = 0.41662, acc = 0.50000\n",
            "Epoch [3/5] Step [737/1610]: discriminator_loss = 0.57070, target_loss = 0.42919, acc = 0.50000\n",
            "Epoch [3/5] Step [738/1610]: discriminator_loss = 0.55141, target_loss = 0.42020, acc = 0.50000\n",
            "Epoch [3/5] Step [739/1610]: discriminator_loss = 0.54421, target_loss = 0.42450, acc = 0.50000\n",
            "Epoch [3/5] Step [740/1610]: discriminator_loss = 0.55027, target_loss = 0.40554, acc = 0.50000\n",
            "Epoch [3/5] Step [741/1610]: discriminator_loss = 0.55061, target_loss = 0.40410, acc = 0.50000\n",
            "Epoch [3/5] Step [742/1610]: discriminator_loss = 0.54922, target_loss = 0.38235, acc = 0.50000\n",
            "Epoch [3/5] Step [743/1610]: discriminator_loss = 0.61672, target_loss = 0.40715, acc = 0.50000\n",
            "Epoch [3/5] Step [744/1610]: discriminator_loss = 0.58319, target_loss = 0.39057, acc = 0.50000\n",
            "Epoch [3/5] Step [745/1610]: discriminator_loss = 0.55266, target_loss = 0.41970, acc = 0.50000\n",
            "Epoch [3/5] Step [746/1610]: discriminator_loss = 0.55260, target_loss = 0.39648, acc = 0.50000\n",
            "Epoch [3/5] Step [747/1610]: discriminator_loss = 0.61655, target_loss = 0.38952, acc = 0.50000\n",
            "Epoch [3/5] Step [748/1610]: discriminator_loss = 0.54708, target_loss = 0.42480, acc = 0.50000\n",
            "Epoch [3/5] Step [749/1610]: discriminator_loss = 0.54071, target_loss = 0.39226, acc = 0.50000\n",
            "Epoch [3/5] Step [750/1610]: discriminator_loss = 0.55625, target_loss = 0.42159, acc = 0.50000\n",
            "Epoch [3/5] Step [751/1610]: discriminator_loss = 0.56648, target_loss = 0.42134, acc = 0.50000\n",
            "Epoch [3/5] Step [752/1610]: discriminator_loss = 0.56013, target_loss = 0.39080, acc = 0.53125\n",
            "Epoch [3/5] Step [753/1610]: discriminator_loss = 0.54061, target_loss = 0.38517, acc = 0.50000\n",
            "Epoch [3/5] Step [754/1610]: discriminator_loss = 0.56966, target_loss = 0.42119, acc = 0.50000\n",
            "Epoch [3/5] Step [755/1610]: discriminator_loss = 0.54494, target_loss = 0.40623, acc = 0.50000\n",
            "Epoch [3/5] Step [756/1610]: discriminator_loss = 0.61029, target_loss = 0.41210, acc = 0.50000\n",
            "Epoch [3/5] Step [757/1610]: discriminator_loss = 0.55449, target_loss = 0.39633, acc = 0.50000\n",
            "Epoch [3/5] Step [758/1610]: discriminator_loss = 0.57286, target_loss = 0.40867, acc = 0.50000\n",
            "Epoch [3/5] Step [759/1610]: discriminator_loss = 0.57478, target_loss = 0.36362, acc = 0.50000\n",
            "Epoch [3/5] Step [760/1610]: discriminator_loss = 0.58267, target_loss = 0.34687, acc = 0.50000\n",
            "Epoch [3/5] Step [761/1610]: discriminator_loss = 0.58527, target_loss = 0.38664, acc = 0.50000\n",
            "Epoch [3/5] Step [762/1610]: discriminator_loss = 0.58492, target_loss = 0.38677, acc = 0.50000\n",
            "Epoch [3/5] Step [763/1610]: discriminator_loss = 0.55877, target_loss = 0.38040, acc = 0.50000\n",
            "Epoch [3/5] Step [764/1610]: discriminator_loss = 0.55997, target_loss = 0.37348, acc = 0.50000\n",
            "Epoch [3/5] Step [765/1610]: discriminator_loss = 0.59884, target_loss = 0.38789, acc = 0.50000\n",
            "Epoch [3/5] Step [766/1610]: discriminator_loss = 0.60844, target_loss = 0.41056, acc = 0.50000\n",
            "Epoch [3/5] Step [767/1610]: discriminator_loss = 0.57082, target_loss = 0.36134, acc = 0.50000\n",
            "Epoch [3/5] Step [768/1610]: discriminator_loss = 0.58095, target_loss = 0.42809, acc = 0.50000\n",
            "Epoch [3/5] Step [769/1610]: discriminator_loss = 0.66423, target_loss = 0.41383, acc = 0.50000\n",
            "Epoch [3/5] Step [770/1610]: discriminator_loss = 0.58127, target_loss = 0.43643, acc = 0.50000\n",
            "Epoch [3/5] Step [771/1610]: discriminator_loss = 0.54529, target_loss = 0.38607, acc = 0.50000\n",
            "Epoch [3/5] Step [772/1610]: discriminator_loss = 0.55950, target_loss = 0.43464, acc = 0.50000\n",
            "Epoch [3/5] Step [773/1610]: discriminator_loss = 0.56523, target_loss = 0.43181, acc = 0.50000\n",
            "Epoch [3/5] Step [774/1610]: discriminator_loss = 0.55745, target_loss = 0.43497, acc = 0.50000\n",
            "Epoch [3/5] Step [775/1610]: discriminator_loss = 0.52926, target_loss = 0.43685, acc = 0.50000\n",
            "Epoch [3/5] Step [776/1610]: discriminator_loss = 0.55013, target_loss = 0.43765, acc = 0.50000\n",
            "Epoch [3/5] Step [777/1610]: discriminator_loss = 0.54761, target_loss = 0.40648, acc = 0.50000\n",
            "Epoch [3/5] Step [778/1610]: discriminator_loss = 0.54677, target_loss = 0.41336, acc = 0.50000\n",
            "Epoch [3/5] Step [779/1610]: discriminator_loss = 0.54460, target_loss = 0.40077, acc = 0.53125\n",
            "Epoch [3/5] Step [780/1610]: discriminator_loss = 0.55878, target_loss = 0.40020, acc = 0.50000\n",
            "Epoch [3/5] Step [781/1610]: discriminator_loss = 0.57204, target_loss = 0.38931, acc = 0.50000\n",
            "Epoch [3/5] Step [782/1610]: discriminator_loss = 0.59338, target_loss = 0.37478, acc = 0.50000\n",
            "Epoch [3/5] Step [783/1610]: discriminator_loss = 0.60752, target_loss = 0.37580, acc = 0.50000\n",
            "Epoch [3/5] Step [784/1610]: discriminator_loss = 0.58580, target_loss = 0.36273, acc = 0.50000\n",
            "Epoch [3/5] Step [785/1610]: discriminator_loss = 0.56871, target_loss = 0.36402, acc = 0.50000\n",
            "Epoch [3/5] Step [786/1610]: discriminator_loss = 0.58227, target_loss = 0.37895, acc = 0.50000\n",
            "Epoch [3/5] Step [787/1610]: discriminator_loss = 0.62873, target_loss = 0.34971, acc = 0.50000\n",
            "Epoch [3/5] Step [788/1610]: discriminator_loss = 0.58386, target_loss = 0.34201, acc = 0.50000\n",
            "Epoch [3/5] Step [789/1610]: discriminator_loss = 0.64026, target_loss = 0.33906, acc = 0.50000\n",
            "Epoch [3/5] Step [790/1610]: discriminator_loss = 0.62951, target_loss = 0.37931, acc = 0.50000\n",
            "Epoch [3/5] Step [791/1610]: discriminator_loss = 0.57841, target_loss = 0.38165, acc = 0.50000\n",
            "Epoch [3/5] Step [792/1610]: discriminator_loss = 0.57421, target_loss = 0.38677, acc = 0.50000\n",
            "Epoch [3/5] Step [793/1610]: discriminator_loss = 0.61592, target_loss = 0.33909, acc = 0.50000\n",
            "Epoch [3/5] Step [794/1610]: discriminator_loss = 0.55629, target_loss = 0.42395, acc = 0.50000\n",
            "Epoch [3/5] Step [795/1610]: discriminator_loss = 0.66147, target_loss = 0.39474, acc = 0.50000\n",
            "Epoch [3/5] Step [796/1610]: discriminator_loss = 0.57997, target_loss = 0.43227, acc = 0.50000\n",
            "Epoch [3/5] Step [797/1610]: discriminator_loss = 0.53440, target_loss = 0.43365, acc = 0.50000\n",
            "Epoch [3/5] Step [798/1610]: discriminator_loss = 0.54313, target_loss = 0.45662, acc = 0.50000\n",
            "Epoch [3/5] Step [799/1610]: discriminator_loss = 0.52892, target_loss = 0.48013, acc = 0.50000\n",
            "Epoch [3/5] Step [800/1610]: discriminator_loss = 0.49357, target_loss = 0.47993, acc = 0.50000\n",
            "Epoch [3/5] Step [801/1610]: discriminator_loss = 0.56055, target_loss = 0.43008, acc = 0.50000\n",
            "Epoch [3/5] Step [802/1610]: discriminator_loss = 0.53146, target_loss = 0.42096, acc = 0.50000\n",
            "Epoch [3/5] Step [803/1610]: discriminator_loss = 0.52506, target_loss = 0.43858, acc = 0.50000\n",
            "Epoch [3/5] Step [804/1610]: discriminator_loss = 0.54266, target_loss = 0.43653, acc = 0.50000\n",
            "Epoch [3/5] Step [805/1610]: discriminator_loss = 0.54195, target_loss = 0.40436, acc = 0.50000\n",
            "Epoch [3/5] Step [806/1610]: discriminator_loss = 0.51892, target_loss = 0.41808, acc = 0.50000\n",
            "Epoch [3/5] Step [807/1610]: discriminator_loss = 0.58558, target_loss = 0.39266, acc = 0.50000\n",
            "Epoch [3/5] Step [808/1610]: discriminator_loss = 0.58752, target_loss = 0.40149, acc = 0.50000\n",
            "Epoch [3/5] Step [809/1610]: discriminator_loss = 0.58975, target_loss = 0.38488, acc = 0.50000\n",
            "Epoch [3/5] Step [810/1610]: discriminator_loss = 0.55655, target_loss = 0.40818, acc = 0.50000\n",
            "Epoch [3/5] Step [811/1610]: discriminator_loss = 0.60028, target_loss = 0.38454, acc = 0.50000\n",
            "Epoch [3/5] Step [812/1610]: discriminator_loss = 0.56993, target_loss = 0.40241, acc = 0.50000\n",
            "Epoch [3/5] Step [813/1610]: discriminator_loss = 0.58205, target_loss = 0.35806, acc = 0.50000\n",
            "Epoch [3/5] Step [814/1610]: discriminator_loss = 0.63025, target_loss = 0.35433, acc = 0.50000\n",
            "Epoch [3/5] Step [815/1610]: discriminator_loss = 0.58357, target_loss = 0.39533, acc = 0.50000\n",
            "Epoch [3/5] Step [816/1610]: discriminator_loss = 0.62966, target_loss = 0.39267, acc = 0.50000\n",
            "Epoch [3/5] Step [817/1610]: discriminator_loss = 0.67245, target_loss = 0.38225, acc = 0.50000\n",
            "Epoch [3/5] Step [818/1610]: discriminator_loss = 0.57692, target_loss = 0.39466, acc = 0.50000\n",
            "Epoch [3/5] Step [819/1610]: discriminator_loss = 0.57228, target_loss = 0.40578, acc = 0.50000\n",
            "Epoch [3/5] Step [820/1610]: discriminator_loss = 0.56306, target_loss = 0.39766, acc = 0.50000\n",
            "Epoch [3/5] Step [821/1610]: discriminator_loss = 0.60218, target_loss = 0.39694, acc = 0.50000\n",
            "Epoch [3/5] Step [822/1610]: discriminator_loss = 0.55051, target_loss = 0.39812, acc = 0.50000\n",
            "Epoch [3/5] Step [823/1610]: discriminator_loss = 0.55130, target_loss = 0.41936, acc = 0.50000\n",
            "Epoch [3/5] Step [824/1610]: discriminator_loss = 0.56783, target_loss = 0.42734, acc = 0.50000\n",
            "Epoch [3/5] Step [825/1610]: discriminator_loss = 0.56448, target_loss = 0.42022, acc = 0.50000\n",
            "Epoch [3/5] Step [826/1610]: discriminator_loss = 0.52373, target_loss = 0.44766, acc = 0.50000\n",
            "Epoch [3/5] Step [827/1610]: discriminator_loss = 0.53464, target_loss = 0.41310, acc = 0.50000\n",
            "Epoch [3/5] Step [828/1610]: discriminator_loss = 0.53301, target_loss = 0.42907, acc = 0.50000\n",
            "Epoch [3/5] Step [829/1610]: discriminator_loss = 0.59312, target_loss = 0.41790, acc = 0.50000\n",
            "Epoch [3/5] Step [830/1610]: discriminator_loss = 0.57021, target_loss = 0.39476, acc = 0.50000\n",
            "Epoch [3/5] Step [831/1610]: discriminator_loss = 0.56513, target_loss = 0.42067, acc = 0.50000\n",
            "Epoch [3/5] Step [832/1610]: discriminator_loss = 0.56472, target_loss = 0.40611, acc = 0.50000\n",
            "Epoch [3/5] Step [833/1610]: discriminator_loss = 0.58246, target_loss = 0.43717, acc = 0.50000\n",
            "Epoch [3/5] Step [834/1610]: discriminator_loss = 0.55655, target_loss = 0.39567, acc = 0.50000\n",
            "Epoch [3/5] Step [835/1610]: discriminator_loss = 0.58120, target_loss = 0.40510, acc = 0.50000\n",
            "Epoch [3/5] Step [836/1610]: discriminator_loss = 0.54015, target_loss = 0.38871, acc = 0.50000\n",
            "Epoch [3/5] Step [837/1610]: discriminator_loss = 0.56972, target_loss = 0.39198, acc = 0.50000\n",
            "Epoch [3/5] Step [838/1610]: discriminator_loss = 0.57642, target_loss = 0.38662, acc = 0.50000\n",
            "Epoch [3/5] Step [839/1610]: discriminator_loss = 0.57934, target_loss = 0.39954, acc = 0.50000\n",
            "Epoch [3/5] Step [840/1610]: discriminator_loss = 0.57834, target_loss = 0.37646, acc = 0.50000\n",
            "Epoch [3/5] Step [841/1610]: discriminator_loss = 0.57271, target_loss = 0.36561, acc = 0.50000\n",
            "Epoch [3/5] Step [842/1610]: discriminator_loss = 0.63017, target_loss = 0.36990, acc = 0.50000\n",
            "Epoch [3/5] Step [843/1610]: discriminator_loss = 0.58477, target_loss = 0.38888, acc = 0.50000\n",
            "Epoch [3/5] Step [844/1610]: discriminator_loss = 0.59536, target_loss = 0.39628, acc = 0.50000\n",
            "Epoch [3/5] Step [845/1610]: discriminator_loss = 0.56196, target_loss = 0.38370, acc = 0.50000\n",
            "Epoch [3/5] Step [846/1610]: discriminator_loss = 0.58802, target_loss = 0.40823, acc = 0.50000\n",
            "Epoch [3/5] Step [847/1610]: discriminator_loss = 0.59073, target_loss = 0.39481, acc = 0.50000\n",
            "Epoch [3/5] Step [848/1610]: discriminator_loss = 0.57215, target_loss = 0.38152, acc = 0.50000\n",
            "Epoch [3/5] Step [849/1610]: discriminator_loss = 0.61080, target_loss = 0.39204, acc = 0.50000\n",
            "Epoch [3/5] Step [850/1610]: discriminator_loss = 0.56160, target_loss = 0.38973, acc = 0.50000\n",
            "Epoch [3/5] Step [851/1610]: discriminator_loss = 0.53674, target_loss = 0.41713, acc = 0.50000\n",
            "Epoch [3/5] Step [852/1610]: discriminator_loss = 0.59030, target_loss = 0.38704, acc = 0.50000\n",
            "Epoch [3/5] Step [853/1610]: discriminator_loss = 0.59402, target_loss = 0.38931, acc = 0.50000\n",
            "Epoch [3/5] Step [854/1610]: discriminator_loss = 0.58705, target_loss = 0.38223, acc = 0.50000\n",
            "Epoch [3/5] Step [855/1610]: discriminator_loss = 0.57509, target_loss = 0.39142, acc = 0.50000\n",
            "Epoch [3/5] Step [856/1610]: discriminator_loss = 0.60508, target_loss = 0.36381, acc = 0.50000\n",
            "Epoch [3/5] Step [857/1610]: discriminator_loss = 0.56399, target_loss = 0.37395, acc = 0.50000\n",
            "Epoch [3/5] Step [858/1610]: discriminator_loss = 0.54453, target_loss = 0.38550, acc = 0.50000\n",
            "Epoch [3/5] Step [859/1610]: discriminator_loss = 0.58531, target_loss = 0.38122, acc = 0.50000\n",
            "Epoch [3/5] Step [860/1610]: discriminator_loss = 0.59557, target_loss = 0.39057, acc = 0.50000\n",
            "Epoch [3/5] Step [861/1610]: discriminator_loss = 0.53496, target_loss = 0.38225, acc = 0.50000\n",
            "Epoch [3/5] Step [862/1610]: discriminator_loss = 0.61527, target_loss = 0.40634, acc = 0.50000\n",
            "Epoch [3/5] Step [863/1610]: discriminator_loss = 0.61524, target_loss = 0.44087, acc = 0.50000\n",
            "Epoch [3/5] Step [864/1610]: discriminator_loss = 0.54121, target_loss = 0.40133, acc = 0.50000\n",
            "Epoch [3/5] Step [865/1610]: discriminator_loss = 0.56221, target_loss = 0.44079, acc = 0.50000\n",
            "Epoch [3/5] Step [866/1610]: discriminator_loss = 0.54585, target_loss = 0.40910, acc = 0.50000\n",
            "Epoch [3/5] Step [867/1610]: discriminator_loss = 0.53372, target_loss = 0.39352, acc = 0.50000\n",
            "Epoch [3/5] Step [868/1610]: discriminator_loss = 0.53434, target_loss = 0.42068, acc = 0.50000\n",
            "Epoch [3/5] Step [869/1610]: discriminator_loss = 0.53161, target_loss = 0.44355, acc = 0.50000\n",
            "Epoch [3/5] Step [870/1610]: discriminator_loss = 0.52324, target_loss = 0.43615, acc = 0.50000\n",
            "Epoch [3/5] Step [871/1610]: discriminator_loss = 0.64686, target_loss = 0.43283, acc = 0.50000\n",
            "Epoch [3/5] Step [872/1610]: discriminator_loss = 0.54653, target_loss = 0.43598, acc = 0.50000\n",
            "Epoch [3/5] Step [873/1610]: discriminator_loss = 0.52150, target_loss = 0.43542, acc = 0.50000\n",
            "Epoch [3/5] Step [874/1610]: discriminator_loss = 0.57129, target_loss = 0.39061, acc = 0.50000\n",
            "Epoch [3/5] Step [875/1610]: discriminator_loss = 0.53345, target_loss = 0.40393, acc = 0.50000\n",
            "Epoch [3/5] Step [876/1610]: discriminator_loss = 0.59797, target_loss = 0.41086, acc = 0.50000\n",
            "Epoch [3/5] Step [877/1610]: discriminator_loss = 0.57670, target_loss = 0.41355, acc = 0.50000\n",
            "Epoch [3/5] Step [878/1610]: discriminator_loss = 0.60427, target_loss = 0.39873, acc = 0.50000\n",
            "Epoch [3/5] Step [879/1610]: discriminator_loss = 0.58562, target_loss = 0.39149, acc = 0.50000\n",
            "Epoch [3/5] Step [880/1610]: discriminator_loss = 0.60017, target_loss = 0.42855, acc = 0.50000\n",
            "Epoch [3/5] Step [881/1610]: discriminator_loss = 0.59882, target_loss = 0.39893, acc = 0.50000\n",
            "Epoch [3/5] Step [882/1610]: discriminator_loss = 0.57999, target_loss = 0.40625, acc = 0.50000\n",
            "Epoch [3/5] Step [883/1610]: discriminator_loss = 0.59322, target_loss = 0.39940, acc = 0.50000\n",
            "Epoch [3/5] Step [884/1610]: discriminator_loss = 0.56134, target_loss = 0.41709, acc = 0.50000\n",
            "Epoch [3/5] Step [885/1610]: discriminator_loss = 0.58649, target_loss = 0.37253, acc = 0.50000\n",
            "Epoch [3/5] Step [886/1610]: discriminator_loss = 0.57805, target_loss = 0.39205, acc = 0.50000\n",
            "Epoch [3/5] Step [887/1610]: discriminator_loss = 0.58083, target_loss = 0.40109, acc = 0.50000\n",
            "Epoch [3/5] Step [888/1610]: discriminator_loss = 0.58614, target_loss = 0.36678, acc = 0.50000\n",
            "Epoch [3/5] Step [889/1610]: discriminator_loss = 0.59535, target_loss = 0.37799, acc = 0.50000\n",
            "Epoch [3/5] Step [890/1610]: discriminator_loss = 0.59126, target_loss = 0.37476, acc = 0.50000\n",
            "Epoch [3/5] Step [891/1610]: discriminator_loss = 0.59293, target_loss = 0.37120, acc = 0.50000\n",
            "Epoch [3/5] Step [892/1610]: discriminator_loss = 0.61881, target_loss = 0.34872, acc = 0.50000\n",
            "Epoch [3/5] Step [893/1610]: discriminator_loss = 0.57447, target_loss = 0.40430, acc = 0.50000\n",
            "Epoch [3/5] Step [894/1610]: discriminator_loss = 0.60359, target_loss = 0.38004, acc = 0.50000\n",
            "Epoch [3/5] Step [895/1610]: discriminator_loss = 0.58851, target_loss = 0.38211, acc = 0.50000\n",
            "Epoch [3/5] Step [896/1610]: discriminator_loss = 0.59889, target_loss = 0.35325, acc = 0.50000\n",
            "Epoch [3/5] Step [897/1610]: discriminator_loss = 0.57891, target_loss = 0.36940, acc = 0.50000\n",
            "Epoch [3/5] Step [898/1610]: discriminator_loss = 0.61068, target_loss = 0.36869, acc = 0.50000\n",
            "Epoch [3/5] Step [899/1610]: discriminator_loss = 0.56048, target_loss = 0.38091, acc = 0.50000\n",
            "Epoch [3/5] Step [900/1610]: discriminator_loss = 0.61209, target_loss = 0.33515, acc = 0.50000\n",
            "Epoch [3/5] Step [901/1610]: discriminator_loss = 0.56541, target_loss = 0.39100, acc = 0.50000\n",
            "Epoch [3/5] Step [902/1610]: discriminator_loss = 0.58095, target_loss = 0.38194, acc = 0.50000\n",
            "Epoch [3/5] Step [903/1610]: discriminator_loss = 0.63918, target_loss = 0.35967, acc = 0.50000\n",
            "Epoch [3/5] Step [904/1610]: discriminator_loss = 0.54816, target_loss = 0.42239, acc = 0.50000\n",
            "Epoch [3/5] Step [905/1610]: discriminator_loss = 0.55710, target_loss = 0.42086, acc = 0.50000\n",
            "Epoch [3/5] Step [906/1610]: discriminator_loss = 0.53645, target_loss = 0.39320, acc = 0.50000\n",
            "Epoch [3/5] Step [907/1610]: discriminator_loss = 0.57911, target_loss = 0.42420, acc = 0.50000\n",
            "Epoch [3/5] Step [908/1610]: discriminator_loss = 0.55155, target_loss = 0.45846, acc = 0.50000\n",
            "Epoch [3/5] Step [909/1610]: discriminator_loss = 0.61743, target_loss = 0.34693, acc = 0.50000\n",
            "Epoch [3/5] Step [910/1610]: discriminator_loss = 0.56417, target_loss = 0.45863, acc = 0.50000\n",
            "Epoch [3/5] Step [911/1610]: discriminator_loss = 0.59705, target_loss = 0.45582, acc = 0.50000\n",
            "Epoch [3/5] Step [912/1610]: discriminator_loss = 0.54973, target_loss = 0.46019, acc = 0.50000\n",
            "Epoch [3/5] Step [913/1610]: discriminator_loss = 0.52793, target_loss = 0.45584, acc = 0.50000\n",
            "Epoch [3/5] Step [914/1610]: discriminator_loss = 0.52032, target_loss = 0.46484, acc = 0.50000\n",
            "Epoch [3/5] Step [915/1610]: discriminator_loss = 0.54378, target_loss = 0.44983, acc = 0.50000\n",
            "Epoch [3/5] Step [916/1610]: discriminator_loss = 0.50899, target_loss = 0.42968, acc = 0.50000\n",
            "Epoch [3/5] Step [917/1610]: discriminator_loss = 0.55412, target_loss = 0.42316, acc = 0.50000\n",
            "Epoch [3/5] Step [918/1610]: discriminator_loss = 0.50988, target_loss = 0.44938, acc = 0.50000\n",
            "Epoch [3/5] Step [919/1610]: discriminator_loss = 0.53156, target_loss = 0.42896, acc = 0.50000\n",
            "Epoch [3/5] Step [920/1610]: discriminator_loss = 0.53865, target_loss = 0.44231, acc = 0.50000\n",
            "Epoch [3/5] Step [921/1610]: discriminator_loss = 0.54258, target_loss = 0.43114, acc = 0.50000\n",
            "Epoch [3/5] Step [922/1610]: discriminator_loss = 0.54318, target_loss = 0.42543, acc = 0.50000\n",
            "Epoch [3/5] Step [923/1610]: discriminator_loss = 0.54988, target_loss = 0.40721, acc = 0.50000\n",
            "Epoch [3/5] Step [924/1610]: discriminator_loss = 0.57476, target_loss = 0.40151, acc = 0.50000\n",
            "Epoch [3/5] Step [925/1610]: discriminator_loss = 0.56831, target_loss = 0.39152, acc = 0.50000\n",
            "Epoch [3/5] Step [926/1610]: discriminator_loss = 0.61960, target_loss = 0.37609, acc = 0.50000\n",
            "Epoch [3/5] Step [927/1610]: discriminator_loss = 0.56589, target_loss = 0.38141, acc = 0.50000\n",
            "Epoch [3/5] Step [928/1610]: discriminator_loss = 0.56769, target_loss = 0.37184, acc = 0.50000\n",
            "Epoch [3/5] Step [929/1610]: discriminator_loss = 0.57452, target_loss = 0.38366, acc = 0.50000\n",
            "Epoch [3/5] Step [930/1610]: discriminator_loss = 0.59097, target_loss = 0.38917, acc = 0.50000\n",
            "Epoch [3/5] Step [931/1610]: discriminator_loss = 0.60772, target_loss = 0.36044, acc = 0.50000\n",
            "Epoch [3/5] Step [932/1610]: discriminator_loss = 0.59770, target_loss = 0.38204, acc = 0.50000\n",
            "Epoch [3/5] Step [933/1610]: discriminator_loss = 0.59987, target_loss = 0.37291, acc = 0.50000\n",
            "Epoch [3/5] Step [934/1610]: discriminator_loss = 0.61027, target_loss = 0.35151, acc = 0.50000\n",
            "Epoch [3/5] Step [935/1610]: discriminator_loss = 0.59565, target_loss = 0.35725, acc = 0.50000\n",
            "Epoch [3/5] Step [936/1610]: discriminator_loss = 0.59294, target_loss = 0.35062, acc = 0.50000\n",
            "Epoch [3/5] Step [937/1610]: discriminator_loss = 0.61943, target_loss = 0.38747, acc = 0.50000\n",
            "Epoch [3/5] Step [938/1610]: discriminator_loss = 0.56712, target_loss = 0.36995, acc = 0.50000\n",
            "Epoch [3/5] Step [939/1610]: discriminator_loss = 0.58525, target_loss = 0.36306, acc = 0.50000\n",
            "Epoch [3/5] Step [940/1610]: discriminator_loss = 0.59399, target_loss = 0.38732, acc = 0.50000\n",
            "Epoch [3/5] Step [941/1610]: discriminator_loss = 0.62330, target_loss = 0.33139, acc = 0.50000\n",
            "Epoch [3/5] Step [942/1610]: discriminator_loss = 0.60437, target_loss = 0.39571, acc = 0.50000\n",
            "Epoch [3/5] Step [943/1610]: discriminator_loss = 0.60093, target_loss = 0.39011, acc = 0.50000\n",
            "Epoch [3/5] Step [944/1610]: discriminator_loss = 0.67707, target_loss = 0.36143, acc = 0.50000\n",
            "Epoch [3/5] Step [945/1610]: discriminator_loss = 0.55710, target_loss = 0.42475, acc = 0.50000\n",
            "Epoch [3/5] Step [946/1610]: discriminator_loss = 0.53624, target_loss = 0.44489, acc = 0.50000\n",
            "Epoch [3/5] Step [947/1610]: discriminator_loss = 0.54040, target_loss = 0.45225, acc = 0.50000\n",
            "Epoch [3/5] Step [948/1610]: discriminator_loss = 0.51190, target_loss = 0.43900, acc = 0.50000\n",
            "Epoch [3/5] Step [949/1610]: discriminator_loss = 0.53758, target_loss = 0.44100, acc = 0.50000\n",
            "Epoch [3/5] Step [950/1610]: discriminator_loss = 0.53594, target_loss = 0.45354, acc = 0.50000\n",
            "Epoch [3/5] Step [951/1610]: discriminator_loss = 0.50809, target_loss = 0.45574, acc = 0.50000\n",
            "Epoch [3/5] Step [952/1610]: discriminator_loss = 0.52502, target_loss = 0.40550, acc = 0.50000\n",
            "Epoch [3/5] Step [953/1610]: discriminator_loss = 0.55561, target_loss = 0.41217, acc = 0.50000\n",
            "Epoch [3/5] Step [954/1610]: discriminator_loss = 0.53330, target_loss = 0.40105, acc = 0.50000\n",
            "Epoch [3/5] Step [955/1610]: discriminator_loss = 0.58083, target_loss = 0.38728, acc = 0.50000\n",
            "Epoch [3/5] Step [956/1610]: discriminator_loss = 0.59342, target_loss = 0.41896, acc = 0.50000\n",
            "Epoch [3/5] Step [957/1610]: discriminator_loss = 0.53492, target_loss = 0.39888, acc = 0.50000\n",
            "Epoch [3/5] Step [958/1610]: discriminator_loss = 0.53780, target_loss = 0.41153, acc = 0.50000\n",
            "Epoch [3/5] Step [959/1610]: discriminator_loss = 0.55156, target_loss = 0.40915, acc = 0.50000\n",
            "Epoch [3/5] Step [960/1610]: discriminator_loss = 0.54860, target_loss = 0.39992, acc = 0.50000\n",
            "Epoch [3/5] Step [961/1610]: discriminator_loss = 0.60034, target_loss = 0.40486, acc = 0.50000\n",
            "Epoch [3/5] Step [962/1610]: discriminator_loss = 0.56729, target_loss = 0.37028, acc = 0.50000\n",
            "Epoch [3/5] Step [963/1610]: discriminator_loss = 0.56287, target_loss = 0.36871, acc = 0.50000\n",
            "Epoch [3/5] Step [964/1610]: discriminator_loss = 0.60969, target_loss = 0.40344, acc = 0.50000\n",
            "Epoch [3/5] Step [965/1610]: discriminator_loss = 0.56044, target_loss = 0.40111, acc = 0.50000\n",
            "Epoch [3/5] Step [966/1610]: discriminator_loss = 0.55735, target_loss = 0.40567, acc = 0.50000\n",
            "Epoch [3/5] Step [967/1610]: discriminator_loss = 0.57181, target_loss = 0.39735, acc = 0.50000\n",
            "Epoch [3/5] Step [968/1610]: discriminator_loss = 0.57002, target_loss = 0.42443, acc = 0.50000\n",
            "Epoch [3/5] Step [969/1610]: discriminator_loss = 0.57484, target_loss = 0.42302, acc = 0.50000\n",
            "Epoch [3/5] Step [970/1610]: discriminator_loss = 0.57406, target_loss = 0.39522, acc = 0.50000\n",
            "Epoch [3/5] Step [971/1610]: discriminator_loss = 0.57127, target_loss = 0.38340, acc = 0.50000\n",
            "Epoch [3/5] Step [972/1610]: discriminator_loss = 0.58250, target_loss = 0.40396, acc = 0.50000\n",
            "Epoch [3/5] Step [973/1610]: discriminator_loss = 0.57396, target_loss = 0.39701, acc = 0.50000\n",
            "Epoch [3/5] Step [974/1610]: discriminator_loss = 0.57428, target_loss = 0.40626, acc = 0.50000\n",
            "Epoch [3/5] Step [975/1610]: discriminator_loss = 0.56973, target_loss = 0.34364, acc = 0.50000\n",
            "Epoch [3/5] Step [976/1610]: discriminator_loss = 0.59872, target_loss = 0.39296, acc = 0.50000\n",
            "Epoch [3/5] Step [977/1610]: discriminator_loss = 0.55849, target_loss = 0.38962, acc = 0.50000\n",
            "Epoch [3/5] Step [978/1610]: discriminator_loss = 0.59247, target_loss = 0.39435, acc = 0.50000\n",
            "Epoch [3/5] Step [979/1610]: discriminator_loss = 0.58052, target_loss = 0.37015, acc = 0.50000\n",
            "Epoch [3/5] Step [980/1610]: discriminator_loss = 0.58481, target_loss = 0.38477, acc = 0.50000\n",
            "Epoch [3/5] Step [981/1610]: discriminator_loss = 0.60983, target_loss = 0.38476, acc = 0.50000\n",
            "Epoch [3/5] Step [982/1610]: discriminator_loss = 0.56972, target_loss = 0.40701, acc = 0.50000\n",
            "Epoch [3/5] Step [983/1610]: discriminator_loss = 0.59737, target_loss = 0.38281, acc = 0.50000\n",
            "Epoch [3/5] Step [984/1610]: discriminator_loss = 0.57428, target_loss = 0.40098, acc = 0.50000\n",
            "Epoch [3/5] Step [985/1610]: discriminator_loss = 0.60638, target_loss = 0.39130, acc = 0.50000\n",
            "Epoch [3/5] Step [986/1610]: discriminator_loss = 0.57229, target_loss = 0.42112, acc = 0.50000\n",
            "Epoch [3/5] Step [987/1610]: discriminator_loss = 0.53726, target_loss = 0.42727, acc = 0.50000\n",
            "Epoch [3/5] Step [988/1610]: discriminator_loss = 0.56629, target_loss = 0.39080, acc = 0.50000\n",
            "Epoch [3/5] Step [989/1610]: discriminator_loss = 0.54640, target_loss = 0.41574, acc = 0.50000\n",
            "Epoch [3/5] Step [990/1610]: discriminator_loss = 0.56650, target_loss = 0.42453, acc = 0.50000\n",
            "Epoch [3/5] Step [991/1610]: discriminator_loss = 0.59866, target_loss = 0.42193, acc = 0.50000\n",
            "Epoch [3/5] Step [992/1610]: discriminator_loss = 0.53377, target_loss = 0.42761, acc = 0.50000\n",
            "Epoch [3/5] Step [993/1610]: discriminator_loss = 0.55573, target_loss = 0.42126, acc = 0.50000\n",
            "Epoch [3/5] Step [994/1610]: discriminator_loss = 0.55029, target_loss = 0.44562, acc = 0.50000\n",
            "Epoch [3/5] Step [995/1610]: discriminator_loss = 0.52666, target_loss = 0.43275, acc = 0.50000\n",
            "Epoch [3/5] Step [996/1610]: discriminator_loss = 0.56860, target_loss = 0.41118, acc = 0.50000\n",
            "Epoch [3/5] Step [997/1610]: discriminator_loss = 0.55982, target_loss = 0.41576, acc = 0.50000\n",
            "Epoch [3/5] Step [998/1610]: discriminator_loss = 0.55101, target_loss = 0.41067, acc = 0.50000\n",
            "Epoch [3/5] Step [999/1610]: discriminator_loss = 0.60220, target_loss = 0.40483, acc = 0.50000\n",
            "Epoch [3/5] Step [1000/1610]: discriminator_loss = 0.54196, target_loss = 0.42702, acc = 0.50000\n",
            "Epoch [3/5] Step [1001/1610]: discriminator_loss = 0.56905, target_loss = 0.40657, acc = 0.50000\n",
            "Epoch [3/5] Step [1002/1610]: discriminator_loss = 0.56881, target_loss = 0.40373, acc = 0.50000\n",
            "Epoch [3/5] Step [1003/1610]: discriminator_loss = 0.57047, target_loss = 0.38919, acc = 0.50000\n",
            "Epoch [3/5] Step [1004/1610]: discriminator_loss = 0.59950, target_loss = 0.39867, acc = 0.50000\n",
            "Epoch [3/5] Step [1005/1610]: discriminator_loss = 0.56393, target_loss = 0.39992, acc = 0.50000\n",
            "Epoch [3/5] Step [1006/1610]: discriminator_loss = 0.56280, target_loss = 0.38411, acc = 0.50000\n",
            "Epoch [3/5] Step [1007/1610]: discriminator_loss = 0.58868, target_loss = 0.39872, acc = 0.50000\n",
            "Epoch [3/5] Step [1008/1610]: discriminator_loss = 0.58095, target_loss = 0.38347, acc = 0.50000\n",
            "Epoch [3/5] Step [1009/1610]: discriminator_loss = 0.60618, target_loss = 0.38751, acc = 0.50000\n",
            "Epoch [3/5] Step [1010/1610]: discriminator_loss = 0.58624, target_loss = 0.39100, acc = 0.50000\n",
            "Epoch [3/5] Step [1011/1610]: discriminator_loss = 0.59887, target_loss = 0.39835, acc = 0.50000\n",
            "Epoch [3/5] Step [1012/1610]: discriminator_loss = 0.60294, target_loss = 0.39305, acc = 0.50000\n",
            "Epoch [3/5] Step [1013/1610]: discriminator_loss = 0.62125, target_loss = 0.40445, acc = 0.50000\n",
            "Epoch [3/5] Step [1014/1610]: discriminator_loss = 0.57059, target_loss = 0.39362, acc = 0.50000\n",
            "Epoch [3/5] Step [1015/1610]: discriminator_loss = 0.56781, target_loss = 0.38646, acc = 0.50000\n",
            "Epoch [3/5] Step [1016/1610]: discriminator_loss = 0.57549, target_loss = 0.40480, acc = 0.50000\n",
            "Epoch [3/5] Step [1017/1610]: discriminator_loss = 0.54845, target_loss = 0.39753, acc = 0.50000\n",
            "Epoch [3/5] Step [1018/1610]: discriminator_loss = 0.58570, target_loss = 0.40839, acc = 0.50000\n",
            "Epoch [3/5] Step [1019/1610]: discriminator_loss = 0.56222, target_loss = 0.37285, acc = 0.50000\n",
            "Epoch [3/5] Step [1020/1610]: discriminator_loss = 0.55069, target_loss = 0.39580, acc = 0.50000\n",
            "Epoch [3/5] Step [1021/1610]: discriminator_loss = 0.57518, target_loss = 0.38846, acc = 0.50000\n",
            "Epoch [3/5] Step [1022/1610]: discriminator_loss = 0.57085, target_loss = 0.39497, acc = 0.50000\n",
            "Epoch [3/5] Step [1023/1610]: discriminator_loss = 0.54896, target_loss = 0.39294, acc = 0.50000\n",
            "Epoch [3/5] Step [1024/1610]: discriminator_loss = 0.58658, target_loss = 0.40351, acc = 0.50000\n",
            "Epoch [3/5] Step [1025/1610]: discriminator_loss = 0.57150, target_loss = 0.38149, acc = 0.50000\n",
            "Epoch [3/5] Step [1026/1610]: discriminator_loss = 0.56072, target_loss = 0.42629, acc = 0.50000\n",
            "Epoch [3/5] Step [1027/1610]: discriminator_loss = 0.54461, target_loss = 0.39860, acc = 0.50000\n",
            "Epoch [3/5] Step [1028/1610]: discriminator_loss = 0.57042, target_loss = 0.41793, acc = 0.50000\n",
            "Epoch [3/5] Step [1029/1610]: discriminator_loss = 0.57915, target_loss = 0.42166, acc = 0.50000\n",
            "Epoch [3/5] Step [1030/1610]: discriminator_loss = 0.58197, target_loss = 0.38313, acc = 0.50000\n",
            "Epoch [3/5] Step [1031/1610]: discriminator_loss = 0.55771, target_loss = 0.41123, acc = 0.50000\n",
            "Epoch [3/5] Step [1032/1610]: discriminator_loss = 0.55917, target_loss = 0.41316, acc = 0.50000\n",
            "Epoch [3/5] Step [1033/1610]: discriminator_loss = 0.53778, target_loss = 0.38942, acc = 0.50000\n",
            "Epoch [3/5] Step [1034/1610]: discriminator_loss = 0.54466, target_loss = 0.38960, acc = 0.50000\n",
            "Epoch [3/5] Step [1035/1610]: discriminator_loss = 0.56081, target_loss = 0.39840, acc = 0.50000\n",
            "Epoch [3/5] Step [1036/1610]: discriminator_loss = 0.54075, target_loss = 0.40576, acc = 0.50000\n",
            "Epoch [3/5] Step [1037/1610]: discriminator_loss = 0.56664, target_loss = 0.38569, acc = 0.50000\n",
            "Epoch [3/5] Step [1038/1610]: discriminator_loss = 0.56750, target_loss = 0.39926, acc = 0.50000\n",
            "Epoch [3/5] Step [1039/1610]: discriminator_loss = 0.56477, target_loss = 0.40576, acc = 0.50000\n",
            "Epoch [3/5] Step [1040/1610]: discriminator_loss = 0.61188, target_loss = 0.36622, acc = 0.50000\n",
            "Epoch [3/5] Step [1041/1610]: discriminator_loss = 0.61025, target_loss = 0.41289, acc = 0.50000\n",
            "Epoch [3/5] Step [1042/1610]: discriminator_loss = 0.58134, target_loss = 0.40481, acc = 0.50000\n",
            "Epoch [3/5] Step [1043/1610]: discriminator_loss = 0.62383, target_loss = 0.40466, acc = 0.50000\n",
            "Epoch [3/5] Step [1044/1610]: discriminator_loss = 0.60009, target_loss = 0.40093, acc = 0.50000\n",
            "Epoch [3/5] Step [1045/1610]: discriminator_loss = 0.54087, target_loss = 0.43465, acc = 0.50000\n",
            "Epoch [3/5] Step [1046/1610]: discriminator_loss = 0.54530, target_loss = 0.41937, acc = 0.50000\n",
            "Epoch [3/5] Step [1047/1610]: discriminator_loss = 0.54994, target_loss = 0.43294, acc = 0.50000\n",
            "Epoch [3/5] Step [1048/1610]: discriminator_loss = 0.56617, target_loss = 0.42965, acc = 0.50000\n",
            "Epoch [3/5] Step [1049/1610]: discriminator_loss = 0.53238, target_loss = 0.42113, acc = 0.50000\n",
            "Epoch [3/5] Step [1050/1610]: discriminator_loss = 0.55675, target_loss = 0.42672, acc = 0.50000\n",
            "Epoch [3/5] Step [1051/1610]: discriminator_loss = 0.54664, target_loss = 0.40088, acc = 0.50000\n",
            "Epoch [3/5] Step [1052/1610]: discriminator_loss = 0.56893, target_loss = 0.39290, acc = 0.50000\n",
            "Epoch [3/5] Step [1053/1610]: discriminator_loss = 0.53388, target_loss = 0.41234, acc = 0.50000\n",
            "Epoch [3/5] Step [1054/1610]: discriminator_loss = 0.56446, target_loss = 0.38721, acc = 0.50000\n",
            "Epoch [3/5] Step [1055/1610]: discriminator_loss = 0.60541, target_loss = 0.38409, acc = 0.50000\n",
            "Epoch [3/5] Step [1056/1610]: discriminator_loss = 0.57656, target_loss = 0.41299, acc = 0.50000\n",
            "Epoch [3/5] Step [1057/1610]: discriminator_loss = 0.59928, target_loss = 0.39046, acc = 0.50000\n",
            "Epoch [3/5] Step [1058/1610]: discriminator_loss = 0.57020, target_loss = 0.40056, acc = 0.50000\n",
            "Epoch [3/5] Step [1059/1610]: discriminator_loss = 0.56982, target_loss = 0.37274, acc = 0.50000\n",
            "Epoch [3/5] Step [1060/1610]: discriminator_loss = 0.60719, target_loss = 0.36508, acc = 0.50000\n",
            "Epoch [3/5] Step [1061/1610]: discriminator_loss = 0.64806, target_loss = 0.34603, acc = 0.50000\n",
            "Epoch [3/5] Step [1062/1610]: discriminator_loss = 0.61084, target_loss = 0.40042, acc = 0.50000\n",
            "Epoch [3/5] Step [1063/1610]: discriminator_loss = 0.58407, target_loss = 0.38059, acc = 0.50000\n",
            "Epoch [3/5] Step [1064/1610]: discriminator_loss = 0.56681, target_loss = 0.38931, acc = 0.50000\n",
            "Epoch [3/5] Step [1065/1610]: discriminator_loss = 0.60647, target_loss = 0.37237, acc = 0.50000\n",
            "Epoch [3/5] Step [1066/1610]: discriminator_loss = 0.55012, target_loss = 0.43379, acc = 0.50000\n",
            "Epoch [3/5] Step [1067/1610]: discriminator_loss = 0.56664, target_loss = 0.38276, acc = 0.50000\n",
            "Epoch [3/5] Step [1068/1610]: discriminator_loss = 0.58864, target_loss = 0.38827, acc = 0.50000\n",
            "Epoch [3/5] Step [1069/1610]: discriminator_loss = 0.56864, target_loss = 0.38668, acc = 0.50000\n",
            "Epoch [3/5] Step [1070/1610]: discriminator_loss = 0.61065, target_loss = 0.38342, acc = 0.50000\n",
            "Epoch [3/5] Step [1071/1610]: discriminator_loss = 0.58057, target_loss = 0.42447, acc = 0.50000\n",
            "Epoch [3/5] Step [1072/1610]: discriminator_loss = 0.58516, target_loss = 0.39685, acc = 0.50000\n",
            "Epoch [3/5] Step [1073/1610]: discriminator_loss = 0.57315, target_loss = 0.39491, acc = 0.50000\n",
            "Epoch [3/5] Step [1074/1610]: discriminator_loss = 0.55329, target_loss = 0.37686, acc = 0.50000\n",
            "Epoch [3/5] Step [1075/1610]: discriminator_loss = 0.52853, target_loss = 0.39546, acc = 0.50000\n",
            "Epoch [3/5] Step [1076/1610]: discriminator_loss = 0.54845, target_loss = 0.38957, acc = 0.50000\n",
            "Epoch [3/5] Step [1077/1610]: discriminator_loss = 0.55263, target_loss = 0.37823, acc = 0.50000\n",
            "Epoch [3/5] Step [1078/1610]: discriminator_loss = 0.57164, target_loss = 0.42398, acc = 0.50000\n",
            "Epoch [3/5] Step [1079/1610]: discriminator_loss = 0.55283, target_loss = 0.42128, acc = 0.50000\n",
            "Epoch [3/5] Step [1080/1610]: discriminator_loss = 0.55065, target_loss = 0.41318, acc = 0.50000\n",
            "Epoch [3/5] Step [1081/1610]: discriminator_loss = 0.53097, target_loss = 0.41849, acc = 0.50000\n",
            "Epoch [3/5] Step [1082/1610]: discriminator_loss = 0.52809, target_loss = 0.41045, acc = 0.50000\n",
            "Epoch [3/5] Step [1083/1610]: discriminator_loss = 0.55974, target_loss = 0.40930, acc = 0.50000\n",
            "Epoch [3/5] Step [1084/1610]: discriminator_loss = 0.52463, target_loss = 0.39143, acc = 0.50000\n",
            "Epoch [3/5] Step [1085/1610]: discriminator_loss = 0.54768, target_loss = 0.40277, acc = 0.50000\n",
            "Epoch [3/5] Step [1086/1610]: discriminator_loss = 0.55656, target_loss = 0.41940, acc = 0.50000\n",
            "Epoch [3/5] Step [1087/1610]: discriminator_loss = 0.55118, target_loss = 0.42434, acc = 0.50000\n",
            "Epoch [3/5] Step [1088/1610]: discriminator_loss = 0.57343, target_loss = 0.40165, acc = 0.50000\n",
            "Epoch [3/5] Step [1089/1610]: discriminator_loss = 0.55681, target_loss = 0.41413, acc = 0.50000\n",
            "Epoch [3/5] Step [1090/1610]: discriminator_loss = 0.57957, target_loss = 0.41367, acc = 0.50000\n",
            "Epoch [3/5] Step [1091/1610]: discriminator_loss = 0.58487, target_loss = 0.40146, acc = 0.50000\n",
            "Epoch [3/5] Step [1092/1610]: discriminator_loss = 0.57391, target_loss = 0.38705, acc = 0.50000\n",
            "Epoch [3/5] Step [1093/1610]: discriminator_loss = 0.56558, target_loss = 0.39469, acc = 0.50000\n",
            "Epoch [3/5] Step [1094/1610]: discriminator_loss = 0.58548, target_loss = 0.40625, acc = 0.50000\n",
            "Epoch [3/5] Step [1095/1610]: discriminator_loss = 0.57309, target_loss = 0.40200, acc = 0.50000\n",
            "Epoch [3/5] Step [1096/1610]: discriminator_loss = 0.61701, target_loss = 0.38807, acc = 0.50000\n",
            "Epoch [3/5] Step [1097/1610]: discriminator_loss = 0.54402, target_loss = 0.40585, acc = 0.50000\n",
            "Epoch [3/5] Step [1098/1610]: discriminator_loss = 0.56548, target_loss = 0.42317, acc = 0.50000\n",
            "Epoch [3/5] Step [1099/1610]: discriminator_loss = 0.58507, target_loss = 0.39877, acc = 0.50000\n",
            "Epoch [3/5] Step [1100/1610]: discriminator_loss = 0.56608, target_loss = 0.37811, acc = 0.50000\n",
            "Epoch [3/5] Step [1101/1610]: discriminator_loss = 0.55660, target_loss = 0.39239, acc = 0.50000\n",
            "Epoch [3/5] Step [1102/1610]: discriminator_loss = 0.56053, target_loss = 0.39060, acc = 0.50000\n",
            "Epoch [3/5] Step [1103/1610]: discriminator_loss = 0.56327, target_loss = 0.40938, acc = 0.50000\n",
            "Epoch [3/5] Step [1104/1610]: discriminator_loss = 0.56173, target_loss = 0.40410, acc = 0.50000\n",
            "Epoch [3/5] Step [1105/1610]: discriminator_loss = 0.58449, target_loss = 0.38278, acc = 0.50000\n",
            "Epoch [3/5] Step [1106/1610]: discriminator_loss = 0.57843, target_loss = 0.41092, acc = 0.50000\n",
            "Epoch [3/5] Step [1107/1610]: discriminator_loss = 0.57644, target_loss = 0.38979, acc = 0.50000\n",
            "Epoch [3/5] Step [1108/1610]: discriminator_loss = 0.59342, target_loss = 0.40831, acc = 0.50000\n",
            "Epoch [3/5] Step [1109/1610]: discriminator_loss = 0.56415, target_loss = 0.38083, acc = 0.50000\n",
            "Epoch [3/5] Step [1110/1610]: discriminator_loss = 0.59208, target_loss = 0.38377, acc = 0.50000\n",
            "Epoch [3/5] Step [1111/1610]: discriminator_loss = 0.59380, target_loss = 0.40360, acc = 0.50000\n",
            "Epoch [3/5] Step [1112/1610]: discriminator_loss = 0.56838, target_loss = 0.37435, acc = 0.50000\n",
            "Epoch [3/5] Step [1113/1610]: discriminator_loss = 0.58270, target_loss = 0.39602, acc = 0.50000\n",
            "Epoch [3/5] Step [1114/1610]: discriminator_loss = 0.57314, target_loss = 0.40151, acc = 0.50000\n",
            "Epoch [3/5] Step [1115/1610]: discriminator_loss = 0.57180, target_loss = 0.39861, acc = 0.50000\n",
            "Epoch [3/5] Step [1116/1610]: discriminator_loss = 0.57131, target_loss = 0.36960, acc = 0.50000\n",
            "Epoch [3/5] Step [1117/1610]: discriminator_loss = 0.56494, target_loss = 0.40275, acc = 0.50000\n",
            "Epoch [3/5] Step [1118/1610]: discriminator_loss = 0.55681, target_loss = 0.38030, acc = 0.50000\n",
            "Epoch [3/5] Step [1119/1610]: discriminator_loss = 0.55265, target_loss = 0.36950, acc = 0.50000\n",
            "Epoch [3/5] Step [1120/1610]: discriminator_loss = 0.56227, target_loss = 0.38038, acc = 0.50000\n",
            "Epoch [3/5] Step [1121/1610]: discriminator_loss = 0.59793, target_loss = 0.37472, acc = 0.50000\n",
            "Epoch [3/5] Step [1122/1610]: discriminator_loss = 0.67917, target_loss = 0.38291, acc = 0.50000\n",
            "Epoch [3/5] Step [1123/1610]: discriminator_loss = 0.60354, target_loss = 0.40361, acc = 0.50000\n",
            "Epoch [3/5] Step [1124/1610]: discriminator_loss = 0.55484, target_loss = 0.42181, acc = 0.50000\n",
            "Epoch [3/5] Step [1125/1610]: discriminator_loss = 0.55399, target_loss = 0.39664, acc = 0.50000\n",
            "Epoch [3/5] Step [1126/1610]: discriminator_loss = 0.59093, target_loss = 0.42825, acc = 0.50000\n",
            "Epoch [3/5] Step [1127/1610]: discriminator_loss = 0.53679, target_loss = 0.41104, acc = 0.50000\n",
            "Epoch [3/5] Step [1128/1610]: discriminator_loss = 0.51608, target_loss = 0.44611, acc = 0.50000\n",
            "Epoch [3/5] Step [1129/1610]: discriminator_loss = 0.50786, target_loss = 0.43846, acc = 0.50000\n",
            "Epoch [3/5] Step [1130/1610]: discriminator_loss = 0.54077, target_loss = 0.42010, acc = 0.50000\n",
            "Epoch [3/5] Step [1131/1610]: discriminator_loss = 0.54706, target_loss = 0.42527, acc = 0.50000\n",
            "Epoch [3/5] Step [1132/1610]: discriminator_loss = 0.54722, target_loss = 0.41408, acc = 0.50000\n",
            "Epoch [3/5] Step [1133/1610]: discriminator_loss = 0.54556, target_loss = 0.42137, acc = 0.50000\n",
            "Epoch [3/5] Step [1134/1610]: discriminator_loss = 0.55127, target_loss = 0.42510, acc = 0.50000\n",
            "Epoch [3/5] Step [1135/1610]: discriminator_loss = 0.57045, target_loss = 0.40157, acc = 0.50000\n",
            "Epoch [3/5] Step [1136/1610]: discriminator_loss = 0.58023, target_loss = 0.41220, acc = 0.50000\n",
            "Epoch [3/5] Step [1137/1610]: discriminator_loss = 0.56395, target_loss = 0.39256, acc = 0.50000\n",
            "Epoch [3/5] Step [1138/1610]: discriminator_loss = 0.56495, target_loss = 0.39954, acc = 0.50000\n",
            "Epoch [3/5] Step [1139/1610]: discriminator_loss = 0.57857, target_loss = 0.37516, acc = 0.50000\n",
            "Epoch [3/5] Step [1140/1610]: discriminator_loss = 0.57433, target_loss = 0.39074, acc = 0.50000\n",
            "Epoch [3/5] Step [1141/1610]: discriminator_loss = 0.58783, target_loss = 0.37301, acc = 0.50000\n",
            "Epoch [3/5] Step [1142/1610]: discriminator_loss = 0.61257, target_loss = 0.35955, acc = 0.50000\n",
            "Epoch [3/5] Step [1143/1610]: discriminator_loss = 0.63250, target_loss = 0.37064, acc = 0.50000\n",
            "Epoch [3/5] Step [1144/1610]: discriminator_loss = 0.59899, target_loss = 0.35313, acc = 0.50000\n",
            "Epoch [3/5] Step [1145/1610]: discriminator_loss = 0.61680, target_loss = 0.33504, acc = 0.50000\n",
            "Epoch [3/5] Step [1146/1610]: discriminator_loss = 0.61601, target_loss = 0.35917, acc = 0.50000\n",
            "Epoch [3/5] Step [1147/1610]: discriminator_loss = 0.63944, target_loss = 0.36345, acc = 0.50000\n",
            "Epoch [3/5] Step [1148/1610]: discriminator_loss = 0.57788, target_loss = 0.37201, acc = 0.50000\n",
            "Epoch [3/5] Step [1149/1610]: discriminator_loss = 0.62107, target_loss = 0.36456, acc = 0.50000\n",
            "Epoch [3/5] Step [1150/1610]: discriminator_loss = 0.61358, target_loss = 0.36025, acc = 0.50000\n",
            "Epoch [3/5] Step [1151/1610]: discriminator_loss = 0.57823, target_loss = 0.38824, acc = 0.50000\n",
            "Epoch [3/5] Step [1152/1610]: discriminator_loss = 0.58189, target_loss = 0.40306, acc = 0.50000\n",
            "Epoch [3/5] Step [1153/1610]: discriminator_loss = 0.57252, target_loss = 0.40540, acc = 0.50000\n",
            "Epoch [3/5] Step [1154/1610]: discriminator_loss = 0.54989, target_loss = 0.38594, acc = 0.50000\n",
            "Epoch [3/5] Step [1155/1610]: discriminator_loss = 0.55090, target_loss = 0.40314, acc = 0.50000\n",
            "Epoch [3/5] Step [1156/1610]: discriminator_loss = 0.54871, target_loss = 0.41121, acc = 0.50000\n",
            "Epoch [3/5] Step [1157/1610]: discriminator_loss = 0.55550, target_loss = 0.42235, acc = 0.50000\n",
            "Epoch [3/5] Step [1158/1610]: discriminator_loss = 0.55765, target_loss = 0.41383, acc = 0.50000\n",
            "Epoch [3/5] Step [1159/1610]: discriminator_loss = 0.53523, target_loss = 0.42061, acc = 0.50000\n",
            "Epoch [3/5] Step [1160/1610]: discriminator_loss = 0.53634, target_loss = 0.38498, acc = 0.50000\n",
            "Epoch [3/5] Step [1161/1610]: discriminator_loss = 0.53308, target_loss = 0.39747, acc = 0.50000\n",
            "Epoch [3/5] Step [1162/1610]: discriminator_loss = 0.56006, target_loss = 0.40265, acc = 0.50000\n",
            "Epoch [3/5] Step [1163/1610]: discriminator_loss = 0.54054, target_loss = 0.39804, acc = 0.50000\n",
            "Epoch [3/5] Step [1164/1610]: discriminator_loss = 0.55545, target_loss = 0.44588, acc = 0.50000\n",
            "Epoch [3/5] Step [1165/1610]: discriminator_loss = 0.54229, target_loss = 0.42229, acc = 0.50000\n",
            "Epoch [3/5] Step [1166/1610]: discriminator_loss = 0.56088, target_loss = 0.37363, acc = 0.50000\n",
            "Epoch [3/5] Step [1167/1610]: discriminator_loss = 0.59755, target_loss = 0.37395, acc = 0.50000\n",
            "Epoch [3/5] Step [1168/1610]: discriminator_loss = 0.56183, target_loss = 0.39297, acc = 0.50000\n",
            "Epoch [3/5] Step [1169/1610]: discriminator_loss = 0.52798, target_loss = 0.39744, acc = 0.56250\n",
            "Epoch [3/5] Step [1170/1610]: discriminator_loss = 0.62385, target_loss = 0.38403, acc = 0.50000\n",
            "Epoch [3/5] Step [1171/1610]: discriminator_loss = 0.56350, target_loss = 0.36420, acc = 0.50000\n",
            "Epoch [3/5] Step [1172/1610]: discriminator_loss = 0.65845, target_loss = 0.36016, acc = 0.50000\n",
            "Epoch [3/5] Step [1173/1610]: discriminator_loss = 0.65161, target_loss = 0.40000, acc = 0.50000\n",
            "Epoch [3/5] Step [1174/1610]: discriminator_loss = 0.62155, target_loss = 0.39195, acc = 0.50000\n",
            "Epoch [3/5] Step [1175/1610]: discriminator_loss = 0.56777, target_loss = 0.40982, acc = 0.50000\n",
            "Epoch [3/5] Step [1176/1610]: discriminator_loss = 0.54496, target_loss = 0.43902, acc = 0.50000\n",
            "Epoch [3/5] Step [1177/1610]: discriminator_loss = 0.52486, target_loss = 0.42642, acc = 0.50000\n",
            "Epoch [3/5] Step [1178/1610]: discriminator_loss = 0.52896, target_loss = 0.42955, acc = 0.50000\n",
            "Epoch [3/5] Step [1179/1610]: discriminator_loss = 0.51798, target_loss = 0.44416, acc = 0.50000\n",
            "Epoch [3/5] Step [1180/1610]: discriminator_loss = 0.57188, target_loss = 0.43564, acc = 0.50000\n",
            "Epoch [3/5] Step [1181/1610]: discriminator_loss = 0.55363, target_loss = 0.43603, acc = 0.50000\n",
            "Epoch [3/5] Step [1182/1610]: discriminator_loss = 0.54497, target_loss = 0.43557, acc = 0.50000\n",
            "Epoch [3/5] Step [1183/1610]: discriminator_loss = 0.52184, target_loss = 0.40647, acc = 0.50000\n",
            "Epoch [3/5] Step [1184/1610]: discriminator_loss = 0.54004, target_loss = 0.42789, acc = 0.50000\n",
            "Epoch [3/5] Step [1185/1610]: discriminator_loss = 0.54363, target_loss = 0.41480, acc = 0.50000\n",
            "Epoch [3/5] Step [1186/1610]: discriminator_loss = 0.57394, target_loss = 0.41139, acc = 0.50000\n",
            "Epoch [3/5] Step [1187/1610]: discriminator_loss = 0.59495, target_loss = 0.40268, acc = 0.50000\n",
            "Epoch [3/5] Step [1188/1610]: discriminator_loss = 0.54652, target_loss = 0.40086, acc = 0.50000\n",
            "Epoch [3/5] Step [1189/1610]: discriminator_loss = 0.58517, target_loss = 0.38578, acc = 0.50000\n",
            "Epoch [3/5] Step [1190/1610]: discriminator_loss = 0.58769, target_loss = 0.38353, acc = 0.50000\n",
            "Epoch [3/5] Step [1191/1610]: discriminator_loss = 0.57356, target_loss = 0.36542, acc = 0.50000\n",
            "Epoch [3/5] Step [1192/1610]: discriminator_loss = 0.58021, target_loss = 0.37274, acc = 0.50000\n",
            "Epoch [3/5] Step [1193/1610]: discriminator_loss = 0.57161, target_loss = 0.38763, acc = 0.50000\n",
            "Epoch [3/5] Step [1194/1610]: discriminator_loss = 0.59959, target_loss = 0.37202, acc = 0.50000\n",
            "Epoch [3/5] Step [1195/1610]: discriminator_loss = 0.58813, target_loss = 0.37521, acc = 0.50000\n",
            "Epoch [3/5] Step [1196/1610]: discriminator_loss = 0.62987, target_loss = 0.34287, acc = 0.50000\n",
            "Epoch [3/5] Step [1197/1610]: discriminator_loss = 0.55652, target_loss = 0.38999, acc = 0.50000\n",
            "Epoch [3/5] Step [1198/1610]: discriminator_loss = 0.62005, target_loss = 0.34414, acc = 0.50000\n",
            "Epoch [3/5] Step [1199/1610]: discriminator_loss = 0.59801, target_loss = 0.40059, acc = 0.50000\n",
            "Epoch [3/5] Step [1200/1610]: discriminator_loss = 0.56324, target_loss = 0.38840, acc = 0.50000\n",
            "Epoch [3/5] Step [1201/1610]: discriminator_loss = 0.57755, target_loss = 0.39492, acc = 0.50000\n",
            "Epoch [3/5] Step [1202/1610]: discriminator_loss = 0.56288, target_loss = 0.38967, acc = 0.50000\n",
            "Epoch [3/5] Step [1203/1610]: discriminator_loss = 0.56065, target_loss = 0.36677, acc = 0.50000\n",
            "Epoch [3/5] Step [1204/1610]: discriminator_loss = 0.53560, target_loss = 0.40034, acc = 0.50000\n",
            "Epoch [3/5] Step [1205/1610]: discriminator_loss = 0.56875, target_loss = 0.38961, acc = 0.50000\n",
            "Epoch [3/5] Step [1206/1610]: discriminator_loss = 0.57392, target_loss = 0.35529, acc = 0.50000\n",
            "Epoch [3/5] Step [1207/1610]: discriminator_loss = 0.57134, target_loss = 0.40928, acc = 0.50000\n",
            "Epoch [3/5] Step [1208/1610]: discriminator_loss = 0.56546, target_loss = 0.39880, acc = 0.50000\n",
            "Epoch [3/5] Step [1209/1610]: discriminator_loss = 0.59470, target_loss = 0.38000, acc = 0.50000\n",
            "Epoch [3/5] Step [1210/1610]: discriminator_loss = 0.56065, target_loss = 0.41380, acc = 0.53125\n",
            "Epoch [3/5] Step [1211/1610]: discriminator_loss = 0.56032, target_loss = 0.41360, acc = 0.50000\n",
            "Epoch [3/5] Step [1212/1610]: discriminator_loss = 0.61393, target_loss = 0.38981, acc = 0.50000\n",
            "Epoch [3/5] Step [1213/1610]: discriminator_loss = 0.54887, target_loss = 0.38812, acc = 0.50000\n",
            "Epoch [3/5] Step [1214/1610]: discriminator_loss = 0.56061, target_loss = 0.41100, acc = 0.50000\n",
            "Epoch [3/5] Step [1215/1610]: discriminator_loss = 0.56190, target_loss = 0.41212, acc = 0.50000\n",
            "Epoch [3/5] Step [1216/1610]: discriminator_loss = 0.56319, target_loss = 0.40791, acc = 0.50000\n",
            "Epoch [3/5] Step [1217/1610]: discriminator_loss = 0.53958, target_loss = 0.42536, acc = 0.50000\n",
            "Epoch [3/5] Step [1218/1610]: discriminator_loss = 0.55924, target_loss = 0.40446, acc = 0.50000\n",
            "Epoch [3/5] Step [1219/1610]: discriminator_loss = 0.52988, target_loss = 0.44454, acc = 0.50000\n",
            "Epoch [3/5] Step [1220/1610]: discriminator_loss = 0.52602, target_loss = 0.42859, acc = 0.50000\n",
            "Epoch [3/5] Step [1221/1610]: discriminator_loss = 0.54528, target_loss = 0.42184, acc = 0.50000\n",
            "Epoch [3/5] Step [1222/1610]: discriminator_loss = 0.55237, target_loss = 0.40780, acc = 0.50000\n",
            "Epoch [3/5] Step [1223/1610]: discriminator_loss = 0.55969, target_loss = 0.42208, acc = 0.50000\n",
            "Epoch [3/5] Step [1224/1610]: discriminator_loss = 0.57501, target_loss = 0.40324, acc = 0.50000\n",
            "Epoch [3/5] Step [1225/1610]: discriminator_loss = 0.60068, target_loss = 0.38522, acc = 0.50000\n",
            "Epoch [3/5] Step [1226/1610]: discriminator_loss = 0.56593, target_loss = 0.38734, acc = 0.50000\n",
            "Epoch [3/5] Step [1227/1610]: discriminator_loss = 0.59796, target_loss = 0.36841, acc = 0.50000\n",
            "Epoch [3/5] Step [1228/1610]: discriminator_loss = 0.56732, target_loss = 0.41120, acc = 0.50000\n",
            "Epoch [3/5] Step [1229/1610]: discriminator_loss = 0.60170, target_loss = 0.35122, acc = 0.53125\n",
            "Epoch [3/5] Step [1230/1610]: discriminator_loss = 0.58410, target_loss = 0.37590, acc = 0.50000\n",
            "Epoch [3/5] Step [1231/1610]: discriminator_loss = 0.59096, target_loss = 0.38684, acc = 0.50000\n",
            "Epoch [3/5] Step [1232/1610]: discriminator_loss = 0.60822, target_loss = 0.40500, acc = 0.50000\n",
            "Epoch [3/5] Step [1233/1610]: discriminator_loss = 0.57899, target_loss = 0.38643, acc = 0.50000\n",
            "Epoch [3/5] Step [1234/1610]: discriminator_loss = 0.54639, target_loss = 0.42412, acc = 0.50000\n",
            "Epoch [3/5] Step [1235/1610]: discriminator_loss = 0.56041, target_loss = 0.43178, acc = 0.50000\n",
            "Epoch [3/5] Step [1236/1610]: discriminator_loss = 0.58982, target_loss = 0.37768, acc = 0.50000\n",
            "Epoch [3/5] Step [1237/1610]: discriminator_loss = 0.56901, target_loss = 0.39543, acc = 0.50000\n",
            "Epoch [3/5] Step [1238/1610]: discriminator_loss = 0.58367, target_loss = 0.42002, acc = 0.50000\n",
            "Epoch [3/5] Step [1239/1610]: discriminator_loss = 0.57248, target_loss = 0.39525, acc = 0.50000\n",
            "Epoch [3/5] Step [1240/1610]: discriminator_loss = 0.56782, target_loss = 0.38695, acc = 0.50000\n",
            "Epoch [3/5] Step [1241/1610]: discriminator_loss = 0.58042, target_loss = 0.39832, acc = 0.50000\n",
            "Epoch [3/5] Step [1242/1610]: discriminator_loss = 0.58560, target_loss = 0.38130, acc = 0.50000\n",
            "Epoch [3/5] Step [1243/1610]: discriminator_loss = 0.52004, target_loss = 0.42551, acc = 0.50000\n",
            "Epoch [3/5] Step [1244/1610]: discriminator_loss = 0.54630, target_loss = 0.40172, acc = 0.50000\n",
            "Epoch [3/5] Step [1245/1610]: discriminator_loss = 0.53765, target_loss = 0.41842, acc = 0.50000\n",
            "Epoch [3/5] Step [1246/1610]: discriminator_loss = 0.52727, target_loss = 0.45865, acc = 0.50000\n",
            "Epoch [3/5] Step [1247/1610]: discriminator_loss = 0.53461, target_loss = 0.42308, acc = 0.50000\n",
            "Epoch [3/5] Step [1248/1610]: discriminator_loss = 0.53674, target_loss = 0.42369, acc = 0.50000\n",
            "Epoch [3/5] Step [1249/1610]: discriminator_loss = 0.56990, target_loss = 0.38391, acc = 0.50000\n",
            "Epoch [3/5] Step [1250/1610]: discriminator_loss = 0.53684, target_loss = 0.43509, acc = 0.50000\n",
            "Epoch [3/5] Step [1251/1610]: discriminator_loss = 0.54770, target_loss = 0.41320, acc = 0.50000\n",
            "Epoch [3/5] Step [1252/1610]: discriminator_loss = 0.56518, target_loss = 0.41528, acc = 0.50000\n",
            "Epoch [3/5] Step [1253/1610]: discriminator_loss = 0.65033, target_loss = 0.43328, acc = 0.50000\n",
            "Epoch [3/5] Step [1254/1610]: discriminator_loss = 0.54239, target_loss = 0.42393, acc = 0.50000\n",
            "Epoch [3/5] Step [1255/1610]: discriminator_loss = 0.54854, target_loss = 0.42144, acc = 0.50000\n",
            "Epoch [3/5] Step [1256/1610]: discriminator_loss = 0.55492, target_loss = 0.42966, acc = 0.50000\n",
            "Epoch [3/5] Step [1257/1610]: discriminator_loss = 0.59443, target_loss = 0.41747, acc = 0.50000\n",
            "Epoch [3/5] Step [1258/1610]: discriminator_loss = 0.54541, target_loss = 0.41678, acc = 0.50000\n",
            "Epoch [3/5] Step [1259/1610]: discriminator_loss = 0.54941, target_loss = 0.44026, acc = 0.50000\n",
            "Epoch [3/5] Step [1260/1610]: discriminator_loss = 0.55186, target_loss = 0.40965, acc = 0.50000\n",
            "Epoch [3/5] Step [1261/1610]: discriminator_loss = 0.58993, target_loss = 0.38213, acc = 0.50000\n",
            "Epoch [3/5] Step [1262/1610]: discriminator_loss = 0.55145, target_loss = 0.40844, acc = 0.50000\n",
            "Epoch [3/5] Step [1263/1610]: discriminator_loss = 0.55154, target_loss = 0.39649, acc = 0.50000\n",
            "Epoch [3/5] Step [1264/1610]: discriminator_loss = 0.55702, target_loss = 0.39788, acc = 0.50000\n",
            "Epoch [3/5] Step [1265/1610]: discriminator_loss = 0.56106, target_loss = 0.39746, acc = 0.50000\n",
            "Epoch [3/5] Step [1266/1610]: discriminator_loss = 0.55809, target_loss = 0.39779, acc = 0.50000\n",
            "Epoch [3/5] Step [1267/1610]: discriminator_loss = 0.56425, target_loss = 0.37577, acc = 0.50000\n",
            "Epoch [3/5] Step [1268/1610]: discriminator_loss = 0.57473, target_loss = 0.38363, acc = 0.50000\n",
            "Epoch [3/5] Step [1269/1610]: discriminator_loss = 0.57461, target_loss = 0.38255, acc = 0.50000\n",
            "Epoch [3/5] Step [1270/1610]: discriminator_loss = 0.58307, target_loss = 0.35757, acc = 0.50000\n",
            "Epoch [3/5] Step [1271/1610]: discriminator_loss = 0.63380, target_loss = 0.38909, acc = 0.50000\n",
            "Epoch [3/5] Step [1272/1610]: discriminator_loss = 0.57321, target_loss = 0.34594, acc = 0.50000\n",
            "Epoch [3/5] Step [1273/1610]: discriminator_loss = 0.57070, target_loss = 0.39188, acc = 0.50000\n",
            "Epoch [3/5] Step [1274/1610]: discriminator_loss = 0.56691, target_loss = 0.35921, acc = 0.50000\n",
            "Epoch [3/5] Step [1275/1610]: discriminator_loss = 0.57738, target_loss = 0.41989, acc = 0.50000\n",
            "Epoch [3/5] Step [1276/1610]: discriminator_loss = 0.56682, target_loss = 0.38006, acc = 0.53125\n",
            "Epoch [3/5] Step [1277/1610]: discriminator_loss = 0.55994, target_loss = 0.38982, acc = 0.50000\n",
            "Epoch [3/5] Step [1278/1610]: discriminator_loss = 0.56483, target_loss = 0.38154, acc = 0.50000\n",
            "Epoch [3/5] Step [1279/1610]: discriminator_loss = 0.56512, target_loss = 0.40627, acc = 0.50000\n",
            "Epoch [3/5] Step [1280/1610]: discriminator_loss = 0.55131, target_loss = 0.44306, acc = 0.50000\n",
            "Epoch [3/5] Step [1281/1610]: discriminator_loss = 0.54231, target_loss = 0.40305, acc = 0.50000\n",
            "Epoch [3/5] Step [1282/1610]: discriminator_loss = 0.57293, target_loss = 0.39301, acc = 0.50000\n",
            "Epoch [3/5] Step [1283/1610]: discriminator_loss = 0.54627, target_loss = 0.40211, acc = 0.50000\n",
            "Epoch [3/5] Step [1284/1610]: discriminator_loss = 0.58263, target_loss = 0.39245, acc = 0.50000\n",
            "Epoch [3/5] Step [1285/1610]: discriminator_loss = 0.56634, target_loss = 0.41065, acc = 0.50000\n",
            "Epoch [3/5] Step [1286/1610]: discriminator_loss = 0.58481, target_loss = 0.41179, acc = 0.50000\n",
            "Epoch [3/5] Step [1287/1610]: discriminator_loss = 0.56445, target_loss = 0.42083, acc = 0.50000\n",
            "Epoch [3/5] Step [1288/1610]: discriminator_loss = 0.54921, target_loss = 0.42057, acc = 0.50000\n",
            "Epoch [3/5] Step [1289/1610]: discriminator_loss = 0.56940, target_loss = 0.44327, acc = 0.50000\n",
            "Epoch [3/5] Step [1290/1610]: discriminator_loss = 0.57664, target_loss = 0.39737, acc = 0.50000\n",
            "Epoch [3/5] Step [1291/1610]: discriminator_loss = 0.54970, target_loss = 0.43161, acc = 0.50000\n",
            "Epoch [3/5] Step [1292/1610]: discriminator_loss = 0.53984, target_loss = 0.41980, acc = 0.50000\n",
            "Epoch [3/5] Step [1293/1610]: discriminator_loss = 0.53918, target_loss = 0.41754, acc = 0.50000\n",
            "Epoch [3/5] Step [1294/1610]: discriminator_loss = 0.52804, target_loss = 0.43303, acc = 0.50000\n",
            "Epoch [3/5] Step [1295/1610]: discriminator_loss = 0.55585, target_loss = 0.42073, acc = 0.50000\n",
            "Epoch [3/5] Step [1296/1610]: discriminator_loss = 0.55198, target_loss = 0.43556, acc = 0.50000\n",
            "Epoch [3/5] Step [1297/1610]: discriminator_loss = 0.54864, target_loss = 0.41777, acc = 0.50000\n",
            "Epoch [3/5] Step [1298/1610]: discriminator_loss = 0.53777, target_loss = 0.38820, acc = 0.50000\n",
            "Epoch [3/5] Step [1299/1610]: discriminator_loss = 0.54783, target_loss = 0.39913, acc = 0.50000\n",
            "Epoch [3/5] Step [1300/1610]: discriminator_loss = 0.56442, target_loss = 0.39841, acc = 0.50000\n",
            "Epoch [3/5] Step [1301/1610]: discriminator_loss = 0.58186, target_loss = 0.39094, acc = 0.50000\n",
            "Epoch [3/5] Step [1302/1610]: discriminator_loss = 0.58409, target_loss = 0.39085, acc = 0.50000\n",
            "Epoch [3/5] Step [1303/1610]: discriminator_loss = 0.58627, target_loss = 0.38217, acc = 0.50000\n",
            "Epoch [3/5] Step [1304/1610]: discriminator_loss = 0.56776, target_loss = 0.37240, acc = 0.50000\n",
            "Epoch [3/5] Step [1305/1610]: discriminator_loss = 0.57552, target_loss = 0.38211, acc = 0.50000\n",
            "Epoch [3/5] Step [1306/1610]: discriminator_loss = 0.56044, target_loss = 0.38514, acc = 0.50000\n",
            "Epoch [3/5] Step [1307/1610]: discriminator_loss = 0.56914, target_loss = 0.38006, acc = 0.50000\n",
            "Epoch [3/5] Step [1308/1610]: discriminator_loss = 0.64971, target_loss = 0.35412, acc = 0.50000\n",
            "Epoch [3/5] Step [1309/1610]: discriminator_loss = 0.66574, target_loss = 0.38139, acc = 0.50000\n",
            "Epoch [3/5] Step [1310/1610]: discriminator_loss = 0.55073, target_loss = 0.40960, acc = 0.50000\n",
            "Epoch [3/5] Step [1311/1610]: discriminator_loss = 0.56739, target_loss = 0.40012, acc = 0.50000\n",
            "Epoch [3/5] Step [1312/1610]: discriminator_loss = 0.54609, target_loss = 0.40040, acc = 0.50000\n",
            "Epoch [3/5] Step [1313/1610]: discriminator_loss = 0.54295, target_loss = 0.41283, acc = 0.50000\n",
            "Epoch [3/5] Step [1314/1610]: discriminator_loss = 0.53932, target_loss = 0.39951, acc = 0.50000\n",
            "Epoch [3/5] Step [1315/1610]: discriminator_loss = 0.56465, target_loss = 0.41190, acc = 0.50000\n",
            "Epoch [3/5] Step [1316/1610]: discriminator_loss = 0.56246, target_loss = 0.42296, acc = 0.50000\n",
            "Epoch [3/5] Step [1317/1610]: discriminator_loss = 0.54797, target_loss = 0.42074, acc = 0.50000\n",
            "Epoch [3/5] Step [1318/1610]: discriminator_loss = 0.55821, target_loss = 0.40655, acc = 0.50000\n",
            "Epoch [3/5] Step [1319/1610]: discriminator_loss = 0.54242, target_loss = 0.42085, acc = 0.50000\n",
            "Epoch [3/5] Step [1320/1610]: discriminator_loss = 0.55261, target_loss = 0.39588, acc = 0.50000\n",
            "Epoch [3/5] Step [1321/1610]: discriminator_loss = 0.55994, target_loss = 0.38966, acc = 0.50000\n",
            "Epoch [3/5] Step [1322/1610]: discriminator_loss = 0.57882, target_loss = 0.40205, acc = 0.50000\n",
            "Epoch [3/5] Step [1323/1610]: discriminator_loss = 0.57311, target_loss = 0.40532, acc = 0.50000\n",
            "Epoch [3/5] Step [1324/1610]: discriminator_loss = 0.54117, target_loss = 0.41179, acc = 0.50000\n",
            "Epoch [3/5] Step [1325/1610]: discriminator_loss = 0.55579, target_loss = 0.36256, acc = 0.50000\n",
            "Epoch [3/5] Step [1326/1610]: discriminator_loss = 0.60763, target_loss = 0.39927, acc = 0.50000\n",
            "Epoch [3/5] Step [1327/1610]: discriminator_loss = 0.58301, target_loss = 0.40984, acc = 0.50000\n",
            "Epoch [3/5] Step [1328/1610]: discriminator_loss = 0.56248, target_loss = 0.39528, acc = 0.50000\n",
            "Epoch [3/5] Step [1329/1610]: discriminator_loss = 0.59271, target_loss = 0.39949, acc = 0.50000\n",
            "Epoch [3/5] Step [1330/1610]: discriminator_loss = 0.57016, target_loss = 0.39891, acc = 0.50000\n",
            "Epoch [3/5] Step [1331/1610]: discriminator_loss = 0.58011, target_loss = 0.38171, acc = 0.50000\n",
            "Epoch [3/5] Step [1332/1610]: discriminator_loss = 0.60323, target_loss = 0.39800, acc = 0.50000\n",
            "Epoch [3/5] Step [1333/1610]: discriminator_loss = 0.59819, target_loss = 0.40658, acc = 0.50000\n",
            "Epoch [3/5] Step [1334/1610]: discriminator_loss = 0.65864, target_loss = 0.40765, acc = 0.50000\n",
            "Epoch [3/5] Step [1335/1610]: discriminator_loss = 0.61070, target_loss = 0.42949, acc = 0.50000\n",
            "Epoch [3/5] Step [1336/1610]: discriminator_loss = 0.62588, target_loss = 0.42550, acc = 0.50000\n",
            "Epoch [3/5] Step [1337/1610]: discriminator_loss = 0.54356, target_loss = 0.44037, acc = 0.50000\n",
            "Epoch [3/5] Step [1338/1610]: discriminator_loss = 0.55511, target_loss = 0.42977, acc = 0.50000\n",
            "Epoch [3/5] Step [1339/1610]: discriminator_loss = 0.54478, target_loss = 0.43194, acc = 0.50000\n",
            "Epoch [3/5] Step [1340/1610]: discriminator_loss = 0.55777, target_loss = 0.42328, acc = 0.50000\n",
            "Epoch [3/5] Step [1341/1610]: discriminator_loss = 0.58708, target_loss = 0.43695, acc = 0.50000\n",
            "Epoch [3/5] Step [1342/1610]: discriminator_loss = 0.54022, target_loss = 0.42548, acc = 0.50000\n",
            "Epoch [3/5] Step [1343/1610]: discriminator_loss = 0.55663, target_loss = 0.41633, acc = 0.50000\n",
            "Epoch [3/5] Step [1344/1610]: discriminator_loss = 0.59387, target_loss = 0.41140, acc = 0.50000\n",
            "Epoch [3/5] Step [1345/1610]: discriminator_loss = 0.58538, target_loss = 0.39130, acc = 0.50000\n",
            "Epoch [3/5] Step [1346/1610]: discriminator_loss = 0.60174, target_loss = 0.39422, acc = 0.50000\n",
            "Epoch [3/5] Step [1347/1610]: discriminator_loss = 0.59425, target_loss = 0.37436, acc = 0.50000\n",
            "Epoch [3/5] Step [1348/1610]: discriminator_loss = 0.62217, target_loss = 0.39138, acc = 0.50000\n",
            "Epoch [3/5] Step [1349/1610]: discriminator_loss = 0.61796, target_loss = 0.37277, acc = 0.50000\n",
            "Epoch [3/5] Step [1350/1610]: discriminator_loss = 0.56253, target_loss = 0.38751, acc = 0.50000\n",
            "Epoch [3/5] Step [1351/1610]: discriminator_loss = 0.61885, target_loss = 0.37671, acc = 0.50000\n",
            "Epoch [3/5] Step [1352/1610]: discriminator_loss = 0.61245, target_loss = 0.37438, acc = 0.50000\n",
            "Epoch [3/5] Step [1353/1610]: discriminator_loss = 0.61023, target_loss = 0.37749, acc = 0.50000\n",
            "Epoch [3/5] Step [1354/1610]: discriminator_loss = 0.59459, target_loss = 0.38287, acc = 0.50000\n",
            "Epoch [3/5] Step [1355/1610]: discriminator_loss = 0.57999, target_loss = 0.37704, acc = 0.50000\n",
            "Epoch [3/5] Step [1356/1610]: discriminator_loss = 0.59805, target_loss = 0.38472, acc = 0.50000\n",
            "Epoch [3/5] Step [1357/1610]: discriminator_loss = 0.57536, target_loss = 0.37210, acc = 0.50000\n",
            "Epoch [3/5] Step [1358/1610]: discriminator_loss = 0.61217, target_loss = 0.36207, acc = 0.50000\n",
            "Epoch [3/5] Step [1359/1610]: discriminator_loss = 0.59912, target_loss = 0.38841, acc = 0.50000\n",
            "Epoch [3/5] Step [1360/1610]: discriminator_loss = 0.56004, target_loss = 0.38222, acc = 0.50000\n",
            "Epoch [3/5] Step [1361/1610]: discriminator_loss = 0.55211, target_loss = 0.39576, acc = 0.50000\n",
            "Epoch [3/5] Step [1362/1610]: discriminator_loss = 0.56697, target_loss = 0.42424, acc = 0.50000\n",
            "Epoch [3/5] Step [1363/1610]: discriminator_loss = 0.56586, target_loss = 0.40287, acc = 0.50000\n",
            "Epoch [3/5] Step [1364/1610]: discriminator_loss = 0.55201, target_loss = 0.39499, acc = 0.50000\n",
            "Epoch [3/5] Step [1365/1610]: discriminator_loss = 0.61990, target_loss = 0.40400, acc = 0.50000\n",
            "Epoch [3/5] Step [1366/1610]: discriminator_loss = 0.56342, target_loss = 0.40003, acc = 0.50000\n",
            "Epoch [3/5] Step [1367/1610]: discriminator_loss = 0.55448, target_loss = 0.39090, acc = 0.50000\n",
            "Epoch [3/5] Step [1368/1610]: discriminator_loss = 0.55817, target_loss = 0.39875, acc = 0.50000\n",
            "Epoch [3/5] Step [1369/1610]: discriminator_loss = 0.58985, target_loss = 0.41113, acc = 0.50000\n",
            "Epoch [3/5] Step [1370/1610]: discriminator_loss = 0.53023, target_loss = 0.41765, acc = 0.50000\n",
            "Epoch [3/5] Step [1371/1610]: discriminator_loss = 0.55491, target_loss = 0.42543, acc = 0.50000\n",
            "Epoch [3/5] Step [1372/1610]: discriminator_loss = 0.53861, target_loss = 0.41899, acc = 0.50000\n",
            "Epoch [3/5] Step [1373/1610]: discriminator_loss = 0.53894, target_loss = 0.44954, acc = 0.50000\n",
            "Epoch [3/5] Step [1374/1610]: discriminator_loss = 0.55018, target_loss = 0.41303, acc = 0.50000\n",
            "Epoch [3/5] Step [1375/1610]: discriminator_loss = 0.53349, target_loss = 0.40908, acc = 0.50000\n",
            "Epoch [3/5] Step [1376/1610]: discriminator_loss = 0.53818, target_loss = 0.45940, acc = 0.50000\n",
            "Epoch [3/5] Step [1377/1610]: discriminator_loss = 0.54824, target_loss = 0.41367, acc = 0.50000\n",
            "Epoch [3/5] Step [1378/1610]: discriminator_loss = 0.54957, target_loss = 0.39453, acc = 0.50000\n",
            "Epoch [3/5] Step [1379/1610]: discriminator_loss = 0.62467, target_loss = 0.40093, acc = 0.50000\n",
            "Epoch [3/5] Step [1380/1610]: discriminator_loss = 0.55783, target_loss = 0.41290, acc = 0.50000\n",
            "Epoch [3/5] Step [1381/1610]: discriminator_loss = 0.56701, target_loss = 0.38504, acc = 0.50000\n",
            "Epoch [3/5] Step [1382/1610]: discriminator_loss = 0.57716, target_loss = 0.35730, acc = 0.50000\n",
            "Epoch [3/5] Step [1383/1610]: discriminator_loss = 0.56251, target_loss = 0.39067, acc = 0.50000\n",
            "Epoch [3/5] Step [1384/1610]: discriminator_loss = 0.54958, target_loss = 0.38376, acc = 0.50000\n",
            "Epoch [3/5] Step [1385/1610]: discriminator_loss = 0.56791, target_loss = 0.38447, acc = 0.50000\n",
            "Epoch [3/5] Step [1386/1610]: discriminator_loss = 0.56132, target_loss = 0.38848, acc = 0.50000\n",
            "Epoch [3/5] Step [1387/1610]: discriminator_loss = 0.57708, target_loss = 0.38215, acc = 0.50000\n",
            "Epoch [3/5] Step [1388/1610]: discriminator_loss = 0.56331, target_loss = 0.39296, acc = 0.50000\n",
            "Epoch [3/5] Step [1389/1610]: discriminator_loss = 0.57085, target_loss = 0.37228, acc = 0.50000\n",
            "Epoch [3/5] Step [1390/1610]: discriminator_loss = 0.59772, target_loss = 0.39904, acc = 0.50000\n",
            "Epoch [3/5] Step [1391/1610]: discriminator_loss = 0.56516, target_loss = 0.37411, acc = 0.50000\n",
            "Epoch [3/5] Step [1392/1610]: discriminator_loss = 0.58296, target_loss = 0.38700, acc = 0.50000\n",
            "Epoch [3/5] Step [1393/1610]: discriminator_loss = 0.67833, target_loss = 0.36595, acc = 0.50000\n",
            "Epoch [3/5] Step [1394/1610]: discriminator_loss = 0.56392, target_loss = 0.37579, acc = 0.50000\n",
            "Epoch [3/5] Step [1395/1610]: discriminator_loss = 0.54181, target_loss = 0.37476, acc = 0.50000\n",
            "Epoch [3/5] Step [1396/1610]: discriminator_loss = 0.54710, target_loss = 0.40352, acc = 0.50000\n",
            "Epoch [3/5] Step [1397/1610]: discriminator_loss = 0.55726, target_loss = 0.41623, acc = 0.50000\n",
            "Epoch [3/5] Step [1398/1610]: discriminator_loss = 0.56171, target_loss = 0.42851, acc = 0.50000\n",
            "Epoch [3/5] Step [1399/1610]: discriminator_loss = 0.54293, target_loss = 0.42729, acc = 0.50000\n",
            "Epoch [3/5] Step [1400/1610]: discriminator_loss = 0.51765, target_loss = 0.42882, acc = 0.50000\n",
            "Epoch [3/5] Step [1401/1610]: discriminator_loss = 0.53221, target_loss = 0.41570, acc = 0.50000\n",
            "Epoch [3/5] Step [1402/1610]: discriminator_loss = 0.52750, target_loss = 0.41720, acc = 0.50000\n",
            "Epoch [3/5] Step [1403/1610]: discriminator_loss = 0.52717, target_loss = 0.41383, acc = 0.50000\n",
            "Epoch [3/5] Step [1404/1610]: discriminator_loss = 0.51834, target_loss = 0.42632, acc = 0.50000\n",
            "Epoch [3/5] Step [1405/1610]: discriminator_loss = 0.52210, target_loss = 0.44283, acc = 0.50000\n",
            "Epoch [3/5] Step [1406/1610]: discriminator_loss = 0.69312, target_loss = 0.41157, acc = 0.50000\n",
            "Epoch [3/5] Step [1407/1610]: discriminator_loss = 0.57370, target_loss = 0.41089, acc = 0.50000\n",
            "Epoch [3/5] Step [1408/1610]: discriminator_loss = 0.52681, target_loss = 0.42937, acc = 0.50000\n",
            "Epoch [3/5] Step [1409/1610]: discriminator_loss = 0.56256, target_loss = 0.43096, acc = 0.50000\n",
            "Epoch [3/5] Step [1410/1610]: discriminator_loss = 0.57734, target_loss = 0.42582, acc = 0.50000\n",
            "Epoch [3/5] Step [1411/1610]: discriminator_loss = 0.53164, target_loss = 0.43716, acc = 0.50000\n",
            "Epoch [3/5] Step [1412/1610]: discriminator_loss = 0.56478, target_loss = 0.39983, acc = 0.50000\n",
            "Epoch [3/5] Step [1413/1610]: discriminator_loss = 0.55752, target_loss = 0.40463, acc = 0.50000\n",
            "Epoch [3/5] Step [1414/1610]: discriminator_loss = 0.57179, target_loss = 0.40887, acc = 0.50000\n",
            "Epoch [3/5] Step [1415/1610]: discriminator_loss = 0.55575, target_loss = 0.38354, acc = 0.50000\n",
            "Epoch [3/5] Step [1416/1610]: discriminator_loss = 0.58555, target_loss = 0.41903, acc = 0.50000\n",
            "Epoch [3/5] Step [1417/1610]: discriminator_loss = 0.52943, target_loss = 0.39745, acc = 0.50000\n",
            "Epoch [3/5] Step [1418/1610]: discriminator_loss = 0.58995, target_loss = 0.39060, acc = 0.50000\n",
            "Epoch [3/5] Step [1419/1610]: discriminator_loss = 0.55163, target_loss = 0.41759, acc = 0.50000\n",
            "Epoch [3/5] Step [1420/1610]: discriminator_loss = 0.54440, target_loss = 0.40008, acc = 0.50000\n",
            "Epoch [3/5] Step [1421/1610]: discriminator_loss = 0.54630, target_loss = 0.42030, acc = 0.50000\n",
            "Epoch [3/5] Step [1422/1610]: discriminator_loss = 0.55475, target_loss = 0.38639, acc = 0.50000\n",
            "Epoch [3/5] Step [1423/1610]: discriminator_loss = 0.55382, target_loss = 0.36215, acc = 0.50000\n",
            "Epoch [3/5] Step [1424/1610]: discriminator_loss = 0.63425, target_loss = 0.39993, acc = 0.50000\n",
            "Epoch [3/5] Step [1425/1610]: discriminator_loss = 0.56388, target_loss = 0.39950, acc = 0.50000\n",
            "Epoch [3/5] Step [1426/1610]: discriminator_loss = 0.56510, target_loss = 0.40722, acc = 0.50000\n",
            "Epoch [3/5] Step [1427/1610]: discriminator_loss = 0.57176, target_loss = 0.40343, acc = 0.50000\n",
            "Epoch [3/5] Step [1428/1610]: discriminator_loss = 0.56838, target_loss = 0.40149, acc = 0.50000\n",
            "Epoch [3/5] Step [1429/1610]: discriminator_loss = 0.59895, target_loss = 0.40591, acc = 0.50000\n",
            "Epoch [3/5] Step [1430/1610]: discriminator_loss = 0.60498, target_loss = 0.40272, acc = 0.50000\n",
            "Epoch [3/5] Step [1431/1610]: discriminator_loss = 0.56920, target_loss = 0.40290, acc = 0.50000\n",
            "Epoch [3/5] Step [1432/1610]: discriminator_loss = 0.55608, target_loss = 0.39397, acc = 0.50000\n",
            "Epoch [3/5] Step [1433/1610]: discriminator_loss = 0.56791, target_loss = 0.38707, acc = 0.50000\n",
            "Epoch [3/5] Step [1434/1610]: discriminator_loss = 0.58850, target_loss = 0.40237, acc = 0.50000\n",
            "Epoch [3/5] Step [1435/1610]: discriminator_loss = 0.56783, target_loss = 0.38014, acc = 0.50000\n",
            "Epoch [3/5] Step [1436/1610]: discriminator_loss = 0.56781, target_loss = 0.41364, acc = 0.50000\n",
            "Epoch [3/5] Step [1437/1610]: discriminator_loss = 0.54780, target_loss = 0.38076, acc = 0.50000\n",
            "Epoch [3/5] Step [1438/1610]: discriminator_loss = 0.61159, target_loss = 0.38562, acc = 0.50000\n",
            "Epoch [3/5] Step [1439/1610]: discriminator_loss = 0.57068, target_loss = 0.35486, acc = 0.50000\n",
            "Epoch [3/5] Step [1440/1610]: discriminator_loss = 0.61893, target_loss = 0.39613, acc = 0.50000\n",
            "Epoch [3/5] Step [1441/1610]: discriminator_loss = 0.58244, target_loss = 0.36817, acc = 0.50000\n",
            "Epoch [3/5] Step [1442/1610]: discriminator_loss = 0.55688, target_loss = 0.36623, acc = 0.50000\n",
            "Epoch [3/5] Step [1443/1610]: discriminator_loss = 0.61794, target_loss = 0.38582, acc = 0.50000\n",
            "Epoch [3/5] Step [1444/1610]: discriminator_loss = 0.56614, target_loss = 0.36996, acc = 0.50000\n",
            "Epoch [3/5] Step [1445/1610]: discriminator_loss = 0.59435, target_loss = 0.39872, acc = 0.50000\n",
            "Epoch [3/5] Step [1446/1610]: discriminator_loss = 0.59273, target_loss = 0.38978, acc = 0.50000\n",
            "Epoch [3/5] Step [1447/1610]: discriminator_loss = 0.60604, target_loss = 0.41757, acc = 0.50000\n",
            "Epoch [3/5] Step [1448/1610]: discriminator_loss = 0.55936, target_loss = 0.43018, acc = 0.50000\n",
            "Epoch [3/5] Step [1449/1610]: discriminator_loss = 0.53877, target_loss = 0.42074, acc = 0.50000\n",
            "Epoch [3/5] Step [1450/1610]: discriminator_loss = 0.55042, target_loss = 0.40275, acc = 0.50000\n",
            "Epoch [3/5] Step [1451/1610]: discriminator_loss = 0.55478, target_loss = 0.41847, acc = 0.50000\n",
            "Epoch [3/5] Step [1452/1610]: discriminator_loss = 0.55934, target_loss = 0.42089, acc = 0.50000\n",
            "Epoch [3/5] Step [1453/1610]: discriminator_loss = 0.53238, target_loss = 0.41313, acc = 0.50000\n",
            "Epoch [3/5] Step [1454/1610]: discriminator_loss = 0.54561, target_loss = 0.42087, acc = 0.50000\n",
            "Epoch [3/5] Step [1455/1610]: discriminator_loss = 0.54187, target_loss = 0.42031, acc = 0.50000\n",
            "Epoch [3/5] Step [1456/1610]: discriminator_loss = 0.56372, target_loss = 0.39706, acc = 0.50000\n",
            "Epoch [3/5] Step [1457/1610]: discriminator_loss = 0.54028, target_loss = 0.40251, acc = 0.50000\n",
            "Epoch [3/5] Step [1458/1610]: discriminator_loss = 0.56813, target_loss = 0.38639, acc = 0.50000\n",
            "Epoch [3/5] Step [1459/1610]: discriminator_loss = 0.59766, target_loss = 0.39286, acc = 0.50000\n",
            "Epoch [3/5] Step [1460/1610]: discriminator_loss = 0.54045, target_loss = 0.43496, acc = 0.50000\n",
            "Epoch [3/5] Step [1461/1610]: discriminator_loss = 0.57878, target_loss = 0.38958, acc = 0.50000\n",
            "Epoch [3/5] Step [1462/1610]: discriminator_loss = 0.54186, target_loss = 0.42797, acc = 0.50000\n",
            "Epoch [3/5] Step [1463/1610]: discriminator_loss = 0.56626, target_loss = 0.39145, acc = 0.50000\n",
            "Epoch [3/5] Step [1464/1610]: discriminator_loss = 0.56316, target_loss = 0.41436, acc = 0.50000\n",
            "Epoch [3/5] Step [1465/1610]: discriminator_loss = 0.58972, target_loss = 0.41377, acc = 0.50000\n",
            "Epoch [3/5] Step [1466/1610]: discriminator_loss = 0.56419, target_loss = 0.40410, acc = 0.50000\n",
            "Epoch [3/5] Step [1467/1610]: discriminator_loss = 0.56453, target_loss = 0.40325, acc = 0.50000\n",
            "Epoch [3/5] Step [1468/1610]: discriminator_loss = 0.55561, target_loss = 0.39093, acc = 0.50000\n",
            "Epoch [3/5] Step [1469/1610]: discriminator_loss = 0.56453, target_loss = 0.39521, acc = 0.50000\n",
            "Epoch [3/5] Step [1470/1610]: discriminator_loss = 0.56771, target_loss = 0.39112, acc = 0.50000\n",
            "Epoch [3/5] Step [1471/1610]: discriminator_loss = 0.56249, target_loss = 0.41201, acc = 0.50000\n",
            "Epoch [3/5] Step [1472/1610]: discriminator_loss = 0.56924, target_loss = 0.38460, acc = 0.50000\n",
            "Epoch [3/5] Step [1473/1610]: discriminator_loss = 0.57250, target_loss = 0.37675, acc = 0.50000\n",
            "Epoch [3/5] Step [1474/1610]: discriminator_loss = 0.55720, target_loss = 0.39658, acc = 0.50000\n",
            "Epoch [3/5] Step [1475/1610]: discriminator_loss = 0.56245, target_loss = 0.41884, acc = 0.50000\n",
            "Epoch [3/5] Step [1476/1610]: discriminator_loss = 0.59247, target_loss = 0.38782, acc = 0.50000\n",
            "Epoch [3/5] Step [1477/1610]: discriminator_loss = 0.57548, target_loss = 0.38155, acc = 0.50000\n",
            "Epoch [3/5] Step [1478/1610]: discriminator_loss = 0.56485, target_loss = 0.39662, acc = 0.50000\n",
            "Epoch [3/5] Step [1479/1610]: discriminator_loss = 0.54221, target_loss = 0.41783, acc = 0.50000\n",
            "Epoch [3/5] Step [1480/1610]: discriminator_loss = 0.53362, target_loss = 0.40483, acc = 0.50000\n",
            "Epoch [3/5] Step [1481/1610]: discriminator_loss = 0.56730, target_loss = 0.39478, acc = 0.50000\n",
            "Epoch [3/5] Step [1482/1610]: discriminator_loss = 0.58392, target_loss = 0.40918, acc = 0.50000\n",
            "Epoch [3/5] Step [1483/1610]: discriminator_loss = 0.55916, target_loss = 0.40891, acc = 0.50000\n",
            "Epoch [3/5] Step [1484/1610]: discriminator_loss = 0.57378, target_loss = 0.41843, acc = 0.50000\n",
            "Epoch [3/5] Step [1485/1610]: discriminator_loss = 0.61933, target_loss = 0.38493, acc = 0.50000\n",
            "Epoch [3/5] Step [1486/1610]: discriminator_loss = 0.56841, target_loss = 0.39882, acc = 0.50000\n",
            "Epoch [3/5] Step [1487/1610]: discriminator_loss = 0.56134, target_loss = 0.39326, acc = 0.50000\n",
            "Epoch [3/5] Step [1488/1610]: discriminator_loss = 0.55293, target_loss = 0.41641, acc = 0.50000\n",
            "Epoch [3/5] Step [1489/1610]: discriminator_loss = 0.53497, target_loss = 0.40231, acc = 0.50000\n",
            "Epoch [3/5] Step [1490/1610]: discriminator_loss = 0.56046, target_loss = 0.39134, acc = 0.50000\n",
            "Epoch [3/5] Step [1491/1610]: discriminator_loss = 0.54832, target_loss = 0.41429, acc = 0.50000\n",
            "Epoch [3/5] Step [1492/1610]: discriminator_loss = 0.55346, target_loss = 0.40893, acc = 0.50000\n",
            "Epoch [3/5] Step [1493/1610]: discriminator_loss = 0.58280, target_loss = 0.41093, acc = 0.50000\n",
            "Epoch [3/5] Step [1494/1610]: discriminator_loss = 0.54676, target_loss = 0.38163, acc = 0.50000\n",
            "Epoch [3/5] Step [1495/1610]: discriminator_loss = 0.53978, target_loss = 0.41796, acc = 0.50000\n",
            "Epoch [3/5] Step [1496/1610]: discriminator_loss = 0.61203, target_loss = 0.40241, acc = 0.50000\n",
            "Epoch [3/5] Step [1497/1610]: discriminator_loss = 0.57350, target_loss = 0.38319, acc = 0.50000\n",
            "Epoch [3/5] Step [1498/1610]: discriminator_loss = 0.57276, target_loss = 0.40836, acc = 0.50000\n",
            "Epoch [3/5] Step [1499/1610]: discriminator_loss = 0.55948, target_loss = 0.40391, acc = 0.50000\n",
            "Epoch [3/5] Step [1500/1610]: discriminator_loss = 0.60101, target_loss = 0.38635, acc = 0.50000\n",
            "Epoch [3/5] Step [1501/1610]: discriminator_loss = 0.54602, target_loss = 0.39416, acc = 0.50000\n",
            "Epoch [3/5] Step [1502/1610]: discriminator_loss = 0.57251, target_loss = 0.41808, acc = 0.50000\n",
            "Epoch [3/5] Step [1503/1610]: discriminator_loss = 0.56423, target_loss = 0.40534, acc = 0.50000\n",
            "Epoch [3/5] Step [1504/1610]: discriminator_loss = 0.57389, target_loss = 0.40325, acc = 0.50000\n",
            "Epoch [3/5] Step [1505/1610]: discriminator_loss = 0.55782, target_loss = 0.40569, acc = 0.50000\n",
            "Epoch [3/5] Step [1506/1610]: discriminator_loss = 0.56175, target_loss = 0.41250, acc = 0.50000\n",
            "Epoch [3/5] Step [1507/1610]: discriminator_loss = 0.56289, target_loss = 0.42312, acc = 0.50000\n",
            "Epoch [3/5] Step [1508/1610]: discriminator_loss = 0.55732, target_loss = 0.42466, acc = 0.50000\n",
            "Epoch [3/5] Step [1509/1610]: discriminator_loss = 0.59494, target_loss = 0.40443, acc = 0.50000\n",
            "Epoch [3/5] Step [1510/1610]: discriminator_loss = 0.54848, target_loss = 0.42504, acc = 0.50000\n",
            "Epoch [3/5] Step [1511/1610]: discriminator_loss = 0.57203, target_loss = 0.41728, acc = 0.50000\n",
            "Epoch [3/5] Step [1512/1610]: discriminator_loss = 0.56767, target_loss = 0.41790, acc = 0.50000\n",
            "Epoch [3/5] Step [1513/1610]: discriminator_loss = 0.55298, target_loss = 0.39553, acc = 0.50000\n",
            "Epoch [3/5] Step [1514/1610]: discriminator_loss = 0.54525, target_loss = 0.41953, acc = 0.50000\n",
            "Epoch [3/5] Step [1515/1610]: discriminator_loss = 0.53612, target_loss = 0.41584, acc = 0.50000\n",
            "Epoch [3/5] Step [1516/1610]: discriminator_loss = 0.55978, target_loss = 0.42278, acc = 0.50000\n",
            "Epoch [3/5] Step [1517/1610]: discriminator_loss = 0.56482, target_loss = 0.40852, acc = 0.50000\n",
            "Epoch [3/5] Step [1518/1610]: discriminator_loss = 0.56978, target_loss = 0.39840, acc = 0.50000\n",
            "Epoch [3/5] Step [1519/1610]: discriminator_loss = 0.57108, target_loss = 0.38463, acc = 0.50000\n",
            "Epoch [3/5] Step [1520/1610]: discriminator_loss = 0.58180, target_loss = 0.37600, acc = 0.50000\n",
            "Epoch [3/5] Step [1521/1610]: discriminator_loss = 0.56152, target_loss = 0.37671, acc = 0.50000\n",
            "Epoch [3/5] Step [1522/1610]: discriminator_loss = 0.58766, target_loss = 0.38181, acc = 0.50000\n",
            "Epoch [3/5] Step [1523/1610]: discriminator_loss = 0.57887, target_loss = 0.37492, acc = 0.50000\n",
            "Epoch [3/5] Step [1524/1610]: discriminator_loss = 0.57583, target_loss = 0.37654, acc = 0.50000\n",
            "Epoch [3/5] Step [1525/1610]: discriminator_loss = 0.57924, target_loss = 0.37045, acc = 0.50000\n",
            "Epoch [3/5] Step [1526/1610]: discriminator_loss = 0.61797, target_loss = 0.36423, acc = 0.50000\n",
            "Epoch [3/5] Step [1527/1610]: discriminator_loss = 0.61989, target_loss = 0.36947, acc = 0.50000\n",
            "Epoch [3/5] Step [1528/1610]: discriminator_loss = 0.62023, target_loss = 0.36559, acc = 0.50000\n",
            "Epoch [3/5] Step [1529/1610]: discriminator_loss = 0.56083, target_loss = 0.37587, acc = 0.50000\n",
            "Epoch [3/5] Step [1530/1610]: discriminator_loss = 0.60495, target_loss = 0.41482, acc = 0.50000\n",
            "Epoch [3/5] Step [1531/1610]: discriminator_loss = 0.56775, target_loss = 0.40269, acc = 0.50000\n",
            "Epoch [3/5] Step [1532/1610]: discriminator_loss = 0.54807, target_loss = 0.37568, acc = 0.50000\n",
            "Epoch [3/5] Step [1533/1610]: discriminator_loss = 0.61712, target_loss = 0.39787, acc = 0.50000\n",
            "Epoch [3/5] Step [1534/1610]: discriminator_loss = 0.54778, target_loss = 0.39079, acc = 0.50000\n",
            "Epoch [3/5] Step [1535/1610]: discriminator_loss = 0.58136, target_loss = 0.39994, acc = 0.50000\n",
            "Epoch [3/5] Step [1536/1610]: discriminator_loss = 0.54905, target_loss = 0.41486, acc = 0.50000\n",
            "Epoch [3/5] Step [1537/1610]: discriminator_loss = 0.53887, target_loss = 0.42573, acc = 0.50000\n",
            "Epoch [3/5] Step [1538/1610]: discriminator_loss = 0.53777, target_loss = 0.40792, acc = 0.50000\n",
            "Epoch [3/5] Step [1539/1610]: discriminator_loss = 0.54751, target_loss = 0.45253, acc = 0.50000\n",
            "Epoch [3/5] Step [1540/1610]: discriminator_loss = 0.57807, target_loss = 0.42366, acc = 0.50000\n",
            "Epoch [3/5] Step [1541/1610]: discriminator_loss = 0.56238, target_loss = 0.41582, acc = 0.50000\n",
            "Epoch [3/5] Step [1542/1610]: discriminator_loss = 0.54335, target_loss = 0.42882, acc = 0.50000\n",
            "Epoch [3/5] Step [1543/1610]: discriminator_loss = 0.53982, target_loss = 0.41184, acc = 0.50000\n",
            "Epoch [3/5] Step [1544/1610]: discriminator_loss = 0.56882, target_loss = 0.43954, acc = 0.50000\n",
            "Epoch [3/5] Step [1545/1610]: discriminator_loss = 0.55998, target_loss = 0.39656, acc = 0.50000\n",
            "Epoch [3/5] Step [1546/1610]: discriminator_loss = 0.56280, target_loss = 0.41129, acc = 0.50000\n",
            "Epoch [3/5] Step [1547/1610]: discriminator_loss = 0.58196, target_loss = 0.40406, acc = 0.50000\n",
            "Epoch [3/5] Step [1548/1610]: discriminator_loss = 0.54416, target_loss = 0.40285, acc = 0.50000\n",
            "Epoch [3/5] Step [1549/1610]: discriminator_loss = 0.57411, target_loss = 0.38224, acc = 0.50000\n",
            "Epoch [3/5] Step [1550/1610]: discriminator_loss = 0.56640, target_loss = 0.39286, acc = 0.50000\n",
            "Epoch [3/5] Step [1551/1610]: discriminator_loss = 0.56154, target_loss = 0.37890, acc = 0.50000\n",
            "Epoch [3/5] Step [1552/1610]: discriminator_loss = 0.57109, target_loss = 0.39236, acc = 0.50000\n",
            "Epoch [3/5] Step [1553/1610]: discriminator_loss = 0.62852, target_loss = 0.36737, acc = 0.50000\n",
            "Epoch [3/5] Step [1554/1610]: discriminator_loss = 0.58837, target_loss = 0.37315, acc = 0.50000\n",
            "Epoch [3/5] Step [1555/1610]: discriminator_loss = 0.58699, target_loss = 0.39266, acc = 0.50000\n",
            "Epoch [3/5] Step [1556/1610]: discriminator_loss = 0.59745, target_loss = 0.38505, acc = 0.50000\n",
            "Epoch [3/5] Step [1557/1610]: discriminator_loss = 0.59693, target_loss = 0.38182, acc = 0.50000\n",
            "Epoch [3/5] Step [1558/1610]: discriminator_loss = 0.58393, target_loss = 0.37950, acc = 0.50000\n",
            "Epoch [3/5] Step [1559/1610]: discriminator_loss = 0.57017, target_loss = 0.39870, acc = 0.50000\n",
            "Epoch [3/5] Step [1560/1610]: discriminator_loss = 0.57449, target_loss = 0.38936, acc = 0.50000\n",
            "Epoch [3/5] Step [1561/1610]: discriminator_loss = 0.56980, target_loss = 0.38135, acc = 0.50000\n",
            "Epoch [3/5] Step [1562/1610]: discriminator_loss = 0.58766, target_loss = 0.38211, acc = 0.50000\n",
            "Epoch [3/5] Step [1563/1610]: discriminator_loss = 0.57928, target_loss = 0.40005, acc = 0.50000\n",
            "Epoch [3/5] Step [1564/1610]: discriminator_loss = 0.57494, target_loss = 0.37538, acc = 0.50000\n",
            "Epoch [3/5] Step [1565/1610]: discriminator_loss = 0.58335, target_loss = 0.41434, acc = 0.50000\n",
            "Epoch [3/5] Step [1566/1610]: discriminator_loss = 0.56674, target_loss = 0.38984, acc = 0.50000\n",
            "Epoch [3/5] Step [1567/1610]: discriminator_loss = 0.58416, target_loss = 0.40161, acc = 0.50000\n",
            "Epoch [3/5] Step [1568/1610]: discriminator_loss = 0.57816, target_loss = 0.40678, acc = 0.50000\n",
            "Epoch [3/5] Step [1569/1610]: discriminator_loss = 0.56820, target_loss = 0.40257, acc = 0.50000\n",
            "Epoch [3/5] Step [1570/1610]: discriminator_loss = 0.56434, target_loss = 0.40971, acc = 0.50000\n",
            "Epoch [3/5] Step [1571/1610]: discriminator_loss = 0.55698, target_loss = 0.39670, acc = 0.50000\n",
            "Epoch [3/5] Step [1572/1610]: discriminator_loss = 0.53186, target_loss = 0.41856, acc = 0.50000\n",
            "Epoch [3/5] Step [1573/1610]: discriminator_loss = 0.57895, target_loss = 0.41580, acc = 0.50000\n",
            "Epoch [3/5] Step [1574/1610]: discriminator_loss = 0.53810, target_loss = 0.41414, acc = 0.50000\n",
            "Epoch [3/5] Step [1575/1610]: discriminator_loss = 0.55552, target_loss = 0.41661, acc = 0.50000\n",
            "Epoch [3/5] Step [1576/1610]: discriminator_loss = 0.54449, target_loss = 0.41306, acc = 0.50000\n",
            "Epoch [3/5] Step [1577/1610]: discriminator_loss = 0.55030, target_loss = 0.40251, acc = 0.50000\n",
            "Epoch [3/5] Step [1578/1610]: discriminator_loss = 0.53332, target_loss = 0.42472, acc = 0.50000\n",
            "Epoch [3/5] Step [1579/1610]: discriminator_loss = 0.54682, target_loss = 0.40539, acc = 0.50000\n",
            "Epoch [3/5] Step [1580/1610]: discriminator_loss = 0.54931, target_loss = 0.40237, acc = 0.50000\n",
            "Epoch [3/5] Step [1581/1610]: discriminator_loss = 0.57819, target_loss = 0.39461, acc = 0.50000\n",
            "Epoch [3/5] Step [1582/1610]: discriminator_loss = 0.55297, target_loss = 0.42234, acc = 0.50000\n",
            "Epoch [3/5] Step [1583/1610]: discriminator_loss = 0.55839, target_loss = 0.39780, acc = 0.50000\n",
            "Epoch [3/5] Step [1584/1610]: discriminator_loss = 0.57505, target_loss = 0.39501, acc = 0.50000\n",
            "Epoch [3/5] Step [1585/1610]: discriminator_loss = 0.55644, target_loss = 0.40208, acc = 0.50000\n",
            "Epoch [3/5] Step [1586/1610]: discriminator_loss = 0.58554, target_loss = 0.39746, acc = 0.50000\n",
            "Epoch [3/5] Step [1587/1610]: discriminator_loss = 0.54413, target_loss = 0.40734, acc = 0.50000\n",
            "Epoch [3/5] Step [1588/1610]: discriminator_loss = 0.55343, target_loss = 0.41827, acc = 0.50000\n",
            "Epoch [3/5] Step [1589/1610]: discriminator_loss = 0.55574, target_loss = 0.40436, acc = 0.50000\n",
            "Epoch [3/5] Step [1590/1610]: discriminator_loss = 0.56363, target_loss = 0.38692, acc = 0.50000\n",
            "Epoch [3/5] Step [1591/1610]: discriminator_loss = 0.55071, target_loss = 0.39598, acc = 0.50000\n",
            "Epoch [3/5] Step [1592/1610]: discriminator_loss = 0.55371, target_loss = 0.41575, acc = 0.50000\n",
            "Epoch [3/5] Step [1593/1610]: discriminator_loss = 0.54411, target_loss = 0.40294, acc = 0.50000\n",
            "Epoch [3/5] Step [1594/1610]: discriminator_loss = 0.56051, target_loss = 0.40628, acc = 0.50000\n",
            "Epoch [3/5] Step [1595/1610]: discriminator_loss = 0.57387, target_loss = 0.39959, acc = 0.50000\n",
            "Epoch [3/5] Step [1596/1610]: discriminator_loss = 0.57274, target_loss = 0.39695, acc = 0.50000\n",
            "Epoch [3/5] Step [1597/1610]: discriminator_loss = 0.56287, target_loss = 0.41453, acc = 0.50000\n",
            "Epoch [3/5] Step [1598/1610]: discriminator_loss = 0.58797, target_loss = 0.40255, acc = 0.50000\n",
            "Epoch [3/5] Step [1599/1610]: discriminator_loss = 0.57924, target_loss = 0.40935, acc = 0.50000\n",
            "Epoch [3/5] Step [1600/1610]: discriminator_loss = 0.54477, target_loss = 0.39336, acc = 0.50000\n",
            "Epoch [3/5] Step [1601/1610]: discriminator_loss = 0.56155, target_loss = 0.41266, acc = 0.50000\n",
            "Epoch [3/5] Step [1602/1610]: discriminator_loss = 0.56119, target_loss = 0.42249, acc = 0.50000\n",
            "Epoch [3/5] Step [1603/1610]: discriminator_loss = 0.57297, target_loss = 0.40009, acc = 0.50000\n",
            "Epoch [3/5] Step [1604/1610]: discriminator_loss = 0.57450, target_loss = 0.39100, acc = 0.50000\n",
            "Epoch [3/5] Step [1605/1610]: discriminator_loss = 0.55025, target_loss = 0.42832, acc = 0.50000\n",
            "Epoch [3/5] Step [1606/1610]: discriminator_loss = 0.55621, target_loss = 0.41812, acc = 0.50000\n",
            "Epoch [3/5] Step [1607/1610]: discriminator_loss = 0.52981, target_loss = 0.38652, acc = 0.50000\n",
            "Epoch [3/5] Step [1608/1610]: discriminator_loss = 0.55031, target_loss = 0.41076, acc = 0.50000\n",
            "Epoch [3/5] Step [1609/1610]: discriminator_loss = 0.63318, target_loss = 0.36256, acc = 0.50000\n",
            "Epoch [3/5] Step [1610/1610]: discriminator_loss = 0.55866, target_loss = 0.36620, acc = 0.50000\n",
            "Epoch [4/5] Step [1/1610]: discriminator_loss = 0.67851, target_loss = 0.39186, acc = 0.50000\n",
            "Epoch [4/5] Step [2/1610]: discriminator_loss = 0.53919, target_loss = 0.40802, acc = 0.50000\n",
            "Epoch [4/5] Step [3/1610]: discriminator_loss = 0.57579, target_loss = 0.39151, acc = 0.50000\n",
            "Epoch [4/5] Step [4/1610]: discriminator_loss = 0.54665, target_loss = 0.41667, acc = 0.50000\n",
            "Epoch [4/5] Step [5/1610]: discriminator_loss = 0.57211, target_loss = 0.38252, acc = 0.50000\n",
            "Epoch [4/5] Step [6/1610]: discriminator_loss = 0.56805, target_loss = 0.37448, acc = 0.50000\n",
            "Epoch [4/5] Step [7/1610]: discriminator_loss = 0.58857, target_loss = 0.36762, acc = 0.50000\n",
            "Epoch [4/5] Step [8/1610]: discriminator_loss = 0.55496, target_loss = 0.40299, acc = 0.50000\n",
            "Epoch [4/5] Step [9/1610]: discriminator_loss = 0.61594, target_loss = 0.40856, acc = 0.50000\n",
            "Epoch [4/5] Step [10/1610]: discriminator_loss = 0.53723, target_loss = 0.38104, acc = 0.50000\n",
            "Epoch [4/5] Step [11/1610]: discriminator_loss = 0.53777, target_loss = 0.39977, acc = 0.50000\n",
            "Epoch [4/5] Step [12/1610]: discriminator_loss = 0.57936, target_loss = 0.38735, acc = 0.50000\n",
            "Epoch [4/5] Step [13/1610]: discriminator_loss = 0.58008, target_loss = 0.38088, acc = 0.50000\n",
            "Epoch [4/5] Step [14/1610]: discriminator_loss = 0.61438, target_loss = 0.40523, acc = 0.50000\n",
            "Epoch [4/5] Step [15/1610]: discriminator_loss = 0.56856, target_loss = 0.42177, acc = 0.50000\n",
            "Epoch [4/5] Step [16/1610]: discriminator_loss = 0.57851, target_loss = 0.40171, acc = 0.50000\n",
            "Epoch [4/5] Step [17/1610]: discriminator_loss = 0.55180, target_loss = 0.40213, acc = 0.50000\n",
            "Epoch [4/5] Step [18/1610]: discriminator_loss = 0.56949, target_loss = 0.42932, acc = 0.50000\n",
            "Epoch [4/5] Step [19/1610]: discriminator_loss = 0.53229, target_loss = 0.43618, acc = 0.50000\n",
            "Epoch [4/5] Step [20/1610]: discriminator_loss = 0.55604, target_loss = 0.43663, acc = 0.50000\n",
            "Epoch [4/5] Step [21/1610]: discriminator_loss = 0.51990, target_loss = 0.43233, acc = 0.50000\n",
            "Epoch [4/5] Step [22/1610]: discriminator_loss = 0.57976, target_loss = 0.44249, acc = 0.50000\n",
            "Epoch [4/5] Step [23/1610]: discriminator_loss = 0.53997, target_loss = 0.42473, acc = 0.50000\n",
            "Epoch [4/5] Step [24/1610]: discriminator_loss = 0.57231, target_loss = 0.43482, acc = 0.50000\n",
            "Epoch [4/5] Step [25/1610]: discriminator_loss = 0.54602, target_loss = 0.41996, acc = 0.50000\n",
            "Epoch [4/5] Step [26/1610]: discriminator_loss = 0.53691, target_loss = 0.41353, acc = 0.50000\n",
            "Epoch [4/5] Step [27/1610]: discriminator_loss = 0.58607, target_loss = 0.39869, acc = 0.50000\n",
            "Epoch [4/5] Step [28/1610]: discriminator_loss = 0.54964, target_loss = 0.41363, acc = 0.50000\n",
            "Epoch [4/5] Step [29/1610]: discriminator_loss = 0.54751, target_loss = 0.40110, acc = 0.50000\n",
            "Epoch [4/5] Step [30/1610]: discriminator_loss = 0.54185, target_loss = 0.39579, acc = 0.50000\n",
            "Epoch [4/5] Step [31/1610]: discriminator_loss = 0.59183, target_loss = 0.39832, acc = 0.50000\n",
            "Epoch [4/5] Step [32/1610]: discriminator_loss = 0.54608, target_loss = 0.37580, acc = 0.50000\n",
            "Epoch [4/5] Step [33/1610]: discriminator_loss = 0.55455, target_loss = 0.38989, acc = 0.50000\n",
            "Epoch [4/5] Step [34/1610]: discriminator_loss = 0.56506, target_loss = 0.41144, acc = 0.50000\n",
            "Epoch [4/5] Step [35/1610]: discriminator_loss = 0.58211, target_loss = 0.39902, acc = 0.50000\n",
            "Epoch [4/5] Step [36/1610]: discriminator_loss = 0.55998, target_loss = 0.40232, acc = 0.50000\n",
            "Epoch [4/5] Step [37/1610]: discriminator_loss = 0.57577, target_loss = 0.39626, acc = 0.50000\n",
            "Epoch [4/5] Step [38/1610]: discriminator_loss = 0.57819, target_loss = 0.35186, acc = 0.50000\n",
            "Epoch [4/5] Step [39/1610]: discriminator_loss = 0.57268, target_loss = 0.38013, acc = 0.50000\n",
            "Epoch [4/5] Step [40/1610]: discriminator_loss = 0.56540, target_loss = 0.37884, acc = 0.50000\n",
            "Epoch [4/5] Step [41/1610]: discriminator_loss = 0.58987, target_loss = 0.37167, acc = 0.50000\n",
            "Epoch [4/5] Step [42/1610]: discriminator_loss = 0.57292, target_loss = 0.37072, acc = 0.50000\n",
            "Epoch [4/5] Step [43/1610]: discriminator_loss = 0.56824, target_loss = 0.38902, acc = 0.50000\n",
            "Epoch [4/5] Step [44/1610]: discriminator_loss = 0.57377, target_loss = 0.38886, acc = 0.50000\n",
            "Epoch [4/5] Step [45/1610]: discriminator_loss = 0.59101, target_loss = 0.39121, acc = 0.50000\n",
            "Epoch [4/5] Step [46/1610]: discriminator_loss = 0.56597, target_loss = 0.40177, acc = 0.50000\n",
            "Epoch [4/5] Step [47/1610]: discriminator_loss = 0.57193, target_loss = 0.40989, acc = 0.50000\n",
            "Epoch [4/5] Step [48/1610]: discriminator_loss = 0.54383, target_loss = 0.40738, acc = 0.50000\n",
            "Epoch [4/5] Step [49/1610]: discriminator_loss = 0.58677, target_loss = 0.40397, acc = 0.50000\n",
            "Epoch [4/5] Step [50/1610]: discriminator_loss = 0.58043, target_loss = 0.39659, acc = 0.50000\n",
            "Epoch [4/5] Step [51/1610]: discriminator_loss = 0.54506, target_loss = 0.41580, acc = 0.50000\n",
            "Epoch [4/5] Step [52/1610]: discriminator_loss = 0.54289, target_loss = 0.41810, acc = 0.50000\n",
            "Epoch [4/5] Step [53/1610]: discriminator_loss = 0.55705, target_loss = 0.40986, acc = 0.50000\n",
            "Epoch [4/5] Step [54/1610]: discriminator_loss = 0.54068, target_loss = 0.39090, acc = 0.50000\n",
            "Epoch [4/5] Step [55/1610]: discriminator_loss = 0.53823, target_loss = 0.43113, acc = 0.50000\n",
            "Epoch [4/5] Step [56/1610]: discriminator_loss = 0.54617, target_loss = 0.39481, acc = 0.50000\n",
            "Epoch [4/5] Step [57/1610]: discriminator_loss = 0.54223, target_loss = 0.39845, acc = 0.50000\n",
            "Epoch [4/5] Step [58/1610]: discriminator_loss = 0.66931, target_loss = 0.38746, acc = 0.50000\n",
            "Epoch [4/5] Step [59/1610]: discriminator_loss = 0.54272, target_loss = 0.40760, acc = 0.50000\n",
            "Epoch [4/5] Step [60/1610]: discriminator_loss = 0.60107, target_loss = 0.37946, acc = 0.50000\n",
            "Epoch [4/5] Step [61/1610]: discriminator_loss = 0.56275, target_loss = 0.42633, acc = 0.50000\n",
            "Epoch [4/5] Step [62/1610]: discriminator_loss = 0.54474, target_loss = 0.39799, acc = 0.50000\n",
            "Epoch [4/5] Step [63/1610]: discriminator_loss = 0.54510, target_loss = 0.40415, acc = 0.50000\n",
            "Epoch [4/5] Step [64/1610]: discriminator_loss = 0.55666, target_loss = 0.41682, acc = 0.50000\n",
            "Epoch [4/5] Step [65/1610]: discriminator_loss = 0.51591, target_loss = 0.43898, acc = 0.50000\n",
            "Epoch [4/5] Step [66/1610]: discriminator_loss = 0.54697, target_loss = 0.39994, acc = 0.50000\n",
            "Epoch [4/5] Step [67/1610]: discriminator_loss = 0.55519, target_loss = 0.42356, acc = 0.50000\n",
            "Epoch [4/5] Step [68/1610]: discriminator_loss = 0.54145, target_loss = 0.42990, acc = 0.50000\n",
            "Epoch [4/5] Step [69/1610]: discriminator_loss = 0.55064, target_loss = 0.40750, acc = 0.50000\n",
            "Epoch [4/5] Step [70/1610]: discriminator_loss = 0.53941, target_loss = 0.41495, acc = 0.50000\n",
            "Epoch [4/5] Step [71/1610]: discriminator_loss = 0.56095, target_loss = 0.41044, acc = 0.50000\n",
            "Epoch [4/5] Step [72/1610]: discriminator_loss = 0.56405, target_loss = 0.40510, acc = 0.50000\n",
            "Epoch [4/5] Step [73/1610]: discriminator_loss = 0.60920, target_loss = 0.38048, acc = 0.50000\n",
            "Epoch [4/5] Step [74/1610]: discriminator_loss = 0.55709, target_loss = 0.41500, acc = 0.50000\n",
            "Epoch [4/5] Step [75/1610]: discriminator_loss = 0.54839, target_loss = 0.41021, acc = 0.50000\n",
            "Epoch [4/5] Step [76/1610]: discriminator_loss = 0.59118, target_loss = 0.38737, acc = 0.50000\n",
            "Epoch [4/5] Step [77/1610]: discriminator_loss = 0.55101, target_loss = 0.39509, acc = 0.50000\n",
            "Epoch [4/5] Step [78/1610]: discriminator_loss = 0.56979, target_loss = 0.39050, acc = 0.50000\n",
            "Epoch [4/5] Step [79/1610]: discriminator_loss = 0.55985, target_loss = 0.39681, acc = 0.50000\n",
            "Epoch [4/5] Step [80/1610]: discriminator_loss = 0.59796, target_loss = 0.38425, acc = 0.50000\n",
            "Epoch [4/5] Step [81/1610]: discriminator_loss = 0.54964, target_loss = 0.38840, acc = 0.50000\n",
            "Epoch [4/5] Step [82/1610]: discriminator_loss = 0.55763, target_loss = 0.39845, acc = 0.50000\n",
            "Epoch [4/5] Step [83/1610]: discriminator_loss = 0.56371, target_loss = 0.37586, acc = 0.50000\n",
            "Epoch [4/5] Step [84/1610]: discriminator_loss = 0.56352, target_loss = 0.39948, acc = 0.50000\n",
            "Epoch [4/5] Step [85/1610]: discriminator_loss = 0.57430, target_loss = 0.41879, acc = 0.50000\n",
            "Epoch [4/5] Step [86/1610]: discriminator_loss = 0.59996, target_loss = 0.39223, acc = 0.50000\n",
            "Epoch [4/5] Step [87/1610]: discriminator_loss = 0.54196, target_loss = 0.41762, acc = 0.50000\n",
            "Epoch [4/5] Step [88/1610]: discriminator_loss = 0.54103, target_loss = 0.39390, acc = 0.50000\n",
            "Epoch [4/5] Step [89/1610]: discriminator_loss = 0.56579, target_loss = 0.40535, acc = 0.50000\n",
            "Epoch [4/5] Step [90/1610]: discriminator_loss = 0.56611, target_loss = 0.40596, acc = 0.50000\n",
            "Epoch [4/5] Step [91/1610]: discriminator_loss = 0.56489, target_loss = 0.41068, acc = 0.50000\n",
            "Epoch [4/5] Step [92/1610]: discriminator_loss = 0.55652, target_loss = 0.40376, acc = 0.50000\n",
            "Epoch [4/5] Step [93/1610]: discriminator_loss = 0.55756, target_loss = 0.41836, acc = 0.50000\n",
            "Epoch [4/5] Step [94/1610]: discriminator_loss = 0.55347, target_loss = 0.39577, acc = 0.50000\n",
            "Epoch [4/5] Step [95/1610]: discriminator_loss = 0.58291, target_loss = 0.41679, acc = 0.50000\n",
            "Epoch [4/5] Step [96/1610]: discriminator_loss = 0.53093, target_loss = 0.43250, acc = 0.50000\n",
            "Epoch [4/5] Step [97/1610]: discriminator_loss = 0.52255, target_loss = 0.41258, acc = 0.50000\n",
            "Epoch [4/5] Step [98/1610]: discriminator_loss = 0.56770, target_loss = 0.40395, acc = 0.50000\n",
            "Epoch [4/5] Step [99/1610]: discriminator_loss = 0.56993, target_loss = 0.38941, acc = 0.50000\n",
            "Epoch [4/5] Step [100/1610]: discriminator_loss = 0.56080, target_loss = 0.39163, acc = 0.50000\n",
            "Epoch [4/5] Step [101/1610]: discriminator_loss = 0.56141, target_loss = 0.40158, acc = 0.50000\n",
            "Epoch [4/5] Step [102/1610]: discriminator_loss = 0.57088, target_loss = 0.37714, acc = 0.50000\n",
            "Epoch [4/5] Step [103/1610]: discriminator_loss = 0.60442, target_loss = 0.39536, acc = 0.50000\n",
            "Epoch [4/5] Step [104/1610]: discriminator_loss = 0.58142, target_loss = 0.39257, acc = 0.50000\n",
            "Epoch [4/5] Step [105/1610]: discriminator_loss = 0.56501, target_loss = 0.36372, acc = 0.50000\n",
            "Epoch [4/5] Step [106/1610]: discriminator_loss = 0.56962, target_loss = 0.37152, acc = 0.50000\n",
            "Epoch [4/5] Step [107/1610]: discriminator_loss = 0.58217, target_loss = 0.35066, acc = 0.50000\n",
            "Epoch [4/5] Step [108/1610]: discriminator_loss = 0.58942, target_loss = 0.37947, acc = 0.50000\n",
            "Epoch [4/5] Step [109/1610]: discriminator_loss = 0.59386, target_loss = 0.37481, acc = 0.50000\n",
            "Epoch [4/5] Step [110/1610]: discriminator_loss = 0.59179, target_loss = 0.39155, acc = 0.50000\n",
            "Epoch [4/5] Step [111/1610]: discriminator_loss = 0.60965, target_loss = 0.38635, acc = 0.50000\n",
            "Epoch [4/5] Step [112/1610]: discriminator_loss = 0.55555, target_loss = 0.40075, acc = 0.50000\n",
            "Epoch [4/5] Step [113/1610]: discriminator_loss = 0.57318, target_loss = 0.39698, acc = 0.50000\n",
            "Epoch [4/5] Step [114/1610]: discriminator_loss = 0.53369, target_loss = 0.42333, acc = 0.50000\n",
            "Epoch [4/5] Step [115/1610]: discriminator_loss = 0.55849, target_loss = 0.39142, acc = 0.50000\n",
            "Epoch [4/5] Step [116/1610]: discriminator_loss = 0.53930, target_loss = 0.41517, acc = 0.50000\n",
            "Epoch [4/5] Step [117/1610]: discriminator_loss = 0.53714, target_loss = 0.40427, acc = 0.50000\n",
            "Epoch [4/5] Step [118/1610]: discriminator_loss = 0.54634, target_loss = 0.41747, acc = 0.50000\n",
            "Epoch [4/5] Step [119/1610]: discriminator_loss = 0.60243, target_loss = 0.39785, acc = 0.50000\n",
            "Epoch [4/5] Step [120/1610]: discriminator_loss = 0.49907, target_loss = 0.39960, acc = 0.50000\n",
            "Epoch [4/5] Step [121/1610]: discriminator_loss = 0.58217, target_loss = 0.44304, acc = 0.50000\n",
            "Epoch [4/5] Step [122/1610]: discriminator_loss = 0.52123, target_loss = 0.42900, acc = 0.50000\n",
            "Epoch [4/5] Step [123/1610]: discriminator_loss = 0.54427, target_loss = 0.41932, acc = 0.50000\n",
            "Epoch [4/5] Step [124/1610]: discriminator_loss = 0.60529, target_loss = 0.41903, acc = 0.50000\n",
            "Epoch [4/5] Step [125/1610]: discriminator_loss = 0.52324, target_loss = 0.41944, acc = 0.50000\n",
            "Epoch [4/5] Step [126/1610]: discriminator_loss = 0.52439, target_loss = 0.41851, acc = 0.50000\n",
            "Epoch [4/5] Step [127/1610]: discriminator_loss = 0.53720, target_loss = 0.38100, acc = 0.50000\n",
            "Epoch [4/5] Step [128/1610]: discriminator_loss = 0.56689, target_loss = 0.39773, acc = 0.50000\n",
            "Epoch [4/5] Step [129/1610]: discriminator_loss = 0.54234, target_loss = 0.40065, acc = 0.50000\n",
            "Epoch [4/5] Step [130/1610]: discriminator_loss = 0.67878, target_loss = 0.39883, acc = 0.50000\n",
            "Epoch [4/5] Step [131/1610]: discriminator_loss = 0.54469, target_loss = 0.43725, acc = 0.50000\n",
            "Epoch [4/5] Step [132/1610]: discriminator_loss = 0.52504, target_loss = 0.43615, acc = 0.50000\n",
            "Epoch [4/5] Step [133/1610]: discriminator_loss = 0.57128, target_loss = 0.42651, acc = 0.50000\n",
            "Epoch [4/5] Step [134/1610]: discriminator_loss = 0.50800, target_loss = 0.43525, acc = 0.53125\n",
            "Epoch [4/5] Step [135/1610]: discriminator_loss = 0.55259, target_loss = 0.42143, acc = 0.50000\n",
            "Epoch [4/5] Step [136/1610]: discriminator_loss = 0.54887, target_loss = 0.40432, acc = 0.53125\n",
            "Epoch [4/5] Step [137/1610]: discriminator_loss = 0.54123, target_loss = 0.41945, acc = 0.50000\n",
            "Epoch [4/5] Step [138/1610]: discriminator_loss = 0.56617, target_loss = 0.39337, acc = 0.50000\n",
            "Epoch [4/5] Step [139/1610]: discriminator_loss = 0.55009, target_loss = 0.40448, acc = 0.50000\n",
            "Epoch [4/5] Step [140/1610]: discriminator_loss = 0.56099, target_loss = 0.38040, acc = 0.50000\n",
            "Epoch [4/5] Step [141/1610]: discriminator_loss = 0.65333, target_loss = 0.40495, acc = 0.50000\n",
            "Epoch [4/5] Step [142/1610]: discriminator_loss = 0.61168, target_loss = 0.39243, acc = 0.50000\n",
            "Epoch [4/5] Step [143/1610]: discriminator_loss = 0.58515, target_loss = 0.41239, acc = 0.50000\n",
            "Epoch [4/5] Step [144/1610]: discriminator_loss = 0.58098, target_loss = 0.40055, acc = 0.50000\n",
            "Epoch [4/5] Step [145/1610]: discriminator_loss = 0.61365, target_loss = 0.41710, acc = 0.50000\n",
            "Epoch [4/5] Step [146/1610]: discriminator_loss = 0.57579, target_loss = 0.40626, acc = 0.50000\n",
            "Epoch [4/5] Step [147/1610]: discriminator_loss = 0.58031, target_loss = 0.38728, acc = 0.50000\n",
            "Epoch [4/5] Step [148/1610]: discriminator_loss = 0.60163, target_loss = 0.38113, acc = 0.50000\n",
            "Epoch [4/5] Step [149/1610]: discriminator_loss = 0.60349, target_loss = 0.38501, acc = 0.50000\n",
            "Epoch [4/5] Step [150/1610]: discriminator_loss = 0.58594, target_loss = 0.37843, acc = 0.50000\n",
            "Epoch [4/5] Step [151/1610]: discriminator_loss = 0.60196, target_loss = 0.37208, acc = 0.50000\n",
            "Epoch [4/5] Step [152/1610]: discriminator_loss = 0.58111, target_loss = 0.38552, acc = 0.50000\n",
            "Epoch [4/5] Step [153/1610]: discriminator_loss = 0.59692, target_loss = 0.34055, acc = 0.50000\n",
            "Epoch [4/5] Step [154/1610]: discriminator_loss = 0.58538, target_loss = 0.38604, acc = 0.50000\n",
            "Epoch [4/5] Step [155/1610]: discriminator_loss = 0.59082, target_loss = 0.33440, acc = 0.50000\n",
            "Epoch [4/5] Step [156/1610]: discriminator_loss = 0.58526, target_loss = 0.36861, acc = 0.50000\n",
            "Epoch [4/5] Step [157/1610]: discriminator_loss = 0.66428, target_loss = 0.38709, acc = 0.50000\n",
            "Epoch [4/5] Step [158/1610]: discriminator_loss = 0.61453, target_loss = 0.35159, acc = 0.50000\n",
            "Epoch [4/5] Step [159/1610]: discriminator_loss = 0.59861, target_loss = 0.37411, acc = 0.50000\n",
            "Epoch [4/5] Step [160/1610]: discriminator_loss = 0.58444, target_loss = 0.39877, acc = 0.50000\n",
            "Epoch [4/5] Step [161/1610]: discriminator_loss = 0.56810, target_loss = 0.40097, acc = 0.50000\n",
            "Epoch [4/5] Step [162/1610]: discriminator_loss = 0.59172, target_loss = 0.41971, acc = 0.50000\n",
            "Epoch [4/5] Step [163/1610]: discriminator_loss = 0.56093, target_loss = 0.40155, acc = 0.50000\n",
            "Epoch [4/5] Step [164/1610]: discriminator_loss = 0.55077, target_loss = 0.40864, acc = 0.50000\n",
            "Epoch [4/5] Step [165/1610]: discriminator_loss = 0.56058, target_loss = 0.40003, acc = 0.50000\n",
            "Epoch [4/5] Step [166/1610]: discriminator_loss = 0.57694, target_loss = 0.40465, acc = 0.50000\n",
            "Epoch [4/5] Step [167/1610]: discriminator_loss = 0.54913, target_loss = 0.43691, acc = 0.50000\n",
            "Epoch [4/5] Step [168/1610]: discriminator_loss = 0.55856, target_loss = 0.41280, acc = 0.50000\n",
            "Epoch [4/5] Step [169/1610]: discriminator_loss = 0.55416, target_loss = 0.42041, acc = 0.50000\n",
            "Epoch [4/5] Step [170/1610]: discriminator_loss = 0.54652, target_loss = 0.41114, acc = 0.50000\n",
            "Epoch [4/5] Step [171/1610]: discriminator_loss = 0.54451, target_loss = 0.41922, acc = 0.50000\n",
            "Epoch [4/5] Step [172/1610]: discriminator_loss = 0.54584, target_loss = 0.40902, acc = 0.50000\n",
            "Epoch [4/5] Step [173/1610]: discriminator_loss = 0.55423, target_loss = 0.40702, acc = 0.50000\n",
            "Epoch [4/5] Step [174/1610]: discriminator_loss = 0.53765, target_loss = 0.41643, acc = 0.50000\n",
            "Epoch [4/5] Step [175/1610]: discriminator_loss = 0.55605, target_loss = 0.41370, acc = 0.50000\n",
            "Epoch [4/5] Step [176/1610]: discriminator_loss = 0.56525, target_loss = 0.40395, acc = 0.50000\n",
            "Epoch [4/5] Step [177/1610]: discriminator_loss = 0.55967, target_loss = 0.39639, acc = 0.50000\n",
            "Epoch [4/5] Step [178/1610]: discriminator_loss = 0.54978, target_loss = 0.41070, acc = 0.50000\n",
            "Epoch [4/5] Step [179/1610]: discriminator_loss = 0.56292, target_loss = 0.41197, acc = 0.50000\n",
            "Epoch [4/5] Step [180/1610]: discriminator_loss = 0.56610, target_loss = 0.39473, acc = 0.50000\n",
            "Epoch [4/5] Step [181/1610]: discriminator_loss = 0.60284, target_loss = 0.38700, acc = 0.50000\n",
            "Epoch [4/5] Step [182/1610]: discriminator_loss = 0.58074, target_loss = 0.36515, acc = 0.50000\n",
            "Epoch [4/5] Step [183/1610]: discriminator_loss = 0.57016, target_loss = 0.35930, acc = 0.50000\n",
            "Epoch [4/5] Step [184/1610]: discriminator_loss = 0.56315, target_loss = 0.41224, acc = 0.50000\n",
            "Epoch [4/5] Step [185/1610]: discriminator_loss = 0.58489, target_loss = 0.37997, acc = 0.50000\n",
            "Epoch [4/5] Step [186/1610]: discriminator_loss = 0.62383, target_loss = 0.37834, acc = 0.50000\n",
            "Epoch [4/5] Step [187/1610]: discriminator_loss = 0.55872, target_loss = 0.38676, acc = 0.50000\n",
            "Epoch [4/5] Step [188/1610]: discriminator_loss = 0.57177, target_loss = 0.40179, acc = 0.50000\n",
            "Epoch [4/5] Step [189/1610]: discriminator_loss = 0.56747, target_loss = 0.39695, acc = 0.50000\n",
            "Epoch [4/5] Step [190/1610]: discriminator_loss = 0.55840, target_loss = 0.41122, acc = 0.50000\n",
            "Epoch [4/5] Step [191/1610]: discriminator_loss = 0.55651, target_loss = 0.41045, acc = 0.50000\n",
            "Epoch [4/5] Step [192/1610]: discriminator_loss = 0.57984, target_loss = 0.39639, acc = 0.50000\n",
            "Epoch [4/5] Step [193/1610]: discriminator_loss = 0.59075, target_loss = 0.42285, acc = 0.50000\n",
            "Epoch [4/5] Step [194/1610]: discriminator_loss = 0.54762, target_loss = 0.43905, acc = 0.50000\n",
            "Epoch [4/5] Step [195/1610]: discriminator_loss = 0.55810, target_loss = 0.40294, acc = 0.50000\n",
            "Epoch [4/5] Step [196/1610]: discriminator_loss = 0.55740, target_loss = 0.42138, acc = 0.50000\n",
            "Epoch [4/5] Step [197/1610]: discriminator_loss = 0.58007, target_loss = 0.40769, acc = 0.50000\n",
            "Epoch [4/5] Step [198/1610]: discriminator_loss = 0.56797, target_loss = 0.40950, acc = 0.50000\n",
            "Epoch [4/5] Step [199/1610]: discriminator_loss = 0.54463, target_loss = 0.40359, acc = 0.50000\n",
            "Epoch [4/5] Step [200/1610]: discriminator_loss = 0.55366, target_loss = 0.41179, acc = 0.50000\n",
            "Epoch [4/5] Step [201/1610]: discriminator_loss = 0.57503, target_loss = 0.41205, acc = 0.50000\n",
            "Epoch [4/5] Step [202/1610]: discriminator_loss = 0.56358, target_loss = 0.41922, acc = 0.50000\n",
            "Epoch [4/5] Step [203/1610]: discriminator_loss = 0.55190, target_loss = 0.38076, acc = 0.50000\n",
            "Epoch [4/5] Step [204/1610]: discriminator_loss = 0.58317, target_loss = 0.41977, acc = 0.50000\n",
            "Epoch [4/5] Step [205/1610]: discriminator_loss = 0.59467, target_loss = 0.39476, acc = 0.50000\n",
            "Epoch [4/5] Step [206/1610]: discriminator_loss = 0.55924, target_loss = 0.40361, acc = 0.50000\n",
            "Epoch [4/5] Step [207/1610]: discriminator_loss = 0.56514, target_loss = 0.40465, acc = 0.50000\n",
            "Epoch [4/5] Step [208/1610]: discriminator_loss = 0.56541, target_loss = 0.39262, acc = 0.50000\n",
            "Epoch [4/5] Step [209/1610]: discriminator_loss = 0.58683, target_loss = 0.38009, acc = 0.50000\n",
            "Epoch [4/5] Step [210/1610]: discriminator_loss = 0.58036, target_loss = 0.37635, acc = 0.50000\n",
            "Epoch [4/5] Step [211/1610]: discriminator_loss = 0.58148, target_loss = 0.41170, acc = 0.50000\n",
            "Epoch [4/5] Step [212/1610]: discriminator_loss = 0.55819, target_loss = 0.40038, acc = 0.50000\n",
            "Epoch [4/5] Step [213/1610]: discriminator_loss = 0.59174, target_loss = 0.37784, acc = 0.50000\n",
            "Epoch [4/5] Step [214/1610]: discriminator_loss = 0.57835, target_loss = 0.37881, acc = 0.50000\n",
            "Epoch [4/5] Step [215/1610]: discriminator_loss = 0.57414, target_loss = 0.38772, acc = 0.50000\n",
            "Epoch [4/5] Step [216/1610]: discriminator_loss = 0.57639, target_loss = 0.37417, acc = 0.50000\n",
            "Epoch [4/5] Step [217/1610]: discriminator_loss = 0.57567, target_loss = 0.40618, acc = 0.50000\n",
            "Epoch [4/5] Step [218/1610]: discriminator_loss = 0.60106, target_loss = 0.38818, acc = 0.50000\n",
            "Epoch [4/5] Step [219/1610]: discriminator_loss = 0.58774, target_loss = 0.39134, acc = 0.50000\n",
            "Epoch [4/5] Step [220/1610]: discriminator_loss = 0.55986, target_loss = 0.38911, acc = 0.50000\n",
            "Epoch [4/5] Step [221/1610]: discriminator_loss = 0.56234, target_loss = 0.39923, acc = 0.50000\n",
            "Epoch [4/5] Step [222/1610]: discriminator_loss = 0.58570, target_loss = 0.40301, acc = 0.50000\n",
            "Epoch [4/5] Step [223/1610]: discriminator_loss = 0.54668, target_loss = 0.41073, acc = 0.50000\n",
            "Epoch [4/5] Step [224/1610]: discriminator_loss = 0.55617, target_loss = 0.39859, acc = 0.50000\n",
            "Epoch [4/5] Step [225/1610]: discriminator_loss = 0.56362, target_loss = 0.40387, acc = 0.50000\n",
            "Epoch [4/5] Step [226/1610]: discriminator_loss = 0.56207, target_loss = 0.41378, acc = 0.50000\n",
            "Epoch [4/5] Step [227/1610]: discriminator_loss = 0.56061, target_loss = 0.34814, acc = 0.50000\n",
            "Epoch [4/5] Step [228/1610]: discriminator_loss = 0.63827, target_loss = 0.41079, acc = 0.50000\n",
            "Epoch [4/5] Step [229/1610]: discriminator_loss = 0.57766, target_loss = 0.40496, acc = 0.50000\n",
            "Epoch [4/5] Step [230/1610]: discriminator_loss = 0.55365, target_loss = 0.39750, acc = 0.50000\n",
            "Epoch [4/5] Step [231/1610]: discriminator_loss = 0.55465, target_loss = 0.41316, acc = 0.50000\n",
            "Epoch [4/5] Step [232/1610]: discriminator_loss = 0.54847, target_loss = 0.42697, acc = 0.50000\n",
            "Epoch [4/5] Step [233/1610]: discriminator_loss = 0.58053, target_loss = 0.40469, acc = 0.50000\n",
            "Epoch [4/5] Step [234/1610]: discriminator_loss = 0.55292, target_loss = 0.40999, acc = 0.50000\n",
            "Epoch [4/5] Step [235/1610]: discriminator_loss = 0.58326, target_loss = 0.41150, acc = 0.50000\n",
            "Epoch [4/5] Step [236/1610]: discriminator_loss = 0.55249, target_loss = 0.41089, acc = 0.50000\n",
            "Epoch [4/5] Step [237/1610]: discriminator_loss = 0.56329, target_loss = 0.42203, acc = 0.50000\n",
            "Epoch [4/5] Step [238/1610]: discriminator_loss = 0.55801, target_loss = 0.40189, acc = 0.50000\n",
            "Epoch [4/5] Step [239/1610]: discriminator_loss = 0.56561, target_loss = 0.41637, acc = 0.50000\n",
            "Epoch [4/5] Step [240/1610]: discriminator_loss = 0.56775, target_loss = 0.41853, acc = 0.50000\n",
            "Epoch [4/5] Step [241/1610]: discriminator_loss = 0.57198, target_loss = 0.41072, acc = 0.50000\n",
            "Epoch [4/5] Step [242/1610]: discriminator_loss = 0.55140, target_loss = 0.41165, acc = 0.50000\n",
            "Epoch [4/5] Step [243/1610]: discriminator_loss = 0.54318, target_loss = 0.41510, acc = 0.50000\n",
            "Epoch [4/5] Step [244/1610]: discriminator_loss = 0.55541, target_loss = 0.40664, acc = 0.50000\n",
            "Epoch [4/5] Step [245/1610]: discriminator_loss = 0.53712, target_loss = 0.40295, acc = 0.50000\n",
            "Epoch [4/5] Step [246/1610]: discriminator_loss = 0.55166, target_loss = 0.39938, acc = 0.50000\n",
            "Epoch [4/5] Step [247/1610]: discriminator_loss = 0.56102, target_loss = 0.42229, acc = 0.50000\n",
            "Epoch [4/5] Step [248/1610]: discriminator_loss = 0.56969, target_loss = 0.41129, acc = 0.50000\n",
            "Epoch [4/5] Step [249/1610]: discriminator_loss = 0.56527, target_loss = 0.37354, acc = 0.50000\n",
            "Epoch [4/5] Step [250/1610]: discriminator_loss = 0.56125, target_loss = 0.40576, acc = 0.50000\n",
            "Epoch [4/5] Step [251/1610]: discriminator_loss = 0.58963, target_loss = 0.39220, acc = 0.50000\n",
            "Epoch [4/5] Step [252/1610]: discriminator_loss = 0.58100, target_loss = 0.40600, acc = 0.50000\n",
            "Epoch [4/5] Step [253/1610]: discriminator_loss = 0.55119, target_loss = 0.41570, acc = 0.50000\n",
            "Epoch [4/5] Step [254/1610]: discriminator_loss = 0.55574, target_loss = 0.41830, acc = 0.50000\n",
            "Epoch [4/5] Step [255/1610]: discriminator_loss = 0.56759, target_loss = 0.38438, acc = 0.50000\n",
            "Epoch [4/5] Step [256/1610]: discriminator_loss = 0.57555, target_loss = 0.36707, acc = 0.50000\n",
            "Epoch [4/5] Step [257/1610]: discriminator_loss = 0.54748, target_loss = 0.40343, acc = 0.50000\n",
            "Epoch [4/5] Step [258/1610]: discriminator_loss = 0.56825, target_loss = 0.42428, acc = 0.50000\n",
            "Epoch [4/5] Step [259/1610]: discriminator_loss = 0.58740, target_loss = 0.38443, acc = 0.50000\n",
            "Epoch [4/5] Step [260/1610]: discriminator_loss = 0.57524, target_loss = 0.37901, acc = 0.50000\n",
            "Epoch [4/5] Step [261/1610]: discriminator_loss = 0.55629, target_loss = 0.38590, acc = 0.50000\n",
            "Epoch [4/5] Step [262/1610]: discriminator_loss = 0.58099, target_loss = 0.37013, acc = 0.50000\n",
            "Epoch [4/5] Step [263/1610]: discriminator_loss = 0.53864, target_loss = 0.39570, acc = 0.50000\n",
            "Epoch [4/5] Step [264/1610]: discriminator_loss = 0.57803, target_loss = 0.37040, acc = 0.50000\n",
            "Epoch [4/5] Step [265/1610]: discriminator_loss = 0.57138, target_loss = 0.40372, acc = 0.50000\n",
            "Epoch [4/5] Step [266/1610]: discriminator_loss = 0.55234, target_loss = 0.39727, acc = 0.50000\n",
            "Epoch [4/5] Step [267/1610]: discriminator_loss = 0.55988, target_loss = 0.40357, acc = 0.50000\n",
            "Epoch [4/5] Step [268/1610]: discriminator_loss = 0.57940, target_loss = 0.37773, acc = 0.50000\n",
            "Epoch [4/5] Step [269/1610]: discriminator_loss = 0.58657, target_loss = 0.41064, acc = 0.50000\n",
            "Epoch [4/5] Step [270/1610]: discriminator_loss = 0.53941, target_loss = 0.42077, acc = 0.50000\n",
            "Epoch [4/5] Step [271/1610]: discriminator_loss = 0.56081, target_loss = 0.40650, acc = 0.50000\n",
            "Epoch [4/5] Step [272/1610]: discriminator_loss = 0.55270, target_loss = 0.40361, acc = 0.50000\n",
            "Epoch [4/5] Step [273/1610]: discriminator_loss = 0.56342, target_loss = 0.42071, acc = 0.50000\n",
            "Epoch [4/5] Step [274/1610]: discriminator_loss = 0.53674, target_loss = 0.42417, acc = 0.50000\n",
            "Epoch [4/5] Step [275/1610]: discriminator_loss = 0.55571, target_loss = 0.40626, acc = 0.50000\n",
            "Epoch [4/5] Step [276/1610]: discriminator_loss = 0.53822, target_loss = 0.41647, acc = 0.50000\n",
            "Epoch [4/5] Step [277/1610]: discriminator_loss = 0.56651, target_loss = 0.40120, acc = 0.50000\n",
            "Epoch [4/5] Step [278/1610]: discriminator_loss = 0.54444, target_loss = 0.42468, acc = 0.50000\n",
            "Epoch [4/5] Step [279/1610]: discriminator_loss = 0.55557, target_loss = 0.39069, acc = 0.50000\n",
            "Epoch [4/5] Step [280/1610]: discriminator_loss = 0.57546, target_loss = 0.40811, acc = 0.50000\n",
            "Epoch [4/5] Step [281/1610]: discriminator_loss = 0.55218, target_loss = 0.40413, acc = 0.50000\n",
            "Epoch [4/5] Step [282/1610]: discriminator_loss = 0.55982, target_loss = 0.40804, acc = 0.50000\n",
            "Epoch [4/5] Step [283/1610]: discriminator_loss = 0.60046, target_loss = 0.39784, acc = 0.50000\n",
            "Epoch [4/5] Step [284/1610]: discriminator_loss = 0.54790, target_loss = 0.41460, acc = 0.50000\n",
            "Epoch [4/5] Step [285/1610]: discriminator_loss = 0.54246, target_loss = 0.40631, acc = 0.50000\n",
            "Epoch [4/5] Step [286/1610]: discriminator_loss = 0.55209, target_loss = 0.42226, acc = 0.50000\n",
            "Epoch [4/5] Step [287/1610]: discriminator_loss = 0.55628, target_loss = 0.39939, acc = 0.50000\n",
            "Epoch [4/5] Step [288/1610]: discriminator_loss = 0.56259, target_loss = 0.39671, acc = 0.50000\n",
            "Epoch [4/5] Step [289/1610]: discriminator_loss = 0.58105, target_loss = 0.41125, acc = 0.50000\n",
            "Epoch [4/5] Step [290/1610]: discriminator_loss = 0.58324, target_loss = 0.39606, acc = 0.50000\n",
            "Epoch [4/5] Step [291/1610]: discriminator_loss = 0.58149, target_loss = 0.38301, acc = 0.50000\n",
            "Epoch [4/5] Step [292/1610]: discriminator_loss = 0.56063, target_loss = 0.40781, acc = 0.50000\n",
            "Epoch [4/5] Step [293/1610]: discriminator_loss = 0.58478, target_loss = 0.40453, acc = 0.50000\n",
            "Epoch [4/5] Step [294/1610]: discriminator_loss = 0.56598, target_loss = 0.40031, acc = 0.50000\n",
            "Epoch [4/5] Step [295/1610]: discriminator_loss = 0.62122, target_loss = 0.41268, acc = 0.50000\n",
            "Epoch [4/5] Step [296/1610]: discriminator_loss = 0.59327, target_loss = 0.40011, acc = 0.50000\n",
            "Epoch [4/5] Step [297/1610]: discriminator_loss = 0.57576, target_loss = 0.39360, acc = 0.50000\n",
            "Epoch [4/5] Step [298/1610]: discriminator_loss = 0.60034, target_loss = 0.38026, acc = 0.50000\n",
            "Epoch [4/5] Step [299/1610]: discriminator_loss = 0.57026, target_loss = 0.39418, acc = 0.50000\n",
            "Epoch [4/5] Step [300/1610]: discriminator_loss = 0.60271, target_loss = 0.41327, acc = 0.50000\n",
            "Epoch [4/5] Step [301/1610]: discriminator_loss = 0.57756, target_loss = 0.38512, acc = 0.50000\n",
            "Epoch [4/5] Step [302/1610]: discriminator_loss = 0.57770, target_loss = 0.38723, acc = 0.50000\n",
            "Epoch [4/5] Step [303/1610]: discriminator_loss = 0.57856, target_loss = 0.39167, acc = 0.50000\n",
            "Epoch [4/5] Step [304/1610]: discriminator_loss = 0.58934, target_loss = 0.38300, acc = 0.50000\n",
            "Epoch [4/5] Step [305/1610]: discriminator_loss = 0.57269, target_loss = 0.38671, acc = 0.50000\n",
            "Epoch [4/5] Step [306/1610]: discriminator_loss = 0.60036, target_loss = 0.38416, acc = 0.50000\n",
            "Epoch [4/5] Step [307/1610]: discriminator_loss = 0.59864, target_loss = 0.38265, acc = 0.50000\n",
            "Epoch [4/5] Step [308/1610]: discriminator_loss = 0.56511, target_loss = 0.38145, acc = 0.50000\n",
            "Epoch [4/5] Step [309/1610]: discriminator_loss = 0.60877, target_loss = 0.38489, acc = 0.50000\n",
            "Epoch [4/5] Step [310/1610]: discriminator_loss = 0.56815, target_loss = 0.39619, acc = 0.50000\n",
            "Epoch [4/5] Step [311/1610]: discriminator_loss = 0.56979, target_loss = 0.39866, acc = 0.50000\n",
            "Epoch [4/5] Step [312/1610]: discriminator_loss = 0.57835, target_loss = 0.38720, acc = 0.50000\n",
            "Epoch [4/5] Step [313/1610]: discriminator_loss = 0.56605, target_loss = 0.37637, acc = 0.50000\n",
            "Epoch [4/5] Step [314/1610]: discriminator_loss = 0.56374, target_loss = 0.39870, acc = 0.50000\n",
            "Epoch [4/5] Step [315/1610]: discriminator_loss = 0.59958, target_loss = 0.38244, acc = 0.50000\n",
            "Epoch [4/5] Step [316/1610]: discriminator_loss = 0.57060, target_loss = 0.39883, acc = 0.50000\n",
            "Epoch [4/5] Step [317/1610]: discriminator_loss = 0.57300, target_loss = 0.39630, acc = 0.50000\n",
            "Epoch [4/5] Step [318/1610]: discriminator_loss = 0.58673, target_loss = 0.37871, acc = 0.50000\n",
            "Epoch [4/5] Step [319/1610]: discriminator_loss = 0.59670, target_loss = 0.41747, acc = 0.50000\n",
            "Epoch [4/5] Step [320/1610]: discriminator_loss = 0.55516, target_loss = 0.38613, acc = 0.50000\n",
            "Epoch [4/5] Step [321/1610]: discriminator_loss = 0.56281, target_loss = 0.41717, acc = 0.50000\n",
            "Epoch [4/5] Step [322/1610]: discriminator_loss = 0.54907, target_loss = 0.41081, acc = 0.50000\n",
            "Epoch [4/5] Step [323/1610]: discriminator_loss = 0.54614, target_loss = 0.40599, acc = 0.50000\n",
            "Epoch [4/5] Step [324/1610]: discriminator_loss = 0.56172, target_loss = 0.42166, acc = 0.50000\n",
            "Epoch [4/5] Step [325/1610]: discriminator_loss = 0.52024, target_loss = 0.40672, acc = 0.50000\n",
            "Epoch [4/5] Step [326/1610]: discriminator_loss = 0.55259, target_loss = 0.41850, acc = 0.50000\n",
            "Epoch [4/5] Step [327/1610]: discriminator_loss = 0.56985, target_loss = 0.42251, acc = 0.50000\n",
            "Epoch [4/5] Step [328/1610]: discriminator_loss = 0.53634, target_loss = 0.42420, acc = 0.50000\n",
            "Epoch [4/5] Step [329/1610]: discriminator_loss = 0.55692, target_loss = 0.42364, acc = 0.50000\n",
            "Epoch [4/5] Step [330/1610]: discriminator_loss = 0.54492, target_loss = 0.38985, acc = 0.50000\n",
            "Epoch [4/5] Step [331/1610]: discriminator_loss = 0.55665, target_loss = 0.41034, acc = 0.50000\n",
            "Epoch [4/5] Step [332/1610]: discriminator_loss = 0.55083, target_loss = 0.41951, acc = 0.50000\n",
            "Epoch [4/5] Step [333/1610]: discriminator_loss = 0.61467, target_loss = 0.39129, acc = 0.50000\n",
            "Epoch [4/5] Step [334/1610]: discriminator_loss = 0.54105, target_loss = 0.38987, acc = 0.50000\n",
            "Epoch [4/5] Step [335/1610]: discriminator_loss = 0.53243, target_loss = 0.40798, acc = 0.50000\n",
            "Epoch [4/5] Step [336/1610]: discriminator_loss = 0.56830, target_loss = 0.39961, acc = 0.50000\n",
            "Epoch [4/5] Step [337/1610]: discriminator_loss = 0.62697, target_loss = 0.39898, acc = 0.50000\n",
            "Epoch [4/5] Step [338/1610]: discriminator_loss = 0.58622, target_loss = 0.39162, acc = 0.50000\n",
            "Epoch [4/5] Step [339/1610]: discriminator_loss = 0.56621, target_loss = 0.39575, acc = 0.50000\n",
            "Epoch [4/5] Step [340/1610]: discriminator_loss = 0.55603, target_loss = 0.38219, acc = 0.50000\n",
            "Epoch [4/5] Step [341/1610]: discriminator_loss = 0.57541, target_loss = 0.40035, acc = 0.50000\n",
            "Epoch [4/5] Step [342/1610]: discriminator_loss = 0.54263, target_loss = 0.42115, acc = 0.50000\n",
            "Epoch [4/5] Step [343/1610]: discriminator_loss = 0.55559, target_loss = 0.41100, acc = 0.50000\n",
            "Epoch [4/5] Step [344/1610]: discriminator_loss = 0.53849, target_loss = 0.38960, acc = 0.50000\n",
            "Epoch [4/5] Step [345/1610]: discriminator_loss = 0.54565, target_loss = 0.39838, acc = 0.50000\n",
            "Epoch [4/5] Step [346/1610]: discriminator_loss = 0.56145, target_loss = 0.39709, acc = 0.50000\n",
            "Epoch [4/5] Step [347/1610]: discriminator_loss = 0.54724, target_loss = 0.42322, acc = 0.50000\n",
            "Epoch [4/5] Step [348/1610]: discriminator_loss = 0.56039, target_loss = 0.39769, acc = 0.50000\n",
            "Epoch [4/5] Step [349/1610]: discriminator_loss = 0.57562, target_loss = 0.39216, acc = 0.50000\n",
            "Epoch [4/5] Step [350/1610]: discriminator_loss = 0.55220, target_loss = 0.40853, acc = 0.50000\n",
            "Epoch [4/5] Step [351/1610]: discriminator_loss = 0.57205, target_loss = 0.36724, acc = 0.50000\n",
            "Epoch [4/5] Step [352/1610]: discriminator_loss = 0.59077, target_loss = 0.39572, acc = 0.50000\n",
            "Epoch [4/5] Step [353/1610]: discriminator_loss = 0.58292, target_loss = 0.39100, acc = 0.50000\n",
            "Epoch [4/5] Step [354/1610]: discriminator_loss = 0.57946, target_loss = 0.36575, acc = 0.50000\n",
            "Epoch [4/5] Step [355/1610]: discriminator_loss = 0.60039, target_loss = 0.39546, acc = 0.50000\n",
            "Epoch [4/5] Step [356/1610]: discriminator_loss = 0.57552, target_loss = 0.38518, acc = 0.50000\n",
            "Epoch [4/5] Step [357/1610]: discriminator_loss = 0.56538, target_loss = 0.38857, acc = 0.50000\n",
            "Epoch [4/5] Step [358/1610]: discriminator_loss = 0.56773, target_loss = 0.39196, acc = 0.50000\n",
            "Epoch [4/5] Step [359/1610]: discriminator_loss = 0.61022, target_loss = 0.38218, acc = 0.50000\n",
            "Epoch [4/5] Step [360/1610]: discriminator_loss = 0.54270, target_loss = 0.41879, acc = 0.50000\n",
            "Epoch [4/5] Step [361/1610]: discriminator_loss = 0.58933, target_loss = 0.39464, acc = 0.50000\n",
            "Epoch [4/5] Step [362/1610]: discriminator_loss = 0.53926, target_loss = 0.40858, acc = 0.50000\n",
            "Epoch [4/5] Step [363/1610]: discriminator_loss = 0.55037, target_loss = 0.41206, acc = 0.50000\n",
            "Epoch [4/5] Step [364/1610]: discriminator_loss = 0.55046, target_loss = 0.39053, acc = 0.50000\n",
            "Epoch [4/5] Step [365/1610]: discriminator_loss = 0.52493, target_loss = 0.43704, acc = 0.50000\n",
            "Epoch [4/5] Step [366/1610]: discriminator_loss = 0.51697, target_loss = 0.43251, acc = 0.50000\n",
            "Epoch [4/5] Step [367/1610]: discriminator_loss = 0.54229, target_loss = 0.39765, acc = 0.50000\n",
            "Epoch [4/5] Step [368/1610]: discriminator_loss = 0.54754, target_loss = 0.41598, acc = 0.50000\n",
            "Epoch [4/5] Step [369/1610]: discriminator_loss = 0.57211, target_loss = 0.43511, acc = 0.50000\n",
            "Epoch [4/5] Step [370/1610]: discriminator_loss = 0.52910, target_loss = 0.42456, acc = 0.50000\n",
            "Epoch [4/5] Step [371/1610]: discriminator_loss = 0.54961, target_loss = 0.38578, acc = 0.50000\n",
            "Epoch [4/5] Step [372/1610]: discriminator_loss = 0.55259, target_loss = 0.39311, acc = 0.50000\n",
            "Epoch [4/5] Step [373/1610]: discriminator_loss = 0.53383, target_loss = 0.42103, acc = 0.50000\n",
            "Epoch [4/5] Step [374/1610]: discriminator_loss = 0.53895, target_loss = 0.38150, acc = 0.50000\n",
            "Epoch [4/5] Step [375/1610]: discriminator_loss = 0.57980, target_loss = 0.37228, acc = 0.50000\n",
            "Epoch [4/5] Step [376/1610]: discriminator_loss = 0.77105, target_loss = 0.39147, acc = 0.50000\n",
            "Epoch [4/5] Step [377/1610]: discriminator_loss = 0.60582, target_loss = 0.39263, acc = 0.50000\n",
            "Epoch [4/5] Step [378/1610]: discriminator_loss = 0.53884, target_loss = 0.42378, acc = 0.50000\n",
            "Epoch [4/5] Step [379/1610]: discriminator_loss = 0.55002, target_loss = 0.40083, acc = 0.50000\n",
            "Epoch [4/5] Step [380/1610]: discriminator_loss = 0.56735, target_loss = 0.40854, acc = 0.50000\n",
            "Epoch [4/5] Step [381/1610]: discriminator_loss = 0.54513, target_loss = 0.41948, acc = 0.50000\n",
            "Epoch [4/5] Step [382/1610]: discriminator_loss = 0.53806, target_loss = 0.42483, acc = 0.50000\n",
            "Epoch [4/5] Step [383/1610]: discriminator_loss = 0.54418, target_loss = 0.41567, acc = 0.50000\n",
            "Epoch [4/5] Step [384/1610]: discriminator_loss = 0.53638, target_loss = 0.42563, acc = 0.50000\n",
            "Epoch [4/5] Step [385/1610]: discriminator_loss = 0.54295, target_loss = 0.42015, acc = 0.50000\n",
            "Epoch [4/5] Step [386/1610]: discriminator_loss = 0.54100, target_loss = 0.41399, acc = 0.50000\n",
            "Epoch [4/5] Step [387/1610]: discriminator_loss = 0.53339, target_loss = 0.42665, acc = 0.50000\n",
            "Epoch [4/5] Step [388/1610]: discriminator_loss = 0.56956, target_loss = 0.37769, acc = 0.50000\n",
            "Epoch [4/5] Step [389/1610]: discriminator_loss = 0.55889, target_loss = 0.39671, acc = 0.50000\n",
            "Epoch [4/5] Step [390/1610]: discriminator_loss = 0.53760, target_loss = 0.40967, acc = 0.53125\n",
            "Epoch [4/5] Step [391/1610]: discriminator_loss = 0.56325, target_loss = 0.40722, acc = 0.50000\n",
            "Epoch [4/5] Step [392/1610]: discriminator_loss = 0.57656, target_loss = 0.39507, acc = 0.50000\n",
            "Epoch [4/5] Step [393/1610]: discriminator_loss = 0.56755, target_loss = 0.38067, acc = 0.50000\n",
            "Epoch [4/5] Step [394/1610]: discriminator_loss = 0.57206, target_loss = 0.39467, acc = 0.50000\n",
            "Epoch [4/5] Step [395/1610]: discriminator_loss = 0.56883, target_loss = 0.39169, acc = 0.50000\n",
            "Epoch [4/5] Step [396/1610]: discriminator_loss = 0.59140, target_loss = 0.39856, acc = 0.50000\n",
            "Epoch [4/5] Step [397/1610]: discriminator_loss = 0.61150, target_loss = 0.37169, acc = 0.50000\n",
            "Epoch [4/5] Step [398/1610]: discriminator_loss = 0.58063, target_loss = 0.39081, acc = 0.50000\n",
            "Epoch [4/5] Step [399/1610]: discriminator_loss = 0.58049, target_loss = 0.36822, acc = 0.50000\n",
            "Epoch [4/5] Step [400/1610]: discriminator_loss = 0.60661, target_loss = 0.40705, acc = 0.50000\n",
            "Epoch [4/5] Step [401/1610]: discriminator_loss = 0.58259, target_loss = 0.38790, acc = 0.50000\n",
            "Epoch [4/5] Step [402/1610]: discriminator_loss = 0.58425, target_loss = 0.38774, acc = 0.50000\n",
            "Epoch [4/5] Step [403/1610]: discriminator_loss = 0.58514, target_loss = 0.40048, acc = 0.50000\n",
            "Epoch [4/5] Step [404/1610]: discriminator_loss = 0.57361, target_loss = 0.41734, acc = 0.50000\n",
            "Epoch [4/5] Step [405/1610]: discriminator_loss = 0.57078, target_loss = 0.39346, acc = 0.50000\n",
            "Epoch [4/5] Step [406/1610]: discriminator_loss = 0.54959, target_loss = 0.40348, acc = 0.50000\n",
            "Epoch [4/5] Step [407/1610]: discriminator_loss = 0.57083, target_loss = 0.41338, acc = 0.50000\n",
            "Epoch [4/5] Step [408/1610]: discriminator_loss = 0.55108, target_loss = 0.42297, acc = 0.50000\n",
            "Epoch [4/5] Step [409/1610]: discriminator_loss = 0.54245, target_loss = 0.40896, acc = 0.50000\n",
            "Epoch [4/5] Step [410/1610]: discriminator_loss = 0.56562, target_loss = 0.42322, acc = 0.50000\n",
            "Epoch [4/5] Step [411/1610]: discriminator_loss = 0.58683, target_loss = 0.40199, acc = 0.50000\n",
            "Epoch [4/5] Step [412/1610]: discriminator_loss = 0.56310, target_loss = 0.41964, acc = 0.50000\n",
            "Epoch [4/5] Step [413/1610]: discriminator_loss = 0.54529, target_loss = 0.42050, acc = 0.50000\n",
            "Epoch [4/5] Step [414/1610]: discriminator_loss = 0.54319, target_loss = 0.41956, acc = 0.50000\n",
            "Epoch [4/5] Step [415/1610]: discriminator_loss = 0.53935, target_loss = 0.42600, acc = 0.50000\n",
            "Epoch [4/5] Step [416/1610]: discriminator_loss = 0.54373, target_loss = 0.41549, acc = 0.50000\n",
            "Epoch [4/5] Step [417/1610]: discriminator_loss = 0.57903, target_loss = 0.41420, acc = 0.50000\n",
            "Epoch [4/5] Step [418/1610]: discriminator_loss = 0.54018, target_loss = 0.41194, acc = 0.50000\n",
            "Epoch [4/5] Step [419/1610]: discriminator_loss = 0.53769, target_loss = 0.41360, acc = 0.50000\n",
            "Epoch [4/5] Step [420/1610]: discriminator_loss = 0.55471, target_loss = 0.41111, acc = 0.50000\n",
            "Epoch [4/5] Step [421/1610]: discriminator_loss = 0.55526, target_loss = 0.41329, acc = 0.50000\n",
            "Epoch [4/5] Step [422/1610]: discriminator_loss = 0.56071, target_loss = 0.40879, acc = 0.50000\n",
            "Epoch [4/5] Step [423/1610]: discriminator_loss = 0.54858, target_loss = 0.41497, acc = 0.50000\n",
            "Epoch [4/5] Step [424/1610]: discriminator_loss = 0.55892, target_loss = 0.40956, acc = 0.50000\n",
            "Epoch [4/5] Step [425/1610]: discriminator_loss = 0.58991, target_loss = 0.40066, acc = 0.50000\n",
            "Epoch [4/5] Step [426/1610]: discriminator_loss = 0.56521, target_loss = 0.40390, acc = 0.50000\n",
            "Epoch [4/5] Step [427/1610]: discriminator_loss = 0.57305, target_loss = 0.41345, acc = 0.50000\n",
            "Epoch [4/5] Step [428/1610]: discriminator_loss = 0.57573, target_loss = 0.40620, acc = 0.50000\n",
            "Epoch [4/5] Step [429/1610]: discriminator_loss = 0.57404, target_loss = 0.40043, acc = 0.50000\n",
            "Epoch [4/5] Step [430/1610]: discriminator_loss = 0.56749, target_loss = 0.40606, acc = 0.50000\n",
            "Epoch [4/5] Step [431/1610]: discriminator_loss = 0.56486, target_loss = 0.38155, acc = 0.50000\n",
            "Epoch [4/5] Step [432/1610]: discriminator_loss = 0.58131, target_loss = 0.40134, acc = 0.50000\n",
            "Epoch [4/5] Step [433/1610]: discriminator_loss = 0.56473, target_loss = 0.39813, acc = 0.50000\n",
            "Epoch [4/5] Step [434/1610]: discriminator_loss = 0.59199, target_loss = 0.40256, acc = 0.50000\n",
            "Epoch [4/5] Step [435/1610]: discriminator_loss = 0.56069, target_loss = 0.41274, acc = 0.50000\n",
            "Epoch [4/5] Step [436/1610]: discriminator_loss = 0.56355, target_loss = 0.40396, acc = 0.50000\n",
            "Epoch [4/5] Step [437/1610]: discriminator_loss = 0.63016, target_loss = 0.39065, acc = 0.50000\n",
            "Epoch [4/5] Step [438/1610]: discriminator_loss = 0.55679, target_loss = 0.40202, acc = 0.50000\n",
            "Epoch [4/5] Step [439/1610]: discriminator_loss = 0.54727, target_loss = 0.41466, acc = 0.50000\n",
            "Epoch [4/5] Step [440/1610]: discriminator_loss = 0.56046, target_loss = 0.40636, acc = 0.50000\n",
            "Epoch [4/5] Step [441/1610]: discriminator_loss = 0.55923, target_loss = 0.41441, acc = 0.50000\n",
            "Epoch [4/5] Step [442/1610]: discriminator_loss = 0.55509, target_loss = 0.40554, acc = 0.50000\n",
            "Epoch [4/5] Step [443/1610]: discriminator_loss = 0.56554, target_loss = 0.39792, acc = 0.50000\n",
            "Epoch [4/5] Step [444/1610]: discriminator_loss = 0.55716, target_loss = 0.40525, acc = 0.50000\n",
            "Epoch [4/5] Step [445/1610]: discriminator_loss = 0.54286, target_loss = 0.39394, acc = 0.50000\n",
            "Epoch [4/5] Step [446/1610]: discriminator_loss = 0.58513, target_loss = 0.39361, acc = 0.50000\n",
            "Epoch [4/5] Step [447/1610]: discriminator_loss = 0.56426, target_loss = 0.42320, acc = 0.50000\n",
            "Epoch [4/5] Step [448/1610]: discriminator_loss = 0.55644, target_loss = 0.43906, acc = 0.50000\n",
            "Epoch [4/5] Step [449/1610]: discriminator_loss = 0.55384, target_loss = 0.40599, acc = 0.50000\n",
            "Epoch [4/5] Step [450/1610]: discriminator_loss = 0.55398, target_loss = 0.41121, acc = 0.50000\n",
            "Epoch [4/5] Step [451/1610]: discriminator_loss = 0.56969, target_loss = 0.39972, acc = 0.50000\n",
            "Epoch [4/5] Step [452/1610]: discriminator_loss = 0.57196, target_loss = 0.41410, acc = 0.50000\n",
            "Epoch [4/5] Step [453/1610]: discriminator_loss = 0.56215, target_loss = 0.40119, acc = 0.50000\n",
            "Epoch [4/5] Step [454/1610]: discriminator_loss = 0.56003, target_loss = 0.39745, acc = 0.50000\n",
            "Epoch [4/5] Step [455/1610]: discriminator_loss = 0.58108, target_loss = 0.37055, acc = 0.50000\n",
            "Epoch [4/5] Step [456/1610]: discriminator_loss = 0.58491, target_loss = 0.39831, acc = 0.50000\n",
            "Epoch [4/5] Step [457/1610]: discriminator_loss = 0.56312, target_loss = 0.40024, acc = 0.50000\n",
            "Epoch [4/5] Step [458/1610]: discriminator_loss = 0.55939, target_loss = 0.38982, acc = 0.50000\n",
            "Epoch [4/5] Step [459/1610]: discriminator_loss = 0.56436, target_loss = 0.39499, acc = 0.50000\n",
            "Epoch [4/5] Step [460/1610]: discriminator_loss = 0.55111, target_loss = 0.40212, acc = 0.50000\n",
            "Epoch [4/5] Step [461/1610]: discriminator_loss = 0.58354, target_loss = 0.40100, acc = 0.50000\n",
            "Epoch [4/5] Step [462/1610]: discriminator_loss = 0.53951, target_loss = 0.38272, acc = 0.50000\n",
            "Epoch [4/5] Step [463/1610]: discriminator_loss = 0.57290, target_loss = 0.40113, acc = 0.50000\n",
            "Epoch [4/5] Step [464/1610]: discriminator_loss = 0.55158, target_loss = 0.39687, acc = 0.50000\n",
            "Epoch [4/5] Step [465/1610]: discriminator_loss = 0.65789, target_loss = 0.38471, acc = 0.50000\n",
            "Epoch [4/5] Step [466/1610]: discriminator_loss = 0.55398, target_loss = 0.42266, acc = 0.50000\n",
            "Epoch [4/5] Step [467/1610]: discriminator_loss = 0.56231, target_loss = 0.38567, acc = 0.50000\n",
            "Epoch [4/5] Step [468/1610]: discriminator_loss = 0.53756, target_loss = 0.41219, acc = 0.50000\n",
            "Epoch [4/5] Step [469/1610]: discriminator_loss = 0.54677, target_loss = 0.41143, acc = 0.50000\n",
            "Epoch [4/5] Step [470/1610]: discriminator_loss = 0.53926, target_loss = 0.41057, acc = 0.50000\n",
            "Epoch [4/5] Step [471/1610]: discriminator_loss = 0.56689, target_loss = 0.39467, acc = 0.50000\n",
            "Epoch [4/5] Step [472/1610]: discriminator_loss = 0.58583, target_loss = 0.40741, acc = 0.50000\n",
            "Epoch [4/5] Step [473/1610]: discriminator_loss = 0.55942, target_loss = 0.41725, acc = 0.50000\n",
            "Epoch [4/5] Step [474/1610]: discriminator_loss = 0.59808, target_loss = 0.41410, acc = 0.50000\n",
            "Epoch [4/5] Step [475/1610]: discriminator_loss = 0.55669, target_loss = 0.42047, acc = 0.50000\n",
            "Epoch [4/5] Step [476/1610]: discriminator_loss = 0.54292, target_loss = 0.41036, acc = 0.50000\n",
            "Epoch [4/5] Step [477/1610]: discriminator_loss = 0.54891, target_loss = 0.42099, acc = 0.50000\n",
            "Epoch [4/5] Step [478/1610]: discriminator_loss = 0.57940, target_loss = 0.41356, acc = 0.50000\n",
            "Epoch [4/5] Step [479/1610]: discriminator_loss = 0.56076, target_loss = 0.40506, acc = 0.50000\n",
            "Epoch [4/5] Step [480/1610]: discriminator_loss = 0.56780, target_loss = 0.41281, acc = 0.50000\n",
            "Epoch [4/5] Step [481/1610]: discriminator_loss = 0.56361, target_loss = 0.38734, acc = 0.50000\n",
            "Epoch [4/5] Step [482/1610]: discriminator_loss = 0.57290, target_loss = 0.38719, acc = 0.50000\n",
            "Epoch [4/5] Step [483/1610]: discriminator_loss = 0.54984, target_loss = 0.40963, acc = 0.50000\n",
            "Epoch [4/5] Step [484/1610]: discriminator_loss = 0.55011, target_loss = 0.41202, acc = 0.50000\n",
            "Epoch [4/5] Step [485/1610]: discriminator_loss = 0.56825, target_loss = 0.40123, acc = 0.50000\n",
            "Epoch [4/5] Step [486/1610]: discriminator_loss = 0.57014, target_loss = 0.37816, acc = 0.50000\n",
            "Epoch [4/5] Step [487/1610]: discriminator_loss = 0.56722, target_loss = 0.38829, acc = 0.50000\n",
            "Epoch [4/5] Step [488/1610]: discriminator_loss = 0.57596, target_loss = 0.40010, acc = 0.50000\n",
            "Epoch [4/5] Step [489/1610]: discriminator_loss = 0.56017, target_loss = 0.38885, acc = 0.50000\n",
            "Epoch [4/5] Step [490/1610]: discriminator_loss = 0.58901, target_loss = 0.35821, acc = 0.50000\n",
            "Epoch [4/5] Step [491/1610]: discriminator_loss = 0.57377, target_loss = 0.38957, acc = 0.50000\n",
            "Epoch [4/5] Step [492/1610]: discriminator_loss = 0.57454, target_loss = 0.37438, acc = 0.50000\n",
            "Epoch [4/5] Step [493/1610]: discriminator_loss = 0.60185, target_loss = 0.36121, acc = 0.50000\n",
            "Epoch [4/5] Step [494/1610]: discriminator_loss = 0.57507, target_loss = 0.39737, acc = 0.50000\n",
            "Epoch [4/5] Step [495/1610]: discriminator_loss = 0.57634, target_loss = 0.37293, acc = 0.50000\n",
            "Epoch [4/5] Step [496/1610]: discriminator_loss = 0.57872, target_loss = 0.37885, acc = 0.50000\n",
            "Epoch [4/5] Step [497/1610]: discriminator_loss = 0.56690, target_loss = 0.38893, acc = 0.50000\n",
            "Epoch [4/5] Step [498/1610]: discriminator_loss = 0.58216, target_loss = 0.40277, acc = 0.50000\n",
            "Epoch [4/5] Step [499/1610]: discriminator_loss = 0.56728, target_loss = 0.37500, acc = 0.50000\n",
            "Epoch [4/5] Step [500/1610]: discriminator_loss = 0.62596, target_loss = 0.40298, acc = 0.50000\n",
            "Epoch [4/5] Step [501/1610]: discriminator_loss = 0.61626, target_loss = 0.40187, acc = 0.50000\n",
            "Epoch [4/5] Step [502/1610]: discriminator_loss = 0.55473, target_loss = 0.42428, acc = 0.50000\n",
            "Epoch [4/5] Step [503/1610]: discriminator_loss = 0.55632, target_loss = 0.41272, acc = 0.50000\n",
            "Epoch [4/5] Step [504/1610]: discriminator_loss = 0.56985, target_loss = 0.41212, acc = 0.50000\n",
            "Epoch [4/5] Step [505/1610]: discriminator_loss = 0.55728, target_loss = 0.40718, acc = 0.50000\n",
            "Epoch [4/5] Step [506/1610]: discriminator_loss = 0.54027, target_loss = 0.41288, acc = 0.50000\n",
            "Epoch [4/5] Step [507/1610]: discriminator_loss = 0.60468, target_loss = 0.41459, acc = 0.50000\n",
            "Epoch [4/5] Step [508/1610]: discriminator_loss = 0.58108, target_loss = 0.39737, acc = 0.50000\n",
            "Epoch [4/5] Step [509/1610]: discriminator_loss = 0.55865, target_loss = 0.42770, acc = 0.50000\n",
            "Epoch [4/5] Step [510/1610]: discriminator_loss = 0.58379, target_loss = 0.40804, acc = 0.50000\n",
            "Epoch [4/5] Step [511/1610]: discriminator_loss = 0.55858, target_loss = 0.41182, acc = 0.50000\n",
            "Epoch [4/5] Step [512/1610]: discriminator_loss = 0.55647, target_loss = 0.40614, acc = 0.50000\n",
            "Epoch [4/5] Step [513/1610]: discriminator_loss = 0.55740, target_loss = 0.40397, acc = 0.50000\n",
            "Epoch [4/5] Step [514/1610]: discriminator_loss = 0.56573, target_loss = 0.40257, acc = 0.50000\n",
            "Epoch [4/5] Step [515/1610]: discriminator_loss = 0.58491, target_loss = 0.38899, acc = 0.50000\n",
            "Epoch [4/5] Step [516/1610]: discriminator_loss = 0.55949, target_loss = 0.41048, acc = 0.50000\n",
            "Epoch [4/5] Step [517/1610]: discriminator_loss = 0.55790, target_loss = 0.40915, acc = 0.50000\n",
            "Epoch [4/5] Step [518/1610]: discriminator_loss = 0.56051, target_loss = 0.40903, acc = 0.50000\n",
            "Epoch [4/5] Step [519/1610]: discriminator_loss = 0.56100, target_loss = 0.38691, acc = 0.50000\n",
            "Epoch [4/5] Step [520/1610]: discriminator_loss = 0.56005, target_loss = 0.39734, acc = 0.50000\n",
            "Epoch [4/5] Step [521/1610]: discriminator_loss = 0.57870, target_loss = 0.39104, acc = 0.50000\n",
            "Epoch [4/5] Step [522/1610]: discriminator_loss = 0.57130, target_loss = 0.40456, acc = 0.50000\n",
            "Epoch [4/5] Step [523/1610]: discriminator_loss = 0.56893, target_loss = 0.38950, acc = 0.50000\n",
            "Epoch [4/5] Step [524/1610]: discriminator_loss = 0.56682, target_loss = 0.40814, acc = 0.50000\n",
            "Epoch [4/5] Step [525/1610]: discriminator_loss = 0.54719, target_loss = 0.40137, acc = 0.50000\n",
            "Epoch [4/5] Step [526/1610]: discriminator_loss = 0.56198, target_loss = 0.39987, acc = 0.50000\n",
            "Epoch [4/5] Step [527/1610]: discriminator_loss = 0.56524, target_loss = 0.40471, acc = 0.50000\n",
            "Epoch [4/5] Step [528/1610]: discriminator_loss = 0.57928, target_loss = 0.39045, acc = 0.50000\n",
            "Epoch [4/5] Step [529/1610]: discriminator_loss = 0.54985, target_loss = 0.40136, acc = 0.50000\n",
            "Epoch [4/5] Step [530/1610]: discriminator_loss = 0.56275, target_loss = 0.39919, acc = 0.50000\n",
            "Epoch [4/5] Step [531/1610]: discriminator_loss = 0.57557, target_loss = 0.37889, acc = 0.50000\n",
            "Epoch [4/5] Step [532/1610]: discriminator_loss = 0.57361, target_loss = 0.38916, acc = 0.50000\n",
            "Epoch [4/5] Step [533/1610]: discriminator_loss = 0.55247, target_loss = 0.41481, acc = 0.50000\n",
            "Epoch [4/5] Step [534/1610]: discriminator_loss = 0.53853, target_loss = 0.37384, acc = 0.50000\n",
            "Epoch [4/5] Step [535/1610]: discriminator_loss = 0.56122, target_loss = 0.40159, acc = 0.50000\n",
            "Epoch [4/5] Step [536/1610]: discriminator_loss = 0.59831, target_loss = 0.39288, acc = 0.50000\n",
            "Epoch [4/5] Step [537/1610]: discriminator_loss = 0.56022, target_loss = 0.41501, acc = 0.50000\n",
            "Epoch [4/5] Step [538/1610]: discriminator_loss = 0.56142, target_loss = 0.39531, acc = 0.50000\n",
            "Epoch [4/5] Step [539/1610]: discriminator_loss = 0.54977, target_loss = 0.38002, acc = 0.50000\n",
            "Epoch [4/5] Step [540/1610]: discriminator_loss = 0.54118, target_loss = 0.38966, acc = 0.50000\n",
            "Epoch [4/5] Step [541/1610]: discriminator_loss = 0.58051, target_loss = 0.39509, acc = 0.50000\n",
            "Epoch [4/5] Step [542/1610]: discriminator_loss = 0.58364, target_loss = 0.40440, acc = 0.50000\n",
            "Epoch [4/5] Step [543/1610]: discriminator_loss = 0.58236, target_loss = 0.43113, acc = 0.50000\n",
            "Epoch [4/5] Step [544/1610]: discriminator_loss = 0.56224, target_loss = 0.40194, acc = 0.50000\n",
            "Epoch [4/5] Step [545/1610]: discriminator_loss = 0.59122, target_loss = 0.39041, acc = 0.50000\n",
            "Epoch [4/5] Step [546/1610]: discriminator_loss = 0.55535, target_loss = 0.40956, acc = 0.50000\n",
            "Epoch [4/5] Step [547/1610]: discriminator_loss = 0.52562, target_loss = 0.41793, acc = 0.50000\n",
            "Epoch [4/5] Step [548/1610]: discriminator_loss = 0.53062, target_loss = 0.41573, acc = 0.50000\n",
            "Epoch [4/5] Step [549/1610]: discriminator_loss = 0.54587, target_loss = 0.39505, acc = 0.50000\n",
            "Epoch [4/5] Step [550/1610]: discriminator_loss = 0.53823, target_loss = 0.42805, acc = 0.50000\n",
            "Epoch [4/5] Step [551/1610]: discriminator_loss = 0.55564, target_loss = 0.41456, acc = 0.50000\n",
            "Epoch [4/5] Step [552/1610]: discriminator_loss = 0.53212, target_loss = 0.42802, acc = 0.50000\n",
            "Epoch [4/5] Step [553/1610]: discriminator_loss = 0.53838, target_loss = 0.42603, acc = 0.50000\n",
            "Epoch [4/5] Step [554/1610]: discriminator_loss = 0.54687, target_loss = 0.40798, acc = 0.50000\n",
            "Epoch [4/5] Step [555/1610]: discriminator_loss = 0.58048, target_loss = 0.39030, acc = 0.50000\n",
            "Epoch [4/5] Step [556/1610]: discriminator_loss = 0.55290, target_loss = 0.38422, acc = 0.50000\n",
            "Epoch [4/5] Step [557/1610]: discriminator_loss = 0.55775, target_loss = 0.39442, acc = 0.50000\n",
            "Epoch [4/5] Step [558/1610]: discriminator_loss = 0.57939, target_loss = 0.36994, acc = 0.50000\n",
            "Epoch [4/5] Step [559/1610]: discriminator_loss = 0.58580, target_loss = 0.37175, acc = 0.50000\n",
            "Epoch [4/5] Step [560/1610]: discriminator_loss = 0.57577, target_loss = 0.40066, acc = 0.50000\n",
            "Epoch [4/5] Step [561/1610]: discriminator_loss = 0.55901, target_loss = 0.39463, acc = 0.50000\n",
            "Epoch [4/5] Step [562/1610]: discriminator_loss = 0.59190, target_loss = 0.38227, acc = 0.50000\n",
            "Epoch [4/5] Step [563/1610]: discriminator_loss = 0.57751, target_loss = 0.38228, acc = 0.50000\n",
            "Epoch [4/5] Step [564/1610]: discriminator_loss = 0.57213, target_loss = 0.39707, acc = 0.50000\n",
            "Epoch [4/5] Step [565/1610]: discriminator_loss = 0.57434, target_loss = 0.38791, acc = 0.50000\n",
            "Epoch [4/5] Step [566/1610]: discriminator_loss = 0.58663, target_loss = 0.37595, acc = 0.50000\n",
            "Epoch [4/5] Step [567/1610]: discriminator_loss = 0.60333, target_loss = 0.38687, acc = 0.50000\n",
            "Epoch [4/5] Step [568/1610]: discriminator_loss = 0.58703, target_loss = 0.39124, acc = 0.50000\n",
            "Epoch [4/5] Step [569/1610]: discriminator_loss = 0.55710, target_loss = 0.39989, acc = 0.50000\n",
            "Epoch [4/5] Step [570/1610]: discriminator_loss = 0.55943, target_loss = 0.40527, acc = 0.50000\n",
            "Epoch [4/5] Step [571/1610]: discriminator_loss = 0.55877, target_loss = 0.40745, acc = 0.50000\n",
            "Epoch [4/5] Step [572/1610]: discriminator_loss = 0.54185, target_loss = 0.41051, acc = 0.50000\n",
            "Epoch [4/5] Step [573/1610]: discriminator_loss = 0.54362, target_loss = 0.38974, acc = 0.50000\n",
            "Epoch [4/5] Step [574/1610]: discriminator_loss = 0.54814, target_loss = 0.38880, acc = 0.50000\n",
            "Epoch [4/5] Step [575/1610]: discriminator_loss = 0.60913, target_loss = 0.39463, acc = 0.50000\n",
            "Epoch [4/5] Step [576/1610]: discriminator_loss = 0.61898, target_loss = 0.40493, acc = 0.50000\n",
            "Epoch [4/5] Step [577/1610]: discriminator_loss = 0.52934, target_loss = 0.44759, acc = 0.50000\n",
            "Epoch [4/5] Step [578/1610]: discriminator_loss = 0.58330, target_loss = 0.42487, acc = 0.50000\n",
            "Epoch [4/5] Step [579/1610]: discriminator_loss = 0.53684, target_loss = 0.42325, acc = 0.50000\n",
            "Epoch [4/5] Step [580/1610]: discriminator_loss = 0.55549, target_loss = 0.41494, acc = 0.50000\n",
            "Epoch [4/5] Step [581/1610]: discriminator_loss = 0.51636, target_loss = 0.40924, acc = 0.50000\n",
            "Epoch [4/5] Step [582/1610]: discriminator_loss = 0.53866, target_loss = 0.42763, acc = 0.50000\n",
            "Epoch [4/5] Step [583/1610]: discriminator_loss = 0.54437, target_loss = 0.42317, acc = 0.50000\n",
            "Epoch [4/5] Step [584/1610]: discriminator_loss = 0.54933, target_loss = 0.42628, acc = 0.50000\n",
            "Epoch [4/5] Step [585/1610]: discriminator_loss = 0.54790, target_loss = 0.41637, acc = 0.50000\n",
            "Epoch [4/5] Step [586/1610]: discriminator_loss = 0.55230, target_loss = 0.41194, acc = 0.50000\n",
            "Epoch [4/5] Step [587/1610]: discriminator_loss = 0.56488, target_loss = 0.40625, acc = 0.50000\n",
            "Epoch [4/5] Step [588/1610]: discriminator_loss = 0.55450, target_loss = 0.40510, acc = 0.50000\n",
            "Epoch [4/5] Step [589/1610]: discriminator_loss = 0.56848, target_loss = 0.40255, acc = 0.50000\n",
            "Epoch [4/5] Step [590/1610]: discriminator_loss = 0.56863, target_loss = 0.40354, acc = 0.50000\n",
            "Epoch [4/5] Step [591/1610]: discriminator_loss = 0.58160, target_loss = 0.39742, acc = 0.50000\n",
            "Epoch [4/5] Step [592/1610]: discriminator_loss = 0.55680, target_loss = 0.39478, acc = 0.50000\n",
            "Epoch [4/5] Step [593/1610]: discriminator_loss = 0.57989, target_loss = 0.37512, acc = 0.50000\n",
            "Epoch [4/5] Step [594/1610]: discriminator_loss = 0.56406, target_loss = 0.39174, acc = 0.50000\n",
            "Epoch [4/5] Step [595/1610]: discriminator_loss = 0.56695, target_loss = 0.38321, acc = 0.50000\n",
            "Epoch [4/5] Step [596/1610]: discriminator_loss = 0.57702, target_loss = 0.39124, acc = 0.50000\n",
            "Epoch [4/5] Step [597/1610]: discriminator_loss = 0.57702, target_loss = 0.39121, acc = 0.50000\n",
            "Epoch [4/5] Step [598/1610]: discriminator_loss = 0.61535, target_loss = 0.40340, acc = 0.50000\n",
            "Epoch [4/5] Step [599/1610]: discriminator_loss = 0.57249, target_loss = 0.39222, acc = 0.50000\n",
            "Epoch [4/5] Step [600/1610]: discriminator_loss = 0.58469, target_loss = 0.38154, acc = 0.50000\n",
            "Epoch [4/5] Step [601/1610]: discriminator_loss = 0.58341, target_loss = 0.39497, acc = 0.50000\n",
            "Epoch [4/5] Step [602/1610]: discriminator_loss = 0.59591, target_loss = 0.38168, acc = 0.50000\n",
            "Epoch [4/5] Step [603/1610]: discriminator_loss = 0.57617, target_loss = 0.39119, acc = 0.50000\n",
            "Epoch [4/5] Step [604/1610]: discriminator_loss = 0.59595, target_loss = 0.40137, acc = 0.50000\n",
            "Epoch [4/5] Step [605/1610]: discriminator_loss = 0.55532, target_loss = 0.39466, acc = 0.50000\n",
            "Epoch [4/5] Step [606/1610]: discriminator_loss = 0.56803, target_loss = 0.39710, acc = 0.50000\n",
            "Epoch [4/5] Step [607/1610]: discriminator_loss = 0.54237, target_loss = 0.40590, acc = 0.50000\n",
            "Epoch [4/5] Step [608/1610]: discriminator_loss = 0.58684, target_loss = 0.40567, acc = 0.50000\n",
            "Epoch [4/5] Step [609/1610]: discriminator_loss = 0.55811, target_loss = 0.39881, acc = 0.50000\n",
            "Epoch [4/5] Step [610/1610]: discriminator_loss = 0.55765, target_loss = 0.40607, acc = 0.50000\n",
            "Epoch [4/5] Step [611/1610]: discriminator_loss = 0.56663, target_loss = 0.41662, acc = 0.50000\n",
            "Epoch [4/5] Step [612/1610]: discriminator_loss = 0.57600, target_loss = 0.37936, acc = 0.50000\n",
            "Epoch [4/5] Step [613/1610]: discriminator_loss = 0.56889, target_loss = 0.40145, acc = 0.50000\n",
            "Epoch [4/5] Step [614/1610]: discriminator_loss = 0.57032, target_loss = 0.39927, acc = 0.50000\n",
            "Epoch [4/5] Step [615/1610]: discriminator_loss = 0.56225, target_loss = 0.41184, acc = 0.50000\n",
            "Epoch [4/5] Step [616/1610]: discriminator_loss = 0.54111, target_loss = 0.39563, acc = 0.50000\n",
            "Epoch [4/5] Step [617/1610]: discriminator_loss = 0.60031, target_loss = 0.41778, acc = 0.50000\n",
            "Epoch [4/5] Step [618/1610]: discriminator_loss = 0.57087, target_loss = 0.40182, acc = 0.50000\n",
            "Epoch [4/5] Step [619/1610]: discriminator_loss = 0.55848, target_loss = 0.39630, acc = 0.50000\n",
            "Epoch [4/5] Step [620/1610]: discriminator_loss = 0.54508, target_loss = 0.40129, acc = 0.50000\n",
            "Epoch [4/5] Step [621/1610]: discriminator_loss = 0.54343, target_loss = 0.40704, acc = 0.50000\n",
            "Epoch [4/5] Step [622/1610]: discriminator_loss = 0.56710, target_loss = 0.40640, acc = 0.50000\n",
            "Epoch [4/5] Step [623/1610]: discriminator_loss = 0.58626, target_loss = 0.40475, acc = 0.50000\n",
            "Epoch [4/5] Step [624/1610]: discriminator_loss = 0.55366, target_loss = 0.39192, acc = 0.50000\n",
            "Epoch [4/5] Step [625/1610]: discriminator_loss = 0.53634, target_loss = 0.42779, acc = 0.50000\n",
            "Epoch [4/5] Step [626/1610]: discriminator_loss = 0.54140, target_loss = 0.40690, acc = 0.50000\n",
            "Epoch [4/5] Step [627/1610]: discriminator_loss = 0.56372, target_loss = 0.39152, acc = 0.50000\n",
            "Epoch [4/5] Step [628/1610]: discriminator_loss = 0.57702, target_loss = 0.39673, acc = 0.50000\n",
            "Epoch [4/5] Step [629/1610]: discriminator_loss = 0.54051, target_loss = 0.41942, acc = 0.50000\n",
            "Epoch [4/5] Step [630/1610]: discriminator_loss = 0.55836, target_loss = 0.42725, acc = 0.50000\n",
            "Epoch [4/5] Step [631/1610]: discriminator_loss = 0.54210, target_loss = 0.41018, acc = 0.50000\n",
            "Epoch [4/5] Step [632/1610]: discriminator_loss = 0.54652, target_loss = 0.38968, acc = 0.50000\n",
            "Epoch [4/5] Step [633/1610]: discriminator_loss = 0.54473, target_loss = 0.44929, acc = 0.50000\n",
            "Epoch [4/5] Step [634/1610]: discriminator_loss = 0.55273, target_loss = 0.40044, acc = 0.50000\n",
            "Epoch [4/5] Step [635/1610]: discriminator_loss = 0.52544, target_loss = 0.42647, acc = 0.50000\n",
            "Epoch [4/5] Step [636/1610]: discriminator_loss = 0.54983, target_loss = 0.40426, acc = 0.50000\n",
            "Epoch [4/5] Step [637/1610]: discriminator_loss = 0.60761, target_loss = 0.39033, acc = 0.50000\n",
            "Epoch [4/5] Step [638/1610]: discriminator_loss = 0.56289, target_loss = 0.39846, acc = 0.50000\n",
            "Epoch [4/5] Step [639/1610]: discriminator_loss = 0.61595, target_loss = 0.41042, acc = 0.50000\n",
            "Epoch [4/5] Step [640/1610]: discriminator_loss = 0.55989, target_loss = 0.38606, acc = 0.50000\n",
            "Epoch [4/5] Step [641/1610]: discriminator_loss = 0.55170, target_loss = 0.36640, acc = 0.50000\n",
            "Epoch [4/5] Step [642/1610]: discriminator_loss = 0.54858, target_loss = 0.41199, acc = 0.50000\n",
            "Epoch [4/5] Step [643/1610]: discriminator_loss = 0.58295, target_loss = 0.37908, acc = 0.50000\n",
            "Epoch [4/5] Step [644/1610]: discriminator_loss = 0.58296, target_loss = 0.40820, acc = 0.50000\n",
            "Epoch [4/5] Step [645/1610]: discriminator_loss = 0.55348, target_loss = 0.39632, acc = 0.50000\n",
            "Epoch [4/5] Step [646/1610]: discriminator_loss = 0.54645, target_loss = 0.40263, acc = 0.50000\n",
            "Epoch [4/5] Step [647/1610]: discriminator_loss = 0.54855, target_loss = 0.38658, acc = 0.50000\n",
            "Epoch [4/5] Step [648/1610]: discriminator_loss = 0.55448, target_loss = 0.38303, acc = 0.50000\n",
            "Epoch [4/5] Step [649/1610]: discriminator_loss = 0.55307, target_loss = 0.40660, acc = 0.50000\n",
            "Epoch [4/5] Step [650/1610]: discriminator_loss = 0.57750, target_loss = 0.39009, acc = 0.50000\n",
            "Epoch [4/5] Step [651/1610]: discriminator_loss = 0.56892, target_loss = 0.39414, acc = 0.50000\n",
            "Epoch [4/5] Step [652/1610]: discriminator_loss = 0.55123, target_loss = 0.37245, acc = 0.50000\n",
            "Epoch [4/5] Step [653/1610]: discriminator_loss = 0.57716, target_loss = 0.39377, acc = 0.50000\n",
            "Epoch [4/5] Step [654/1610]: discriminator_loss = 0.55883, target_loss = 0.40581, acc = 0.50000\n",
            "Epoch [4/5] Step [655/1610]: discriminator_loss = 0.61376, target_loss = 0.39297, acc = 0.50000\n",
            "Epoch [4/5] Step [656/1610]: discriminator_loss = 0.57607, target_loss = 0.36615, acc = 0.50000\n",
            "Epoch [4/5] Step [657/1610]: discriminator_loss = 0.56217, target_loss = 0.40890, acc = 0.50000\n",
            "Epoch [4/5] Step [658/1610]: discriminator_loss = 0.54923, target_loss = 0.39354, acc = 0.50000\n",
            "Epoch [4/5] Step [659/1610]: discriminator_loss = 0.55263, target_loss = 0.38192, acc = 0.50000\n",
            "Epoch [4/5] Step [660/1610]: discriminator_loss = 0.55337, target_loss = 0.40611, acc = 0.50000\n",
            "Epoch [4/5] Step [661/1610]: discriminator_loss = 0.56570, target_loss = 0.41410, acc = 0.50000\n",
            "Epoch [4/5] Step [662/1610]: discriminator_loss = 0.58443, target_loss = 0.41671, acc = 0.50000\n",
            "Epoch [4/5] Step [663/1610]: discriminator_loss = 0.59880, target_loss = 0.40294, acc = 0.50000\n",
            "Epoch [4/5] Step [664/1610]: discriminator_loss = 0.56169, target_loss = 0.41946, acc = 0.50000\n",
            "Epoch [4/5] Step [665/1610]: discriminator_loss = 0.52808, target_loss = 0.42855, acc = 0.50000\n",
            "Epoch [4/5] Step [666/1610]: discriminator_loss = 0.54580, target_loss = 0.40545, acc = 0.50000\n",
            "Epoch [4/5] Step [667/1610]: discriminator_loss = 0.54517, target_loss = 0.41045, acc = 0.50000\n",
            "Epoch [4/5] Step [668/1610]: discriminator_loss = 0.55535, target_loss = 0.38724, acc = 0.50000\n",
            "Epoch [4/5] Step [669/1610]: discriminator_loss = 0.53565, target_loss = 0.39158, acc = 0.50000\n",
            "Epoch [4/5] Step [670/1610]: discriminator_loss = 0.57943, target_loss = 0.39732, acc = 0.50000\n",
            "Epoch [4/5] Step [671/1610]: discriminator_loss = 0.56084, target_loss = 0.40874, acc = 0.50000\n",
            "Epoch [4/5] Step [672/1610]: discriminator_loss = 0.60907, target_loss = 0.39770, acc = 0.50000\n",
            "Epoch [4/5] Step [673/1610]: discriminator_loss = 0.55663, target_loss = 0.41697, acc = 0.50000\n",
            "Epoch [4/5] Step [674/1610]: discriminator_loss = 0.54664, target_loss = 0.42875, acc = 0.50000\n",
            "Epoch [4/5] Step [675/1610]: discriminator_loss = 0.54992, target_loss = 0.41957, acc = 0.50000\n",
            "Epoch [4/5] Step [676/1610]: discriminator_loss = 0.53870, target_loss = 0.40934, acc = 0.50000\n",
            "Epoch [4/5] Step [677/1610]: discriminator_loss = 0.55309, target_loss = 0.41846, acc = 0.50000\n",
            "Epoch [4/5] Step [678/1610]: discriminator_loss = 0.56047, target_loss = 0.41970, acc = 0.50000\n",
            "Epoch [4/5] Step [679/1610]: discriminator_loss = 0.52867, target_loss = 0.44041, acc = 0.50000\n",
            "Epoch [4/5] Step [680/1610]: discriminator_loss = 0.52948, target_loss = 0.42521, acc = 0.50000\n",
            "Epoch [4/5] Step [681/1610]: discriminator_loss = 0.53775, target_loss = 0.41327, acc = 0.50000\n",
            "Epoch [4/5] Step [682/1610]: discriminator_loss = 0.53948, target_loss = 0.39465, acc = 0.50000\n",
            "Epoch [4/5] Step [683/1610]: discriminator_loss = 0.53300, target_loss = 0.39861, acc = 0.50000\n",
            "Epoch [4/5] Step [684/1610]: discriminator_loss = 0.52822, target_loss = 0.42631, acc = 0.50000\n",
            "Epoch [4/5] Step [685/1610]: discriminator_loss = 0.57182, target_loss = 0.40924, acc = 0.50000\n",
            "Epoch [4/5] Step [686/1610]: discriminator_loss = 0.53700, target_loss = 0.41858, acc = 0.50000\n",
            "Epoch [4/5] Step [687/1610]: discriminator_loss = 0.60896, target_loss = 0.39662, acc = 0.50000\n",
            "Epoch [4/5] Step [688/1610]: discriminator_loss = 0.55081, target_loss = 0.41553, acc = 0.50000\n",
            "Epoch [4/5] Step [689/1610]: discriminator_loss = 0.55624, target_loss = 0.38498, acc = 0.50000\n",
            "Epoch [4/5] Step [690/1610]: discriminator_loss = 0.55397, target_loss = 0.41966, acc = 0.50000\n",
            "Epoch [4/5] Step [691/1610]: discriminator_loss = 0.55132, target_loss = 0.41628, acc = 0.50000\n",
            "Epoch [4/5] Step [692/1610]: discriminator_loss = 0.55894, target_loss = 0.40839, acc = 0.50000\n",
            "Epoch [4/5] Step [693/1610]: discriminator_loss = 0.55292, target_loss = 0.40675, acc = 0.50000\n",
            "Epoch [4/5] Step [694/1610]: discriminator_loss = 0.55842, target_loss = 0.40360, acc = 0.50000\n",
            "Epoch [4/5] Step [695/1610]: discriminator_loss = 0.57768, target_loss = 0.38173, acc = 0.50000\n",
            "Epoch [4/5] Step [696/1610]: discriminator_loss = 0.56022, target_loss = 0.39454, acc = 0.50000\n",
            "Epoch [4/5] Step [697/1610]: discriminator_loss = 0.60202, target_loss = 0.38661, acc = 0.50000\n",
            "Epoch [4/5] Step [698/1610]: discriminator_loss = 0.56275, target_loss = 0.38306, acc = 0.50000\n",
            "Epoch [4/5] Step [699/1610]: discriminator_loss = 0.60547, target_loss = 0.33515, acc = 0.50000\n",
            "Epoch [4/5] Step [700/1610]: discriminator_loss = 0.59538, target_loss = 0.39286, acc = 0.50000\n",
            "Epoch [4/5] Step [701/1610]: discriminator_loss = 0.61159, target_loss = 0.37935, acc = 0.50000\n",
            "Epoch [4/5] Step [702/1610]: discriminator_loss = 0.58677, target_loss = 0.37236, acc = 0.50000\n",
            "Epoch [4/5] Step [703/1610]: discriminator_loss = 0.57336, target_loss = 0.40279, acc = 0.50000\n",
            "Epoch [4/5] Step [704/1610]: discriminator_loss = 0.59111, target_loss = 0.38403, acc = 0.50000\n",
            "Epoch [4/5] Step [705/1610]: discriminator_loss = 0.56261, target_loss = 0.40414, acc = 0.50000\n",
            "Epoch [4/5] Step [706/1610]: discriminator_loss = 0.57374, target_loss = 0.38680, acc = 0.50000\n",
            "Epoch [4/5] Step [707/1610]: discriminator_loss = 0.56142, target_loss = 0.37932, acc = 0.50000\n",
            "Epoch [4/5] Step [708/1610]: discriminator_loss = 0.56629, target_loss = 0.39889, acc = 0.50000\n",
            "Epoch [4/5] Step [709/1610]: discriminator_loss = 0.57095, target_loss = 0.38508, acc = 0.50000\n",
            "Epoch [4/5] Step [710/1610]: discriminator_loss = 0.58878, target_loss = 0.39036, acc = 0.50000\n",
            "Epoch [4/5] Step [711/1610]: discriminator_loss = 0.56513, target_loss = 0.39235, acc = 0.50000\n",
            "Epoch [4/5] Step [712/1610]: discriminator_loss = 0.54591, target_loss = 0.41284, acc = 0.50000\n",
            "Epoch [4/5] Step [713/1610]: discriminator_loss = 0.54343, target_loss = 0.40009, acc = 0.50000\n",
            "Epoch [4/5] Step [714/1610]: discriminator_loss = 0.55104, target_loss = 0.39617, acc = 0.50000\n",
            "Epoch [4/5] Step [715/1610]: discriminator_loss = 0.53050, target_loss = 0.41952, acc = 0.50000\n",
            "Epoch [4/5] Step [716/1610]: discriminator_loss = 0.55705, target_loss = 0.39829, acc = 0.50000\n",
            "Epoch [4/5] Step [717/1610]: discriminator_loss = 0.54264, target_loss = 0.41714, acc = 0.50000\n",
            "Epoch [4/5] Step [718/1610]: discriminator_loss = 0.54597, target_loss = 0.39399, acc = 0.50000\n",
            "Epoch [4/5] Step [719/1610]: discriminator_loss = 0.53838, target_loss = 0.42025, acc = 0.50000\n",
            "Epoch [4/5] Step [720/1610]: discriminator_loss = 0.54572, target_loss = 0.41833, acc = 0.50000\n",
            "Epoch [4/5] Step [721/1610]: discriminator_loss = 0.60107, target_loss = 0.40162, acc = 0.50000\n",
            "Epoch [4/5] Step [722/1610]: discriminator_loss = 0.55754, target_loss = 0.41740, acc = 0.50000\n",
            "Epoch [4/5] Step [723/1610]: discriminator_loss = 0.53870, target_loss = 0.43230, acc = 0.50000\n",
            "Epoch [4/5] Step [724/1610]: discriminator_loss = 0.53521, target_loss = 0.42281, acc = 0.50000\n",
            "Epoch [4/5] Step [725/1610]: discriminator_loss = 0.54850, target_loss = 0.40037, acc = 0.50000\n",
            "Epoch [4/5] Step [726/1610]: discriminator_loss = 0.56898, target_loss = 0.37973, acc = 0.50000\n",
            "Epoch [4/5] Step [727/1610]: discriminator_loss = 0.60352, target_loss = 0.36587, acc = 0.50000\n",
            "Epoch [4/5] Step [728/1610]: discriminator_loss = 0.54566, target_loss = 0.42051, acc = 0.50000\n",
            "Epoch [4/5] Step [729/1610]: discriminator_loss = 0.54453, target_loss = 0.40041, acc = 0.50000\n",
            "Epoch [4/5] Step [730/1610]: discriminator_loss = 0.62073, target_loss = 0.38586, acc = 0.50000\n",
            "Epoch [4/5] Step [731/1610]: discriminator_loss = 0.55787, target_loss = 0.40090, acc = 0.50000\n",
            "Epoch [4/5] Step [732/1610]: discriminator_loss = 0.54133, target_loss = 0.41846, acc = 0.50000\n",
            "Epoch [4/5] Step [733/1610]: discriminator_loss = 0.55232, target_loss = 0.41608, acc = 0.50000\n",
            "Epoch [4/5] Step [734/1610]: discriminator_loss = 0.56886, target_loss = 0.42808, acc = 0.50000\n",
            "Epoch [4/5] Step [735/1610]: discriminator_loss = 0.54042, target_loss = 0.44116, acc = 0.50000\n",
            "Epoch [4/5] Step [736/1610]: discriminator_loss = 0.54206, target_loss = 0.41848, acc = 0.50000\n",
            "Epoch [4/5] Step [737/1610]: discriminator_loss = 0.57640, target_loss = 0.39054, acc = 0.50000\n",
            "Epoch [4/5] Step [738/1610]: discriminator_loss = 0.55165, target_loss = 0.40827, acc = 0.50000\n",
            "Epoch [4/5] Step [739/1610]: discriminator_loss = 0.55498, target_loss = 0.39621, acc = 0.50000\n",
            "Epoch [4/5] Step [740/1610]: discriminator_loss = 0.55071, target_loss = 0.41114, acc = 0.50000\n",
            "Epoch [4/5] Step [741/1610]: discriminator_loss = 0.54581, target_loss = 0.41143, acc = 0.50000\n",
            "Epoch [4/5] Step [742/1610]: discriminator_loss = 0.54739, target_loss = 0.41173, acc = 0.50000\n",
            "Epoch [4/5] Step [743/1610]: discriminator_loss = 0.56861, target_loss = 0.40674, acc = 0.50000\n",
            "Epoch [4/5] Step [744/1610]: discriminator_loss = 0.56202, target_loss = 0.40254, acc = 0.50000\n",
            "Epoch [4/5] Step [745/1610]: discriminator_loss = 0.56091, target_loss = 0.39668, acc = 0.50000\n",
            "Epoch [4/5] Step [746/1610]: discriminator_loss = 0.56045, target_loss = 0.39909, acc = 0.50000\n",
            "Epoch [4/5] Step [747/1610]: discriminator_loss = 0.56487, target_loss = 0.39015, acc = 0.50000\n",
            "Epoch [4/5] Step [748/1610]: discriminator_loss = 0.56016, target_loss = 0.39665, acc = 0.50000\n",
            "Epoch [4/5] Step [749/1610]: discriminator_loss = 0.57783, target_loss = 0.38793, acc = 0.50000\n",
            "Epoch [4/5] Step [750/1610]: discriminator_loss = 0.56557, target_loss = 0.40340, acc = 0.50000\n",
            "Epoch [4/5] Step [751/1610]: discriminator_loss = 0.56776, target_loss = 0.40263, acc = 0.50000\n",
            "Epoch [4/5] Step [752/1610]: discriminator_loss = 0.56411, target_loss = 0.39315, acc = 0.50000\n",
            "Epoch [4/5] Step [753/1610]: discriminator_loss = 0.58730, target_loss = 0.38449, acc = 0.50000\n",
            "Epoch [4/5] Step [754/1610]: discriminator_loss = 0.55988, target_loss = 0.40315, acc = 0.50000\n",
            "Epoch [4/5] Step [755/1610]: discriminator_loss = 0.58208, target_loss = 0.38762, acc = 0.50000\n",
            "Epoch [4/5] Step [756/1610]: discriminator_loss = 0.61134, target_loss = 0.39087, acc = 0.50000\n",
            "Epoch [4/5] Step [757/1610]: discriminator_loss = 0.55738, target_loss = 0.38740, acc = 0.50000\n",
            "Epoch [4/5] Step [758/1610]: discriminator_loss = 0.56641, target_loss = 0.38439, acc = 0.50000\n",
            "Epoch [4/5] Step [759/1610]: discriminator_loss = 0.57616, target_loss = 0.42025, acc = 0.50000\n",
            "Epoch [4/5] Step [760/1610]: discriminator_loss = 0.55790, target_loss = 0.39816, acc = 0.50000\n",
            "Epoch [4/5] Step [761/1610]: discriminator_loss = 0.55597, target_loss = 0.40860, acc = 0.50000\n",
            "Epoch [4/5] Step [762/1610]: discriminator_loss = 0.57356, target_loss = 0.38746, acc = 0.50000\n",
            "Epoch [4/5] Step [763/1610]: discriminator_loss = 0.56148, target_loss = 0.41129, acc = 0.50000\n",
            "Epoch [4/5] Step [764/1610]: discriminator_loss = 0.54343, target_loss = 0.40433, acc = 0.50000\n",
            "Epoch [4/5] Step [765/1610]: discriminator_loss = 0.55177, target_loss = 0.39063, acc = 0.50000\n",
            "Epoch [4/5] Step [766/1610]: discriminator_loss = 0.54320, target_loss = 0.40839, acc = 0.50000\n",
            "Epoch [4/5] Step [767/1610]: discriminator_loss = 0.55633, target_loss = 0.40275, acc = 0.50000\n",
            "Epoch [4/5] Step [768/1610]: discriminator_loss = 0.56672, target_loss = 0.39198, acc = 0.50000\n",
            "Epoch [4/5] Step [769/1610]: discriminator_loss = 0.54648, target_loss = 0.38075, acc = 0.50000\n",
            "Epoch [4/5] Step [770/1610]: discriminator_loss = 0.61893, target_loss = 0.38859, acc = 0.50000\n",
            "Epoch [4/5] Step [771/1610]: discriminator_loss = 0.59881, target_loss = 0.39622, acc = 0.50000\n",
            "Epoch [4/5] Step [772/1610]: discriminator_loss = 0.58376, target_loss = 0.37325, acc = 0.50000\n",
            "Epoch [4/5] Step [773/1610]: discriminator_loss = 0.55449, target_loss = 0.42822, acc = 0.50000\n",
            "Epoch [4/5] Step [774/1610]: discriminator_loss = 0.53014, target_loss = 0.41153, acc = 0.50000\n",
            "Epoch [4/5] Step [775/1610]: discriminator_loss = 0.56169, target_loss = 0.39472, acc = 0.50000\n",
            "Epoch [4/5] Step [776/1610]: discriminator_loss = 0.68053, target_loss = 0.40250, acc = 0.50000\n",
            "Epoch [4/5] Step [777/1610]: discriminator_loss = 0.61097, target_loss = 0.39511, acc = 0.50000\n",
            "Epoch [4/5] Step [778/1610]: discriminator_loss = 0.55265, target_loss = 0.41636, acc = 0.50000\n",
            "Epoch [4/5] Step [779/1610]: discriminator_loss = 0.55606, target_loss = 0.40391, acc = 0.50000\n",
            "Epoch [4/5] Step [780/1610]: discriminator_loss = 0.55503, target_loss = 0.41597, acc = 0.50000\n",
            "Epoch [4/5] Step [781/1610]: discriminator_loss = 0.54946, target_loss = 0.41219, acc = 0.50000\n",
            "Epoch [4/5] Step [782/1610]: discriminator_loss = 0.54836, target_loss = 0.40435, acc = 0.50000\n",
            "Epoch [4/5] Step [783/1610]: discriminator_loss = 0.53113, target_loss = 0.42504, acc = 0.50000\n",
            "Epoch [4/5] Step [784/1610]: discriminator_loss = 0.53275, target_loss = 0.41564, acc = 0.50000\n",
            "Epoch [4/5] Step [785/1610]: discriminator_loss = 0.56464, target_loss = 0.39719, acc = 0.50000\n",
            "Epoch [4/5] Step [786/1610]: discriminator_loss = 0.54465, target_loss = 0.40616, acc = 0.50000\n",
            "Epoch [4/5] Step [787/1610]: discriminator_loss = 0.55142, target_loss = 0.39811, acc = 0.50000\n",
            "Epoch [4/5] Step [788/1610]: discriminator_loss = 0.62658, target_loss = 0.40671, acc = 0.50000\n",
            "Epoch [4/5] Step [789/1610]: discriminator_loss = 0.56472, target_loss = 0.40973, acc = 0.50000\n",
            "Epoch [4/5] Step [790/1610]: discriminator_loss = 0.56697, target_loss = 0.40064, acc = 0.50000\n",
            "Epoch [4/5] Step [791/1610]: discriminator_loss = 0.57116, target_loss = 0.40750, acc = 0.50000\n",
            "Epoch [4/5] Step [792/1610]: discriminator_loss = 0.58174, target_loss = 0.42105, acc = 0.50000\n",
            "Epoch [4/5] Step [793/1610]: discriminator_loss = 0.55786, target_loss = 0.40546, acc = 0.50000\n",
            "Epoch [4/5] Step [794/1610]: discriminator_loss = 0.55900, target_loss = 0.40447, acc = 0.50000\n",
            "Epoch [4/5] Step [795/1610]: discriminator_loss = 0.55280, target_loss = 0.42765, acc = 0.50000\n",
            "Epoch [4/5] Step [796/1610]: discriminator_loss = 0.54770, target_loss = 0.40090, acc = 0.50000\n",
            "Epoch [4/5] Step [797/1610]: discriminator_loss = 0.56269, target_loss = 0.40311, acc = 0.50000\n",
            "Epoch [4/5] Step [798/1610]: discriminator_loss = 0.57043, target_loss = 0.39634, acc = 0.50000\n",
            "Epoch [4/5] Step [799/1610]: discriminator_loss = 0.56871, target_loss = 0.37476, acc = 0.50000\n",
            "Epoch [4/5] Step [800/1610]: discriminator_loss = 0.57860, target_loss = 0.40004, acc = 0.50000\n",
            "Epoch [4/5] Step [801/1610]: discriminator_loss = 0.58283, target_loss = 0.39185, acc = 0.50000\n",
            "Epoch [4/5] Step [802/1610]: discriminator_loss = 0.59988, target_loss = 0.36438, acc = 0.50000\n",
            "Epoch [4/5] Step [803/1610]: discriminator_loss = 0.56216, target_loss = 0.37871, acc = 0.50000\n",
            "Epoch [4/5] Step [804/1610]: discriminator_loss = 0.57817, target_loss = 0.38868, acc = 0.50000\n",
            "Epoch [4/5] Step [805/1610]: discriminator_loss = 0.59463, target_loss = 0.39615, acc = 0.50000\n",
            "Epoch [4/5] Step [806/1610]: discriminator_loss = 0.58589, target_loss = 0.38714, acc = 0.50000\n",
            "Epoch [4/5] Step [807/1610]: discriminator_loss = 0.58735, target_loss = 0.39616, acc = 0.50000\n",
            "Epoch [4/5] Step [808/1610]: discriminator_loss = 0.57895, target_loss = 0.40899, acc = 0.50000\n",
            "Epoch [4/5] Step [809/1610]: discriminator_loss = 0.60102, target_loss = 0.40478, acc = 0.50000\n",
            "Epoch [4/5] Step [810/1610]: discriminator_loss = 0.59132, target_loss = 0.39099, acc = 0.50000\n",
            "Epoch [4/5] Step [811/1610]: discriminator_loss = 0.54417, target_loss = 0.39928, acc = 0.50000\n",
            "Epoch [4/5] Step [812/1610]: discriminator_loss = 0.55486, target_loss = 0.39652, acc = 0.50000\n",
            "Epoch [4/5] Step [813/1610]: discriminator_loss = 0.56556, target_loss = 0.41319, acc = 0.50000\n",
            "Epoch [4/5] Step [814/1610]: discriminator_loss = 0.56236, target_loss = 0.40285, acc = 0.50000\n",
            "Epoch [4/5] Step [815/1610]: discriminator_loss = 0.55009, target_loss = 0.41366, acc = 0.50000\n",
            "Epoch [4/5] Step [816/1610]: discriminator_loss = 0.56922, target_loss = 0.41000, acc = 0.50000\n",
            "Epoch [4/5] Step [817/1610]: discriminator_loss = 0.55352, target_loss = 0.40801, acc = 0.50000\n",
            "Epoch [4/5] Step [818/1610]: discriminator_loss = 0.54059, target_loss = 0.41916, acc = 0.50000\n",
            "Epoch [4/5] Step [819/1610]: discriminator_loss = 0.54095, target_loss = 0.41685, acc = 0.50000\n",
            "Epoch [4/5] Step [820/1610]: discriminator_loss = 0.54696, target_loss = 0.40635, acc = 0.50000\n",
            "Epoch [4/5] Step [821/1610]: discriminator_loss = 0.56531, target_loss = 0.41662, acc = 0.50000\n",
            "Epoch [4/5] Step [822/1610]: discriminator_loss = 0.55510, target_loss = 0.42098, acc = 0.50000\n",
            "Epoch [4/5] Step [823/1610]: discriminator_loss = 0.54718, target_loss = 0.41034, acc = 0.50000\n",
            "Epoch [4/5] Step [824/1610]: discriminator_loss = 0.56638, target_loss = 0.41037, acc = 0.50000\n",
            "Epoch [4/5] Step [825/1610]: discriminator_loss = 0.54081, target_loss = 0.42738, acc = 0.50000\n",
            "Epoch [4/5] Step [826/1610]: discriminator_loss = 0.54775, target_loss = 0.42473, acc = 0.50000\n",
            "Epoch [4/5] Step [827/1610]: discriminator_loss = 0.55999, target_loss = 0.39299, acc = 0.50000\n",
            "Epoch [4/5] Step [828/1610]: discriminator_loss = 0.53485, target_loss = 0.39741, acc = 0.50000\n",
            "Epoch [4/5] Step [829/1610]: discriminator_loss = 0.55578, target_loss = 0.41373, acc = 0.50000\n",
            "Epoch [4/5] Step [830/1610]: discriminator_loss = 0.54550, target_loss = 0.38307, acc = 0.50000\n",
            "Epoch [4/5] Step [831/1610]: discriminator_loss = 0.56839, target_loss = 0.39543, acc = 0.50000\n",
            "Epoch [4/5] Step [832/1610]: discriminator_loss = 0.54197, target_loss = 0.39471, acc = 0.50000\n",
            "Epoch [4/5] Step [833/1610]: discriminator_loss = 0.59935, target_loss = 0.39153, acc = 0.50000\n",
            "Epoch [4/5] Step [834/1610]: discriminator_loss = 0.59313, target_loss = 0.37899, acc = 0.50000\n",
            "Epoch [4/5] Step [835/1610]: discriminator_loss = 0.55916, target_loss = 0.40746, acc = 0.50000\n",
            "Epoch [4/5] Step [836/1610]: discriminator_loss = 0.54695, target_loss = 0.39597, acc = 0.50000\n",
            "Epoch [4/5] Step [837/1610]: discriminator_loss = 0.58117, target_loss = 0.41022, acc = 0.50000\n",
            "Epoch [4/5] Step [838/1610]: discriminator_loss = 0.56224, target_loss = 0.39009, acc = 0.50000\n",
            "Epoch [4/5] Step [839/1610]: discriminator_loss = 0.56288, target_loss = 0.40130, acc = 0.50000\n",
            "Epoch [4/5] Step [840/1610]: discriminator_loss = 0.55313, target_loss = 0.38202, acc = 0.50000\n",
            "Epoch [4/5] Step [841/1610]: discriminator_loss = 0.56123, target_loss = 0.40694, acc = 0.50000\n",
            "Epoch [4/5] Step [842/1610]: discriminator_loss = 0.57441, target_loss = 0.39435, acc = 0.50000\n",
            "Epoch [4/5] Step [843/1610]: discriminator_loss = 0.58129, target_loss = 0.40365, acc = 0.50000\n",
            "Epoch [4/5] Step [844/1610]: discriminator_loss = 0.56508, target_loss = 0.39682, acc = 0.50000\n",
            "Epoch [4/5] Step [845/1610]: discriminator_loss = 0.55796, target_loss = 0.40191, acc = 0.50000\n",
            "Epoch [4/5] Step [846/1610]: discriminator_loss = 0.56907, target_loss = 0.39403, acc = 0.50000\n",
            "Epoch [4/5] Step [847/1610]: discriminator_loss = 0.55090, target_loss = 0.40493, acc = 0.50000\n",
            "Epoch [4/5] Step [848/1610]: discriminator_loss = 0.56327, target_loss = 0.40827, acc = 0.50000\n",
            "Epoch [4/5] Step [849/1610]: discriminator_loss = 0.55619, target_loss = 0.40487, acc = 0.50000\n",
            "Epoch [4/5] Step [850/1610]: discriminator_loss = 0.55977, target_loss = 0.42544, acc = 0.50000\n",
            "Epoch [4/5] Step [851/1610]: discriminator_loss = 0.56831, target_loss = 0.41966, acc = 0.50000\n",
            "Epoch [4/5] Step [852/1610]: discriminator_loss = 0.55797, target_loss = 0.40873, acc = 0.50000\n",
            "Epoch [4/5] Step [853/1610]: discriminator_loss = 0.56243, target_loss = 0.39825, acc = 0.50000\n",
            "Epoch [4/5] Step [854/1610]: discriminator_loss = 0.56428, target_loss = 0.39100, acc = 0.50000\n",
            "Epoch [4/5] Step [855/1610]: discriminator_loss = 0.56475, target_loss = 0.42707, acc = 0.50000\n",
            "Epoch [4/5] Step [856/1610]: discriminator_loss = 0.58749, target_loss = 0.41186, acc = 0.50000\n",
            "Epoch [4/5] Step [857/1610]: discriminator_loss = 0.57902, target_loss = 0.41895, acc = 0.50000\n",
            "Epoch [4/5] Step [858/1610]: discriminator_loss = 0.55091, target_loss = 0.39874, acc = 0.50000\n",
            "Epoch [4/5] Step [859/1610]: discriminator_loss = 0.55183, target_loss = 0.41613, acc = 0.50000\n",
            "Epoch [4/5] Step [860/1610]: discriminator_loss = 0.55407, target_loss = 0.39802, acc = 0.50000\n",
            "Epoch [4/5] Step [861/1610]: discriminator_loss = 0.55859, target_loss = 0.41618, acc = 0.50000\n",
            "Epoch [4/5] Step [862/1610]: discriminator_loss = 0.55334, target_loss = 0.41135, acc = 0.50000\n",
            "Epoch [4/5] Step [863/1610]: discriminator_loss = 0.54652, target_loss = 0.41435, acc = 0.50000\n",
            "Epoch [4/5] Step [864/1610]: discriminator_loss = 0.56118, target_loss = 0.39144, acc = 0.50000\n",
            "Epoch [4/5] Step [865/1610]: discriminator_loss = 0.56996, target_loss = 0.41543, acc = 0.50000\n",
            "Epoch [4/5] Step [866/1610]: discriminator_loss = 0.55557, target_loss = 0.40725, acc = 0.50000\n",
            "Epoch [4/5] Step [867/1610]: discriminator_loss = 0.57267, target_loss = 0.40505, acc = 0.50000\n",
            "Epoch [4/5] Step [868/1610]: discriminator_loss = 0.55624, target_loss = 0.43249, acc = 0.50000\n",
            "Epoch [4/5] Step [869/1610]: discriminator_loss = 0.56446, target_loss = 0.39843, acc = 0.50000\n",
            "Epoch [4/5] Step [870/1610]: discriminator_loss = 0.56573, target_loss = 0.40631, acc = 0.50000\n",
            "Epoch [4/5] Step [871/1610]: discriminator_loss = 0.56051, target_loss = 0.37365, acc = 0.50000\n",
            "Epoch [4/5] Step [872/1610]: discriminator_loss = 0.56288, target_loss = 0.40119, acc = 0.50000\n",
            "Epoch [4/5] Step [873/1610]: discriminator_loss = 0.57094, target_loss = 0.38749, acc = 0.50000\n",
            "Epoch [4/5] Step [874/1610]: discriminator_loss = 0.55727, target_loss = 0.39875, acc = 0.50000\n",
            "Epoch [4/5] Step [875/1610]: discriminator_loss = 0.54505, target_loss = 0.40423, acc = 0.50000\n",
            "Epoch [4/5] Step [876/1610]: discriminator_loss = 0.58307, target_loss = 0.40633, acc = 0.50000\n",
            "Epoch [4/5] Step [877/1610]: discriminator_loss = 0.57486, target_loss = 0.39466, acc = 0.50000\n",
            "Epoch [4/5] Step [878/1610]: discriminator_loss = 0.56044, target_loss = 0.41092, acc = 0.50000\n",
            "Epoch [4/5] Step [879/1610]: discriminator_loss = 0.53933, target_loss = 0.41080, acc = 0.50000\n",
            "Epoch [4/5] Step [880/1610]: discriminator_loss = 0.55462, target_loss = 0.39300, acc = 0.50000\n",
            "Epoch [4/5] Step [881/1610]: discriminator_loss = 0.57923, target_loss = 0.40871, acc = 0.50000\n",
            "Epoch [4/5] Step [882/1610]: discriminator_loss = 0.55816, target_loss = 0.42240, acc = 0.50000\n",
            "Epoch [4/5] Step [883/1610]: discriminator_loss = 0.54757, target_loss = 0.39840, acc = 0.50000\n",
            "Epoch [4/5] Step [884/1610]: discriminator_loss = 0.55554, target_loss = 0.36327, acc = 0.50000\n",
            "Epoch [4/5] Step [885/1610]: discriminator_loss = 0.54950, target_loss = 0.40288, acc = 0.50000\n",
            "Epoch [4/5] Step [886/1610]: discriminator_loss = 0.58228, target_loss = 0.40038, acc = 0.50000\n",
            "Epoch [4/5] Step [887/1610]: discriminator_loss = 0.57355, target_loss = 0.37905, acc = 0.50000\n",
            "Epoch [4/5] Step [888/1610]: discriminator_loss = 0.57589, target_loss = 0.40062, acc = 0.50000\n",
            "Epoch [4/5] Step [889/1610]: discriminator_loss = 0.54924, target_loss = 0.38756, acc = 0.50000\n",
            "Epoch [4/5] Step [890/1610]: discriminator_loss = 0.56093, target_loss = 0.41221, acc = 0.50000\n",
            "Epoch [4/5] Step [891/1610]: discriminator_loss = 0.55262, target_loss = 0.41176, acc = 0.50000\n",
            "Epoch [4/5] Step [892/1610]: discriminator_loss = 0.55472, target_loss = 0.41546, acc = 0.50000\n",
            "Epoch [4/5] Step [893/1610]: discriminator_loss = 0.58334, target_loss = 0.38196, acc = 0.50000\n",
            "Epoch [4/5] Step [894/1610]: discriminator_loss = 0.58353, target_loss = 0.41434, acc = 0.50000\n",
            "Epoch [4/5] Step [895/1610]: discriminator_loss = 0.54823, target_loss = 0.40186, acc = 0.50000\n",
            "Epoch [4/5] Step [896/1610]: discriminator_loss = 0.55226, target_loss = 0.38831, acc = 0.50000\n",
            "Epoch [4/5] Step [897/1610]: discriminator_loss = 0.57627, target_loss = 0.38766, acc = 0.50000\n",
            "Epoch [4/5] Step [898/1610]: discriminator_loss = 0.56016, target_loss = 0.38123, acc = 0.50000\n",
            "Epoch [4/5] Step [899/1610]: discriminator_loss = 0.58297, target_loss = 0.38320, acc = 0.50000\n",
            "Epoch [4/5] Step [900/1610]: discriminator_loss = 0.58694, target_loss = 0.40874, acc = 0.50000\n",
            "Epoch [4/5] Step [901/1610]: discriminator_loss = 0.56860, target_loss = 0.37417, acc = 0.50000\n",
            "Epoch [4/5] Step [902/1610]: discriminator_loss = 0.57033, target_loss = 0.40793, acc = 0.50000\n",
            "Epoch [4/5] Step [903/1610]: discriminator_loss = 0.54610, target_loss = 0.41498, acc = 0.50000\n",
            "Epoch [4/5] Step [904/1610]: discriminator_loss = 0.54659, target_loss = 0.39537, acc = 0.50000\n",
            "Epoch [4/5] Step [905/1610]: discriminator_loss = 0.56120, target_loss = 0.41624, acc = 0.50000\n",
            "Epoch [4/5] Step [906/1610]: discriminator_loss = 0.54283, target_loss = 0.42199, acc = 0.50000\n",
            "Epoch [4/5] Step [907/1610]: discriminator_loss = 0.52697, target_loss = 0.39847, acc = 0.50000\n",
            "Epoch [4/5] Step [908/1610]: discriminator_loss = 0.56056, target_loss = 0.40490, acc = 0.50000\n",
            "Epoch [4/5] Step [909/1610]: discriminator_loss = 0.56813, target_loss = 0.40925, acc = 0.50000\n",
            "Epoch [4/5] Step [910/1610]: discriminator_loss = 0.52641, target_loss = 0.40294, acc = 0.50000\n",
            "Epoch [4/5] Step [911/1610]: discriminator_loss = 0.53291, target_loss = 0.39141, acc = 0.50000\n",
            "Epoch [4/5] Step [912/1610]: discriminator_loss = 0.54595, target_loss = 0.41823, acc = 0.53125\n",
            "Epoch [4/5] Step [913/1610]: discriminator_loss = 0.57558, target_loss = 0.39055, acc = 0.50000\n",
            "Epoch [4/5] Step [914/1610]: discriminator_loss = 0.52929, target_loss = 0.38116, acc = 0.50000\n",
            "Epoch [4/5] Step [915/1610]: discriminator_loss = 0.52912, target_loss = 0.42499, acc = 0.50000\n",
            "Epoch [4/5] Step [916/1610]: discriminator_loss = 0.54555, target_loss = 0.39151, acc = 0.50000\n",
            "Epoch [4/5] Step [917/1610]: discriminator_loss = 0.55309, target_loss = 0.40508, acc = 0.50000\n",
            "Epoch [4/5] Step [918/1610]: discriminator_loss = 0.61401, target_loss = 0.40484, acc = 0.50000\n",
            "Epoch [4/5] Step [919/1610]: discriminator_loss = 0.54864, target_loss = 0.39714, acc = 0.50000\n",
            "Epoch [4/5] Step [920/1610]: discriminator_loss = 0.57716, target_loss = 0.40448, acc = 0.50000\n",
            "Epoch [4/5] Step [921/1610]: discriminator_loss = 0.54806, target_loss = 0.42144, acc = 0.50000\n",
            "Epoch [4/5] Step [922/1610]: discriminator_loss = 0.55415, target_loss = 0.38565, acc = 0.50000\n",
            "Epoch [4/5] Step [923/1610]: discriminator_loss = 0.54703, target_loss = 0.41547, acc = 0.50000\n",
            "Epoch [4/5] Step [924/1610]: discriminator_loss = 0.56485, target_loss = 0.38124, acc = 0.50000\n",
            "Epoch [4/5] Step [925/1610]: discriminator_loss = 0.54940, target_loss = 0.39909, acc = 0.50000\n",
            "Epoch [4/5] Step [926/1610]: discriminator_loss = 0.56873, target_loss = 0.40450, acc = 0.50000\n",
            "Epoch [4/5] Step [927/1610]: discriminator_loss = 0.55767, target_loss = 0.40950, acc = 0.50000\n",
            "Epoch [4/5] Step [928/1610]: discriminator_loss = 0.56174, target_loss = 0.42047, acc = 0.50000\n",
            "Epoch [4/5] Step [929/1610]: discriminator_loss = 0.54568, target_loss = 0.38802, acc = 0.50000\n",
            "Epoch [4/5] Step [930/1610]: discriminator_loss = 0.57522, target_loss = 0.40526, acc = 0.50000\n",
            "Epoch [4/5] Step [931/1610]: discriminator_loss = 0.56718, target_loss = 0.39470, acc = 0.50000\n",
            "Epoch [4/5] Step [932/1610]: discriminator_loss = 0.58881, target_loss = 0.39975, acc = 0.50000\n",
            "Epoch [4/5] Step [933/1610]: discriminator_loss = 0.57490, target_loss = 0.39466, acc = 0.50000\n",
            "Epoch [4/5] Step [934/1610]: discriminator_loss = 0.56877, target_loss = 0.40608, acc = 0.50000\n",
            "Epoch [4/5] Step [935/1610]: discriminator_loss = 0.58921, target_loss = 0.40984, acc = 0.50000\n",
            "Epoch [4/5] Step [936/1610]: discriminator_loss = 0.55689, target_loss = 0.39173, acc = 0.50000\n",
            "Epoch [4/5] Step [937/1610]: discriminator_loss = 0.54899, target_loss = 0.39801, acc = 0.50000\n",
            "Epoch [4/5] Step [938/1610]: discriminator_loss = 0.57390, target_loss = 0.40147, acc = 0.50000\n",
            "Epoch [4/5] Step [939/1610]: discriminator_loss = 0.55601, target_loss = 0.40805, acc = 0.50000\n",
            "Epoch [4/5] Step [940/1610]: discriminator_loss = 0.55518, target_loss = 0.42054, acc = 0.50000\n",
            "Epoch [4/5] Step [941/1610]: discriminator_loss = 0.56157, target_loss = 0.39509, acc = 0.50000\n",
            "Epoch [4/5] Step [942/1610]: discriminator_loss = 0.56886, target_loss = 0.40857, acc = 0.50000\n",
            "Epoch [4/5] Step [943/1610]: discriminator_loss = 0.57654, target_loss = 0.40164, acc = 0.50000\n",
            "Epoch [4/5] Step [944/1610]: discriminator_loss = 0.57644, target_loss = 0.40449, acc = 0.50000\n",
            "Epoch [4/5] Step [945/1610]: discriminator_loss = 0.56415, target_loss = 0.39033, acc = 0.50000\n",
            "Epoch [4/5] Step [946/1610]: discriminator_loss = 0.58630, target_loss = 0.39108, acc = 0.50000\n",
            "Epoch [4/5] Step [947/1610]: discriminator_loss = 0.54676, target_loss = 0.41223, acc = 0.50000\n",
            "Epoch [4/5] Step [948/1610]: discriminator_loss = 0.55777, target_loss = 0.41791, acc = 0.50000\n",
            "Epoch [4/5] Step [949/1610]: discriminator_loss = 0.57638, target_loss = 0.39567, acc = 0.50000\n",
            "Epoch [4/5] Step [950/1610]: discriminator_loss = 0.55525, target_loss = 0.41337, acc = 0.50000\n",
            "Epoch [4/5] Step [951/1610]: discriminator_loss = 0.56203, target_loss = 0.40316, acc = 0.50000\n",
            "Epoch [4/5] Step [952/1610]: discriminator_loss = 0.55191, target_loss = 0.42942, acc = 0.50000\n",
            "Epoch [4/5] Step [953/1610]: discriminator_loss = 0.56389, target_loss = 0.40436, acc = 0.50000\n",
            "Epoch [4/5] Step [954/1610]: discriminator_loss = 0.63706, target_loss = 0.40349, acc = 0.50000\n",
            "Epoch [4/5] Step [955/1610]: discriminator_loss = 0.59252, target_loss = 0.39855, acc = 0.50000\n",
            "Epoch [4/5] Step [956/1610]: discriminator_loss = 0.55702, target_loss = 0.41127, acc = 0.50000\n",
            "Epoch [4/5] Step [957/1610]: discriminator_loss = 0.53882, target_loss = 0.43001, acc = 0.50000\n",
            "Epoch [4/5] Step [958/1610]: discriminator_loss = 0.55216, target_loss = 0.41070, acc = 0.50000\n",
            "Epoch [4/5] Step [959/1610]: discriminator_loss = 0.53839, target_loss = 0.40791, acc = 0.50000\n",
            "Epoch [4/5] Step [960/1610]: discriminator_loss = 0.57041, target_loss = 0.38854, acc = 0.50000\n",
            "Epoch [4/5] Step [961/1610]: discriminator_loss = 0.55846, target_loss = 0.40879, acc = 0.50000\n",
            "Epoch [4/5] Step [962/1610]: discriminator_loss = 0.56075, target_loss = 0.40223, acc = 0.50000\n",
            "Epoch [4/5] Step [963/1610]: discriminator_loss = 0.55936, target_loss = 0.40413, acc = 0.50000\n",
            "Epoch [4/5] Step [964/1610]: discriminator_loss = 0.55295, target_loss = 0.39794, acc = 0.50000\n",
            "Epoch [4/5] Step [965/1610]: discriminator_loss = 0.57776, target_loss = 0.39493, acc = 0.50000\n",
            "Epoch [4/5] Step [966/1610]: discriminator_loss = 0.55092, target_loss = 0.38128, acc = 0.50000\n",
            "Epoch [4/5] Step [967/1610]: discriminator_loss = 0.61506, target_loss = 0.38517, acc = 0.50000\n",
            "Epoch [4/5] Step [968/1610]: discriminator_loss = 0.55575, target_loss = 0.37972, acc = 0.50000\n",
            "Epoch [4/5] Step [969/1610]: discriminator_loss = 0.58419, target_loss = 0.40276, acc = 0.50000\n",
            "Epoch [4/5] Step [970/1610]: discriminator_loss = 0.56949, target_loss = 0.41161, acc = 0.50000\n",
            "Epoch [4/5] Step [971/1610]: discriminator_loss = 0.55013, target_loss = 0.40176, acc = 0.50000\n",
            "Epoch [4/5] Step [972/1610]: discriminator_loss = 0.57565, target_loss = 0.40379, acc = 0.50000\n",
            "Epoch [4/5] Step [973/1610]: discriminator_loss = 0.57596, target_loss = 0.41491, acc = 0.50000\n",
            "Epoch [4/5] Step [974/1610]: discriminator_loss = 0.58288, target_loss = 0.40269, acc = 0.50000\n",
            "Epoch [4/5] Step [975/1610]: discriminator_loss = 0.55680, target_loss = 0.40495, acc = 0.50000\n",
            "Epoch [4/5] Step [976/1610]: discriminator_loss = 0.56075, target_loss = 0.39268, acc = 0.50000\n",
            "Epoch [4/5] Step [977/1610]: discriminator_loss = 0.59379, target_loss = 0.39651, acc = 0.50000\n",
            "Epoch [4/5] Step [978/1610]: discriminator_loss = 0.61225, target_loss = 0.38424, acc = 0.50000\n",
            "Epoch [4/5] Step [979/1610]: discriminator_loss = 0.59764, target_loss = 0.38542, acc = 0.50000\n",
            "Epoch [4/5] Step [980/1610]: discriminator_loss = 0.55382, target_loss = 0.39984, acc = 0.50000\n",
            "Epoch [4/5] Step [981/1610]: discriminator_loss = 0.56115, target_loss = 0.41210, acc = 0.50000\n",
            "Epoch [4/5] Step [982/1610]: discriminator_loss = 0.59331, target_loss = 0.40864, acc = 0.50000\n",
            "Epoch [4/5] Step [983/1610]: discriminator_loss = 0.57304, target_loss = 0.40949, acc = 0.50000\n",
            "Epoch [4/5] Step [984/1610]: discriminator_loss = 0.58733, target_loss = 0.40193, acc = 0.50000\n",
            "Epoch [4/5] Step [985/1610]: discriminator_loss = 0.57398, target_loss = 0.40056, acc = 0.50000\n",
            "Epoch [4/5] Step [986/1610]: discriminator_loss = 0.55726, target_loss = 0.41602, acc = 0.50000\n",
            "Epoch [4/5] Step [987/1610]: discriminator_loss = 0.53663, target_loss = 0.41679, acc = 0.50000\n",
            "Epoch [4/5] Step [988/1610]: discriminator_loss = 0.54823, target_loss = 0.41139, acc = 0.50000\n",
            "Epoch [4/5] Step [989/1610]: discriminator_loss = 0.57594, target_loss = 0.41816, acc = 0.50000\n",
            "Epoch [4/5] Step [990/1610]: discriminator_loss = 0.55235, target_loss = 0.39986, acc = 0.50000\n",
            "Epoch [4/5] Step [991/1610]: discriminator_loss = 0.55653, target_loss = 0.40649, acc = 0.50000\n",
            "Epoch [4/5] Step [992/1610]: discriminator_loss = 0.55939, target_loss = 0.41189, acc = 0.50000\n",
            "Epoch [4/5] Step [993/1610]: discriminator_loss = 0.56518, target_loss = 0.39682, acc = 0.50000\n",
            "Epoch [4/5] Step [994/1610]: discriminator_loss = 0.55174, target_loss = 0.39760, acc = 0.50000\n",
            "Epoch [4/5] Step [995/1610]: discriminator_loss = 0.57983, target_loss = 0.39749, acc = 0.50000\n",
            "Epoch [4/5] Step [996/1610]: discriminator_loss = 0.57337, target_loss = 0.39261, acc = 0.50000\n",
            "Epoch [4/5] Step [997/1610]: discriminator_loss = 0.58259, target_loss = 0.39408, acc = 0.50000\n",
            "Epoch [4/5] Step [998/1610]: discriminator_loss = 0.58102, target_loss = 0.38047, acc = 0.50000\n",
            "Epoch [4/5] Step [999/1610]: discriminator_loss = 0.57259, target_loss = 0.38740, acc = 0.50000\n",
            "Epoch [4/5] Step [1000/1610]: discriminator_loss = 0.64064, target_loss = 0.38625, acc = 0.50000\n",
            "Epoch [4/5] Step [1001/1610]: discriminator_loss = 0.57180, target_loss = 0.39425, acc = 0.50000\n",
            "Epoch [4/5] Step [1002/1610]: discriminator_loss = 0.60035, target_loss = 0.39970, acc = 0.50000\n",
            "Epoch [4/5] Step [1003/1610]: discriminator_loss = 0.57040, target_loss = 0.39941, acc = 0.50000\n",
            "Epoch [4/5] Step [1004/1610]: discriminator_loss = 0.55471, target_loss = 0.41701, acc = 0.50000\n",
            "Epoch [4/5] Step [1005/1610]: discriminator_loss = 0.56656, target_loss = 0.39509, acc = 0.50000\n",
            "Epoch [4/5] Step [1006/1610]: discriminator_loss = 0.55700, target_loss = 0.41535, acc = 0.50000\n",
            "Epoch [4/5] Step [1007/1610]: discriminator_loss = 0.54334, target_loss = 0.41591, acc = 0.50000\n",
            "Epoch [4/5] Step [1008/1610]: discriminator_loss = 0.57956, target_loss = 0.41543, acc = 0.50000\n",
            "Epoch [4/5] Step [1009/1610]: discriminator_loss = 0.55423, target_loss = 0.41159, acc = 0.50000\n",
            "Epoch [4/5] Step [1010/1610]: discriminator_loss = 0.55296, target_loss = 0.43040, acc = 0.50000\n",
            "Epoch [4/5] Step [1011/1610]: discriminator_loss = 0.56080, target_loss = 0.40445, acc = 0.50000\n",
            "Epoch [4/5] Step [1012/1610]: discriminator_loss = 0.56058, target_loss = 0.40712, acc = 0.50000\n",
            "Epoch [4/5] Step [1013/1610]: discriminator_loss = 0.57123, target_loss = 0.40703, acc = 0.50000\n",
            "Epoch [4/5] Step [1014/1610]: discriminator_loss = 0.56968, target_loss = 0.39744, acc = 0.50000\n",
            "Epoch [4/5] Step [1015/1610]: discriminator_loss = 0.55224, target_loss = 0.39591, acc = 0.50000\n",
            "Epoch [4/5] Step [1016/1610]: discriminator_loss = 0.55591, target_loss = 0.41168, acc = 0.50000\n",
            "Epoch [4/5] Step [1017/1610]: discriminator_loss = 0.56403, target_loss = 0.39975, acc = 0.50000\n",
            "Epoch [4/5] Step [1018/1610]: discriminator_loss = 0.56845, target_loss = 0.40312, acc = 0.50000\n",
            "Epoch [4/5] Step [1019/1610]: discriminator_loss = 0.57573, target_loss = 0.40363, acc = 0.50000\n",
            "Epoch [4/5] Step [1020/1610]: discriminator_loss = 0.57663, target_loss = 0.39263, acc = 0.50000\n",
            "Epoch [4/5] Step [1021/1610]: discriminator_loss = 0.58390, target_loss = 0.38754, acc = 0.50000\n",
            "Epoch [4/5] Step [1022/1610]: discriminator_loss = 0.56156, target_loss = 0.38505, acc = 0.50000\n",
            "Epoch [4/5] Step [1023/1610]: discriminator_loss = 0.59005, target_loss = 0.38589, acc = 0.50000\n",
            "Epoch [4/5] Step [1024/1610]: discriminator_loss = 0.54788, target_loss = 0.40245, acc = 0.50000\n",
            "Epoch [4/5] Step [1025/1610]: discriminator_loss = 0.64703, target_loss = 0.38097, acc = 0.50000\n",
            "Epoch [4/5] Step [1026/1610]: discriminator_loss = 0.57173, target_loss = 0.39475, acc = 0.50000\n",
            "Epoch [4/5] Step [1027/1610]: discriminator_loss = 0.57061, target_loss = 0.39765, acc = 0.50000\n",
            "Epoch [4/5] Step [1028/1610]: discriminator_loss = 0.55192, target_loss = 0.39906, acc = 0.50000\n",
            "Epoch [4/5] Step [1029/1610]: discriminator_loss = 0.56790, target_loss = 0.40239, acc = 0.50000\n",
            "Epoch [4/5] Step [1030/1610]: discriminator_loss = 0.57322, target_loss = 0.38365, acc = 0.50000\n",
            "Epoch [4/5] Step [1031/1610]: discriminator_loss = 0.57671, target_loss = 0.40158, acc = 0.50000\n",
            "Epoch [4/5] Step [1032/1610]: discriminator_loss = 0.56896, target_loss = 0.40443, acc = 0.50000\n",
            "Epoch [4/5] Step [1033/1610]: discriminator_loss = 0.56601, target_loss = 0.38954, acc = 0.50000\n",
            "Epoch [4/5] Step [1034/1610]: discriminator_loss = 0.57649, target_loss = 0.37786, acc = 0.50000\n",
            "Epoch [4/5] Step [1035/1610]: discriminator_loss = 0.57773, target_loss = 0.40778, acc = 0.50000\n",
            "Epoch [4/5] Step [1036/1610]: discriminator_loss = 0.53878, target_loss = 0.41170, acc = 0.50000\n",
            "Epoch [4/5] Step [1037/1610]: discriminator_loss = 0.56890, target_loss = 0.41216, acc = 0.50000\n",
            "Epoch [4/5] Step [1038/1610]: discriminator_loss = 0.56520, target_loss = 0.41246, acc = 0.50000\n",
            "Epoch [4/5] Step [1039/1610]: discriminator_loss = 0.54263, target_loss = 0.41902, acc = 0.50000\n",
            "Epoch [4/5] Step [1040/1610]: discriminator_loss = 0.56158, target_loss = 0.40073, acc = 0.50000\n",
            "Epoch [4/5] Step [1041/1610]: discriminator_loss = 0.53299, target_loss = 0.40798, acc = 0.50000\n",
            "Epoch [4/5] Step [1042/1610]: discriminator_loss = 0.53940, target_loss = 0.40635, acc = 0.50000\n",
            "Epoch [4/5] Step [1043/1610]: discriminator_loss = 0.55689, target_loss = 0.40167, acc = 0.50000\n",
            "Epoch [4/5] Step [1044/1610]: discriminator_loss = 0.55337, target_loss = 0.41724, acc = 0.50000\n",
            "Epoch [4/5] Step [1045/1610]: discriminator_loss = 0.55386, target_loss = 0.41236, acc = 0.50000\n",
            "Epoch [4/5] Step [1046/1610]: discriminator_loss = 0.55848, target_loss = 0.39775, acc = 0.50000\n",
            "Epoch [4/5] Step [1047/1610]: discriminator_loss = 0.55246, target_loss = 0.37523, acc = 0.50000\n",
            "Epoch [4/5] Step [1048/1610]: discriminator_loss = 0.56141, target_loss = 0.39989, acc = 0.50000\n",
            "Epoch [4/5] Step [1049/1610]: discriminator_loss = 0.55650, target_loss = 0.40579, acc = 0.50000\n",
            "Epoch [4/5] Step [1050/1610]: discriminator_loss = 0.56455, target_loss = 0.39740, acc = 0.50000\n",
            "Epoch [4/5] Step [1051/1610]: discriminator_loss = 0.58190, target_loss = 0.39747, acc = 0.50000\n",
            "Epoch [4/5] Step [1052/1610]: discriminator_loss = 0.54562, target_loss = 0.41454, acc = 0.50000\n",
            "Epoch [4/5] Step [1053/1610]: discriminator_loss = 0.61612, target_loss = 0.39861, acc = 0.50000\n",
            "Epoch [4/5] Step [1054/1610]: discriminator_loss = 0.53353, target_loss = 0.42148, acc = 0.50000\n",
            "Epoch [4/5] Step [1055/1610]: discriminator_loss = 0.54928, target_loss = 0.41120, acc = 0.50000\n",
            "Epoch [4/5] Step [1056/1610]: discriminator_loss = 0.55500, target_loss = 0.40910, acc = 0.50000\n",
            "Epoch [4/5] Step [1057/1610]: discriminator_loss = 0.53819, target_loss = 0.41704, acc = 0.50000\n",
            "Epoch [4/5] Step [1058/1610]: discriminator_loss = 0.54477, target_loss = 0.41220, acc = 0.50000\n",
            "Epoch [4/5] Step [1059/1610]: discriminator_loss = 0.56396, target_loss = 0.41853, acc = 0.50000\n",
            "Epoch [4/5] Step [1060/1610]: discriminator_loss = 0.55811, target_loss = 0.41591, acc = 0.50000\n",
            "Epoch [4/5] Step [1061/1610]: discriminator_loss = 0.54642, target_loss = 0.40818, acc = 0.50000\n",
            "Epoch [4/5] Step [1062/1610]: discriminator_loss = 0.54711, target_loss = 0.41552, acc = 0.50000\n",
            "Epoch [4/5] Step [1063/1610]: discriminator_loss = 0.54843, target_loss = 0.42174, acc = 0.50000\n",
            "Epoch [4/5] Step [1064/1610]: discriminator_loss = 0.53150, target_loss = 0.41145, acc = 0.50000\n",
            "Epoch [4/5] Step [1065/1610]: discriminator_loss = 0.55293, target_loss = 0.41301, acc = 0.50000\n",
            "Epoch [4/5] Step [1066/1610]: discriminator_loss = 0.55349, target_loss = 0.41043, acc = 0.50000\n",
            "Epoch [4/5] Step [1067/1610]: discriminator_loss = 0.55430, target_loss = 0.40970, acc = 0.50000\n",
            "Epoch [4/5] Step [1068/1610]: discriminator_loss = 0.55381, target_loss = 0.37248, acc = 0.50000\n",
            "Epoch [4/5] Step [1069/1610]: discriminator_loss = 0.56528, target_loss = 0.41090, acc = 0.50000\n",
            "Epoch [4/5] Step [1070/1610]: discriminator_loss = 0.57464, target_loss = 0.41958, acc = 0.50000\n",
            "Epoch [4/5] Step [1071/1610]: discriminator_loss = 0.56574, target_loss = 0.39385, acc = 0.50000\n",
            "Epoch [4/5] Step [1072/1610]: discriminator_loss = 0.56404, target_loss = 0.41226, acc = 0.50000\n",
            "Epoch [4/5] Step [1073/1610]: discriminator_loss = 0.53323, target_loss = 0.40312, acc = 0.50000\n",
            "Epoch [4/5] Step [1074/1610]: discriminator_loss = 0.55528, target_loss = 0.39001, acc = 0.50000\n",
            "Epoch [4/5] Step [1075/1610]: discriminator_loss = 0.55640, target_loss = 0.38624, acc = 0.50000\n",
            "Epoch [4/5] Step [1076/1610]: discriminator_loss = 0.56681, target_loss = 0.39945, acc = 0.50000\n",
            "Epoch [4/5] Step [1077/1610]: discriminator_loss = 0.59217, target_loss = 0.40018, acc = 0.50000\n",
            "Epoch [4/5] Step [1078/1610]: discriminator_loss = 0.55646, target_loss = 0.40580, acc = 0.50000\n",
            "Epoch [4/5] Step [1079/1610]: discriminator_loss = 0.56336, target_loss = 0.40705, acc = 0.50000\n",
            "Epoch [4/5] Step [1080/1610]: discriminator_loss = 0.57115, target_loss = 0.40872, acc = 0.50000\n",
            "Epoch [4/5] Step [1081/1610]: discriminator_loss = 0.56177, target_loss = 0.39275, acc = 0.50000\n",
            "Epoch [4/5] Step [1082/1610]: discriminator_loss = 0.68871, target_loss = 0.39433, acc = 0.50000\n",
            "Epoch [4/5] Step [1083/1610]: discriminator_loss = 0.55384, target_loss = 0.40293, acc = 0.50000\n",
            "Epoch [4/5] Step [1084/1610]: discriminator_loss = 0.56425, target_loss = 0.41858, acc = 0.50000\n",
            "Epoch [4/5] Step [1085/1610]: discriminator_loss = 0.54462, target_loss = 0.41526, acc = 0.50000\n",
            "Epoch [4/5] Step [1086/1610]: discriminator_loss = 0.56470, target_loss = 0.40314, acc = 0.50000\n",
            "Epoch [4/5] Step [1087/1610]: discriminator_loss = 0.55561, target_loss = 0.39608, acc = 0.50000\n",
            "Epoch [4/5] Step [1088/1610]: discriminator_loss = 0.55747, target_loss = 0.41072, acc = 0.50000\n",
            "Epoch [4/5] Step [1089/1610]: discriminator_loss = 0.57668, target_loss = 0.39642, acc = 0.50000\n",
            "Epoch [4/5] Step [1090/1610]: discriminator_loss = 0.57411, target_loss = 0.40015, acc = 0.50000\n",
            "Epoch [4/5] Step [1091/1610]: discriminator_loss = 0.57373, target_loss = 0.39867, acc = 0.50000\n",
            "Epoch [4/5] Step [1092/1610]: discriminator_loss = 0.56200, target_loss = 0.39860, acc = 0.50000\n",
            "Epoch [4/5] Step [1093/1610]: discriminator_loss = 0.59091, target_loss = 0.38765, acc = 0.50000\n",
            "Epoch [4/5] Step [1094/1610]: discriminator_loss = 0.57274, target_loss = 0.39108, acc = 0.50000\n",
            "Epoch [4/5] Step [1095/1610]: discriminator_loss = 0.56689, target_loss = 0.39252, acc = 0.50000\n",
            "Epoch [4/5] Step [1096/1610]: discriminator_loss = 0.58206, target_loss = 0.38830, acc = 0.50000\n",
            "Epoch [4/5] Step [1097/1610]: discriminator_loss = 0.56355, target_loss = 0.39403, acc = 0.50000\n",
            "Epoch [4/5] Step [1098/1610]: discriminator_loss = 0.56573, target_loss = 0.40919, acc = 0.50000\n",
            "Epoch [4/5] Step [1099/1610]: discriminator_loss = 0.56235, target_loss = 0.41791, acc = 0.50000\n",
            "Epoch [4/5] Step [1100/1610]: discriminator_loss = 0.55332, target_loss = 0.39975, acc = 0.50000\n",
            "Epoch [4/5] Step [1101/1610]: discriminator_loss = 0.55237, target_loss = 0.38631, acc = 0.50000\n",
            "Epoch [4/5] Step [1102/1610]: discriminator_loss = 0.56949, target_loss = 0.39497, acc = 0.50000\n",
            "Epoch [4/5] Step [1103/1610]: discriminator_loss = 0.56633, target_loss = 0.40332, acc = 0.50000\n",
            "Epoch [4/5] Step [1104/1610]: discriminator_loss = 0.54577, target_loss = 0.40811, acc = 0.50000\n",
            "Epoch [4/5] Step [1105/1610]: discriminator_loss = 0.60791, target_loss = 0.37538, acc = 0.50000\n",
            "Epoch [4/5] Step [1106/1610]: discriminator_loss = 0.59216, target_loss = 0.40859, acc = 0.50000\n",
            "Epoch [4/5] Step [1107/1610]: discriminator_loss = 0.58506, target_loss = 0.39897, acc = 0.50000\n",
            "Epoch [4/5] Step [1108/1610]: discriminator_loss = 0.57492, target_loss = 0.39098, acc = 0.50000\n",
            "Epoch [4/5] Step [1109/1610]: discriminator_loss = 0.57132, target_loss = 0.41391, acc = 0.50000\n",
            "Epoch [4/5] Step [1110/1610]: discriminator_loss = 0.56885, target_loss = 0.40237, acc = 0.50000\n",
            "Epoch [4/5] Step [1111/1610]: discriminator_loss = 0.55217, target_loss = 0.40440, acc = 0.50000\n",
            "Epoch [4/5] Step [1112/1610]: discriminator_loss = 0.57004, target_loss = 0.40670, acc = 0.50000\n",
            "Epoch [4/5] Step [1113/1610]: discriminator_loss = 0.55364, target_loss = 0.41221, acc = 0.50000\n",
            "Epoch [4/5] Step [1114/1610]: discriminator_loss = 0.57902, target_loss = 0.41060, acc = 0.50000\n",
            "Epoch [4/5] Step [1115/1610]: discriminator_loss = 0.56491, target_loss = 0.40251, acc = 0.50000\n",
            "Epoch [4/5] Step [1116/1610]: discriminator_loss = 0.54209, target_loss = 0.40918, acc = 0.50000\n",
            "Epoch [4/5] Step [1117/1610]: discriminator_loss = 0.54690, target_loss = 0.42686, acc = 0.50000\n",
            "Epoch [4/5] Step [1118/1610]: discriminator_loss = 0.55698, target_loss = 0.41112, acc = 0.50000\n",
            "Epoch [4/5] Step [1119/1610]: discriminator_loss = 0.54930, target_loss = 0.41544, acc = 0.50000\n",
            "Epoch [4/5] Step [1120/1610]: discriminator_loss = 0.55929, target_loss = 0.38915, acc = 0.50000\n",
            "Epoch [4/5] Step [1121/1610]: discriminator_loss = 0.55845, target_loss = 0.39988, acc = 0.50000\n",
            "Epoch [4/5] Step [1122/1610]: discriminator_loss = 0.55096, target_loss = 0.39552, acc = 0.50000\n",
            "Epoch [4/5] Step [1123/1610]: discriminator_loss = 0.57181, target_loss = 0.40169, acc = 0.50000\n",
            "Epoch [4/5] Step [1124/1610]: discriminator_loss = 0.58968, target_loss = 0.41295, acc = 0.50000\n",
            "Epoch [4/5] Step [1125/1610]: discriminator_loss = 0.54447, target_loss = 0.41378, acc = 0.50000\n",
            "Epoch [4/5] Step [1126/1610]: discriminator_loss = 0.54650, target_loss = 0.40054, acc = 0.53125\n",
            "Epoch [4/5] Step [1127/1610]: discriminator_loss = 0.54745, target_loss = 0.40748, acc = 0.50000\n",
            "Epoch [4/5] Step [1128/1610]: discriminator_loss = 0.57153, target_loss = 0.39876, acc = 0.50000\n",
            "Epoch [4/5] Step [1129/1610]: discriminator_loss = 0.57365, target_loss = 0.40942, acc = 0.50000\n",
            "Epoch [4/5] Step [1130/1610]: discriminator_loss = 0.56120, target_loss = 0.39008, acc = 0.50000\n",
            "Epoch [4/5] Step [1131/1610]: discriminator_loss = 0.56104, target_loss = 0.40411, acc = 0.50000\n",
            "Epoch [4/5] Step [1132/1610]: discriminator_loss = 0.53940, target_loss = 0.41598, acc = 0.50000\n",
            "Epoch [4/5] Step [1133/1610]: discriminator_loss = 0.57812, target_loss = 0.40802, acc = 0.50000\n",
            "Epoch [4/5] Step [1134/1610]: discriminator_loss = 0.57157, target_loss = 0.40060, acc = 0.50000\n",
            "Epoch [4/5] Step [1135/1610]: discriminator_loss = 0.55742, target_loss = 0.39124, acc = 0.50000\n",
            "Epoch [4/5] Step [1136/1610]: discriminator_loss = 0.57711, target_loss = 0.40812, acc = 0.50000\n",
            "Epoch [4/5] Step [1137/1610]: discriminator_loss = 0.55953, target_loss = 0.42089, acc = 0.50000\n",
            "Epoch [4/5] Step [1138/1610]: discriminator_loss = 0.59585, target_loss = 0.39133, acc = 0.50000\n",
            "Epoch [4/5] Step [1139/1610]: discriminator_loss = 0.55458, target_loss = 0.40058, acc = 0.50000\n",
            "Epoch [4/5] Step [1140/1610]: discriminator_loss = 0.57644, target_loss = 0.40683, acc = 0.50000\n",
            "Epoch [4/5] Step [1141/1610]: discriminator_loss = 0.56140, target_loss = 0.39813, acc = 0.50000\n",
            "Epoch [4/5] Step [1142/1610]: discriminator_loss = 0.56739, target_loss = 0.38873, acc = 0.50000\n",
            "Epoch [4/5] Step [1143/1610]: discriminator_loss = 0.55494, target_loss = 0.40500, acc = 0.50000\n",
            "Epoch [4/5] Step [1144/1610]: discriminator_loss = 0.57249, target_loss = 0.39116, acc = 0.50000\n",
            "Epoch [4/5] Step [1145/1610]: discriminator_loss = 0.54873, target_loss = 0.40715, acc = 0.50000\n",
            "Epoch [4/5] Step [1146/1610]: discriminator_loss = 0.56389, target_loss = 0.40572, acc = 0.50000\n",
            "Epoch [4/5] Step [1147/1610]: discriminator_loss = 0.56519, target_loss = 0.40713, acc = 0.50000\n",
            "Epoch [4/5] Step [1148/1610]: discriminator_loss = 0.56475, target_loss = 0.40077, acc = 0.50000\n",
            "Epoch [4/5] Step [1149/1610]: discriminator_loss = 0.57418, target_loss = 0.38245, acc = 0.50000\n",
            "Epoch [4/5] Step [1150/1610]: discriminator_loss = 0.56708, target_loss = 0.38666, acc = 0.50000\n",
            "Epoch [4/5] Step [1151/1610]: discriminator_loss = 0.57211, target_loss = 0.40525, acc = 0.50000\n",
            "Epoch [4/5] Step [1152/1610]: discriminator_loss = 0.58977, target_loss = 0.40092, acc = 0.50000\n",
            "Epoch [4/5] Step [1153/1610]: discriminator_loss = 0.56869, target_loss = 0.38593, acc = 0.50000\n",
            "Epoch [4/5] Step [1154/1610]: discriminator_loss = 0.55935, target_loss = 0.38950, acc = 0.50000\n",
            "Epoch [4/5] Step [1155/1610]: discriminator_loss = 0.57085, target_loss = 0.38018, acc = 0.50000\n",
            "Epoch [4/5] Step [1156/1610]: discriminator_loss = 0.57653, target_loss = 0.39254, acc = 0.50000\n",
            "Epoch [4/5] Step [1157/1610]: discriminator_loss = 0.58084, target_loss = 0.38148, acc = 0.50000\n",
            "Epoch [4/5] Step [1158/1610]: discriminator_loss = 0.58892, target_loss = 0.38373, acc = 0.50000\n",
            "Epoch [4/5] Step [1159/1610]: discriminator_loss = 0.57332, target_loss = 0.39895, acc = 0.50000\n",
            "Epoch [4/5] Step [1160/1610]: discriminator_loss = 0.59811, target_loss = 0.39557, acc = 0.50000\n",
            "Epoch [4/5] Step [1161/1610]: discriminator_loss = 0.55302, target_loss = 0.39781, acc = 0.50000\n",
            "Epoch [4/5] Step [1162/1610]: discriminator_loss = 0.54779, target_loss = 0.41553, acc = 0.50000\n",
            "Epoch [4/5] Step [1163/1610]: discriminator_loss = 0.56819, target_loss = 0.42531, acc = 0.50000\n",
            "Epoch [4/5] Step [1164/1610]: discriminator_loss = 0.58059, target_loss = 0.39001, acc = 0.50000\n",
            "Epoch [4/5] Step [1165/1610]: discriminator_loss = 0.54206, target_loss = 0.42148, acc = 0.50000\n",
            "Epoch [4/5] Step [1166/1610]: discriminator_loss = 0.55749, target_loss = 0.40451, acc = 0.50000\n",
            "Epoch [4/5] Step [1167/1610]: discriminator_loss = 0.56025, target_loss = 0.41020, acc = 0.50000\n",
            "Epoch [4/5] Step [1168/1610]: discriminator_loss = 0.53728, target_loss = 0.41039, acc = 0.50000\n",
            "Epoch [4/5] Step [1169/1610]: discriminator_loss = 0.54978, target_loss = 0.42482, acc = 0.50000\n",
            "Epoch [4/5] Step [1170/1610]: discriminator_loss = 0.54871, target_loss = 0.41167, acc = 0.50000\n",
            "Epoch [4/5] Step [1171/1610]: discriminator_loss = 0.56022, target_loss = 0.40636, acc = 0.50000\n",
            "Epoch [4/5] Step [1172/1610]: discriminator_loss = 0.55004, target_loss = 0.41134, acc = 0.50000\n",
            "Epoch [4/5] Step [1173/1610]: discriminator_loss = 0.54815, target_loss = 0.40403, acc = 0.50000\n",
            "Epoch [4/5] Step [1174/1610]: discriminator_loss = 0.57028, target_loss = 0.39889, acc = 0.50000\n",
            "Epoch [4/5] Step [1175/1610]: discriminator_loss = 0.56482, target_loss = 0.37955, acc = 0.50000\n",
            "Epoch [4/5] Step [1176/1610]: discriminator_loss = 0.54982, target_loss = 0.40851, acc = 0.50000\n",
            "Epoch [4/5] Step [1177/1610]: discriminator_loss = 0.56988, target_loss = 0.41304, acc = 0.50000\n",
            "Epoch [4/5] Step [1178/1610]: discriminator_loss = 0.57260, target_loss = 0.41135, acc = 0.50000\n",
            "Epoch [4/5] Step [1179/1610]: discriminator_loss = 0.53857, target_loss = 0.41177, acc = 0.50000\n",
            "Epoch [4/5] Step [1180/1610]: discriminator_loss = 0.54485, target_loss = 0.41428, acc = 0.50000\n",
            "Epoch [4/5] Step [1181/1610]: discriminator_loss = 0.56261, target_loss = 0.40702, acc = 0.50000\n",
            "Epoch [4/5] Step [1182/1610]: discriminator_loss = 0.56698, target_loss = 0.40196, acc = 0.50000\n",
            "Epoch [4/5] Step [1183/1610]: discriminator_loss = 0.55738, target_loss = 0.39112, acc = 0.50000\n",
            "Epoch [4/5] Step [1184/1610]: discriminator_loss = 0.56059, target_loss = 0.39216, acc = 0.50000\n",
            "Epoch [4/5] Step [1185/1610]: discriminator_loss = 0.55532, target_loss = 0.40334, acc = 0.50000\n",
            "Epoch [4/5] Step [1186/1610]: discriminator_loss = 0.59165, target_loss = 0.39053, acc = 0.50000\n",
            "Epoch [4/5] Step [1187/1610]: discriminator_loss = 0.56339, target_loss = 0.39292, acc = 0.50000\n",
            "Epoch [4/5] Step [1188/1610]: discriminator_loss = 0.55484, target_loss = 0.39491, acc = 0.50000\n",
            "Epoch [4/5] Step [1189/1610]: discriminator_loss = 0.56458, target_loss = 0.40999, acc = 0.50000\n",
            "Epoch [4/5] Step [1190/1610]: discriminator_loss = 0.57041, target_loss = 0.38496, acc = 0.50000\n",
            "Epoch [4/5] Step [1191/1610]: discriminator_loss = 0.57520, target_loss = 0.39631, acc = 0.50000\n",
            "Epoch [4/5] Step [1192/1610]: discriminator_loss = 0.56283, target_loss = 0.38979, acc = 0.50000\n",
            "Epoch [4/5] Step [1193/1610]: discriminator_loss = 0.55287, target_loss = 0.40062, acc = 0.50000\n",
            "Epoch [4/5] Step [1194/1610]: discriminator_loss = 0.55531, target_loss = 0.39284, acc = 0.50000\n",
            "Epoch [4/5] Step [1195/1610]: discriminator_loss = 0.58440, target_loss = 0.40434, acc = 0.50000\n",
            "Epoch [4/5] Step [1196/1610]: discriminator_loss = 0.56470, target_loss = 0.40382, acc = 0.50000\n",
            "Epoch [4/5] Step [1197/1610]: discriminator_loss = 0.56157, target_loss = 0.40901, acc = 0.50000\n",
            "Epoch [4/5] Step [1198/1610]: discriminator_loss = 0.54700, target_loss = 0.41540, acc = 0.50000\n",
            "Epoch [4/5] Step [1199/1610]: discriminator_loss = 0.54354, target_loss = 0.40529, acc = 0.50000\n",
            "Epoch [4/5] Step [1200/1610]: discriminator_loss = 0.54157, target_loss = 0.38741, acc = 0.50000\n",
            "Epoch [4/5] Step [1201/1610]: discriminator_loss = 0.55896, target_loss = 0.38122, acc = 0.50000\n",
            "Epoch [4/5] Step [1202/1610]: discriminator_loss = 0.56175, target_loss = 0.43036, acc = 0.50000\n",
            "Epoch [4/5] Step [1203/1610]: discriminator_loss = 0.53949, target_loss = 0.40986, acc = 0.50000\n",
            "Epoch [4/5] Step [1204/1610]: discriminator_loss = 0.57147, target_loss = 0.40146, acc = 0.50000\n",
            "Epoch [4/5] Step [1205/1610]: discriminator_loss = 0.54792, target_loss = 0.39592, acc = 0.50000\n",
            "Epoch [4/5] Step [1206/1610]: discriminator_loss = 0.53448, target_loss = 0.41540, acc = 0.50000\n",
            "Epoch [4/5] Step [1207/1610]: discriminator_loss = 0.61891, target_loss = 0.41751, acc = 0.50000\n",
            "Epoch [4/5] Step [1208/1610]: discriminator_loss = 0.55118, target_loss = 0.40271, acc = 0.50000\n",
            "Epoch [4/5] Step [1209/1610]: discriminator_loss = 0.54588, target_loss = 0.42232, acc = 0.50000\n",
            "Epoch [4/5] Step [1210/1610]: discriminator_loss = 0.54838, target_loss = 0.41762, acc = 0.50000\n",
            "Epoch [4/5] Step [1211/1610]: discriminator_loss = 0.55130, target_loss = 0.40335, acc = 0.50000\n",
            "Epoch [4/5] Step [1212/1610]: discriminator_loss = 0.54542, target_loss = 0.43469, acc = 0.50000\n",
            "Epoch [4/5] Step [1213/1610]: discriminator_loss = 0.53822, target_loss = 0.42785, acc = 0.50000\n",
            "Epoch [4/5] Step [1214/1610]: discriminator_loss = 0.54169, target_loss = 0.39798, acc = 0.50000\n",
            "Epoch [4/5] Step [1215/1610]: discriminator_loss = 0.55509, target_loss = 0.41711, acc = 0.50000\n",
            "Epoch [4/5] Step [1216/1610]: discriminator_loss = 0.54977, target_loss = 0.39561, acc = 0.50000\n",
            "Epoch [4/5] Step [1217/1610]: discriminator_loss = 0.56355, target_loss = 0.40890, acc = 0.50000\n",
            "Epoch [4/5] Step [1218/1610]: discriminator_loss = 0.55678, target_loss = 0.38907, acc = 0.50000\n",
            "Epoch [4/5] Step [1219/1610]: discriminator_loss = 0.54945, target_loss = 0.40416, acc = 0.50000\n",
            "Epoch [4/5] Step [1220/1610]: discriminator_loss = 0.57034, target_loss = 0.40365, acc = 0.50000\n",
            "Epoch [4/5] Step [1221/1610]: discriminator_loss = 0.56161, target_loss = 0.39888, acc = 0.50000\n",
            "Epoch [4/5] Step [1222/1610]: discriminator_loss = 0.60040, target_loss = 0.38653, acc = 0.50000\n",
            "Epoch [4/5] Step [1223/1610]: discriminator_loss = 0.54747, target_loss = 0.40151, acc = 0.50000\n",
            "Epoch [4/5] Step [1224/1610]: discriminator_loss = 0.53127, target_loss = 0.40073, acc = 0.50000\n",
            "Epoch [4/5] Step [1225/1610]: discriminator_loss = 0.54786, target_loss = 0.39703, acc = 0.50000\n",
            "Epoch [4/5] Step [1226/1610]: discriminator_loss = 0.55313, target_loss = 0.42249, acc = 0.50000\n",
            "Epoch [4/5] Step [1227/1610]: discriminator_loss = 0.55150, target_loss = 0.40296, acc = 0.50000\n",
            "Epoch [4/5] Step [1228/1610]: discriminator_loss = 0.59011, target_loss = 0.41285, acc = 0.50000\n",
            "Epoch [4/5] Step [1229/1610]: discriminator_loss = 0.55143, target_loss = 0.40981, acc = 0.50000\n",
            "Epoch [4/5] Step [1230/1610]: discriminator_loss = 0.56026, target_loss = 0.41048, acc = 0.50000\n",
            "Epoch [4/5] Step [1231/1610]: discriminator_loss = 0.56385, target_loss = 0.40695, acc = 0.50000\n",
            "Epoch [4/5] Step [1232/1610]: discriminator_loss = 0.55171, target_loss = 0.40853, acc = 0.50000\n",
            "Epoch [4/5] Step [1233/1610]: discriminator_loss = 0.56852, target_loss = 0.40593, acc = 0.50000\n",
            "Epoch [4/5] Step [1234/1610]: discriminator_loss = 0.56669, target_loss = 0.38792, acc = 0.50000\n",
            "Epoch [4/5] Step [1235/1610]: discriminator_loss = 0.54356, target_loss = 0.39452, acc = 0.50000\n",
            "Epoch [4/5] Step [1236/1610]: discriminator_loss = 0.56517, target_loss = 0.39255, acc = 0.50000\n",
            "Epoch [4/5] Step [1237/1610]: discriminator_loss = 0.58366, target_loss = 0.37360, acc = 0.50000\n",
            "Epoch [4/5] Step [1238/1610]: discriminator_loss = 0.56387, target_loss = 0.39789, acc = 0.50000\n",
            "Epoch [4/5] Step [1239/1610]: discriminator_loss = 0.56213, target_loss = 0.39803, acc = 0.50000\n",
            "Epoch [4/5] Step [1240/1610]: discriminator_loss = 0.57482, target_loss = 0.39075, acc = 0.50000\n",
            "Epoch [4/5] Step [1241/1610]: discriminator_loss = 0.57575, target_loss = 0.39132, acc = 0.50000\n",
            "Epoch [4/5] Step [1242/1610]: discriminator_loss = 0.56651, target_loss = 0.39255, acc = 0.50000\n",
            "Epoch [4/5] Step [1243/1610]: discriminator_loss = 0.58049, target_loss = 0.40781, acc = 0.50000\n",
            "Epoch [4/5] Step [1244/1610]: discriminator_loss = 0.56387, target_loss = 0.39766, acc = 0.50000\n",
            "Epoch [4/5] Step [1245/1610]: discriminator_loss = 0.57310, target_loss = 0.38268, acc = 0.50000\n",
            "Epoch [4/5] Step [1246/1610]: discriminator_loss = 0.56952, target_loss = 0.38612, acc = 0.50000\n",
            "Epoch [4/5] Step [1247/1610]: discriminator_loss = 0.58496, target_loss = 0.39283, acc = 0.50000\n",
            "Epoch [4/5] Step [1248/1610]: discriminator_loss = 0.55599, target_loss = 0.40454, acc = 0.50000\n",
            "Epoch [4/5] Step [1249/1610]: discriminator_loss = 0.57182, target_loss = 0.42851, acc = 0.50000\n",
            "Epoch [4/5] Step [1250/1610]: discriminator_loss = 0.56333, target_loss = 0.39094, acc = 0.50000\n",
            "Epoch [4/5] Step [1251/1610]: discriminator_loss = 0.56786, target_loss = 0.40474, acc = 0.50000\n",
            "Epoch [4/5] Step [1252/1610]: discriminator_loss = 0.57139, target_loss = 0.40870, acc = 0.50000\n",
            "Epoch [4/5] Step [1253/1610]: discriminator_loss = 0.54437, target_loss = 0.40776, acc = 0.50000\n",
            "Epoch [4/5] Step [1254/1610]: discriminator_loss = 0.56586, target_loss = 0.41622, acc = 0.50000\n",
            "Epoch [4/5] Step [1255/1610]: discriminator_loss = 0.55896, target_loss = 0.41805, acc = 0.50000\n",
            "Epoch [4/5] Step [1256/1610]: discriminator_loss = 0.54887, target_loss = 0.41104, acc = 0.50000\n",
            "Epoch [4/5] Step [1257/1610]: discriminator_loss = 0.56972, target_loss = 0.40658, acc = 0.50000\n",
            "Epoch [4/5] Step [1258/1610]: discriminator_loss = 0.54699, target_loss = 0.41177, acc = 0.50000\n",
            "Epoch [4/5] Step [1259/1610]: discriminator_loss = 0.54946, target_loss = 0.41707, acc = 0.50000\n",
            "Epoch [4/5] Step [1260/1610]: discriminator_loss = 0.54845, target_loss = 0.42328, acc = 0.50000\n",
            "Epoch [4/5] Step [1261/1610]: discriminator_loss = 0.60128, target_loss = 0.40767, acc = 0.50000\n",
            "Epoch [4/5] Step [1262/1610]: discriminator_loss = 0.57225, target_loss = 0.38728, acc = 0.50000\n",
            "Epoch [4/5] Step [1263/1610]: discriminator_loss = 0.56797, target_loss = 0.38562, acc = 0.50000\n",
            "Epoch [4/5] Step [1264/1610]: discriminator_loss = 0.56195, target_loss = 0.39232, acc = 0.50000\n",
            "Epoch [4/5] Step [1265/1610]: discriminator_loss = 0.56581, target_loss = 0.39663, acc = 0.50000\n",
            "Epoch [4/5] Step [1266/1610]: discriminator_loss = 0.57453, target_loss = 0.40432, acc = 0.50000\n",
            "Epoch [4/5] Step [1267/1610]: discriminator_loss = 0.57780, target_loss = 0.40198, acc = 0.50000\n",
            "Epoch [4/5] Step [1268/1610]: discriminator_loss = 0.57366, target_loss = 0.40134, acc = 0.50000\n",
            "Epoch [4/5] Step [1269/1610]: discriminator_loss = 0.56852, target_loss = 0.38567, acc = 0.50000\n",
            "Epoch [4/5] Step [1270/1610]: discriminator_loss = 0.57107, target_loss = 0.39146, acc = 0.50000\n",
            "Epoch [4/5] Step [1271/1610]: discriminator_loss = 0.58252, target_loss = 0.39199, acc = 0.50000\n",
            "Epoch [4/5] Step [1272/1610]: discriminator_loss = 0.56535, target_loss = 0.39757, acc = 0.50000\n",
            "Epoch [4/5] Step [1273/1610]: discriminator_loss = 0.56517, target_loss = 0.37040, acc = 0.50000\n",
            "Epoch [4/5] Step [1274/1610]: discriminator_loss = 0.56815, target_loss = 0.38797, acc = 0.50000\n",
            "Epoch [4/5] Step [1275/1610]: discriminator_loss = 0.56799, target_loss = 0.39389, acc = 0.50000\n",
            "Epoch [4/5] Step [1276/1610]: discriminator_loss = 0.59330, target_loss = 0.39714, acc = 0.50000\n",
            "Epoch [4/5] Step [1277/1610]: discriminator_loss = 0.56588, target_loss = 0.39719, acc = 0.50000\n",
            "Epoch [4/5] Step [1278/1610]: discriminator_loss = 0.65906, target_loss = 0.39443, acc = 0.50000\n",
            "Epoch [4/5] Step [1279/1610]: discriminator_loss = 0.57149, target_loss = 0.39414, acc = 0.50000\n",
            "Epoch [4/5] Step [1280/1610]: discriminator_loss = 0.56993, target_loss = 0.39355, acc = 0.50000\n",
            "Epoch [4/5] Step [1281/1610]: discriminator_loss = 0.55831, target_loss = 0.40356, acc = 0.50000\n",
            "Epoch [4/5] Step [1282/1610]: discriminator_loss = 0.55892, target_loss = 0.40422, acc = 0.50000\n",
            "Epoch [4/5] Step [1283/1610]: discriminator_loss = 0.62940, target_loss = 0.40783, acc = 0.50000\n",
            "Epoch [4/5] Step [1284/1610]: discriminator_loss = 0.56178, target_loss = 0.40616, acc = 0.50000\n",
            "Epoch [4/5] Step [1285/1610]: discriminator_loss = 0.58002, target_loss = 0.39996, acc = 0.50000\n",
            "Epoch [4/5] Step [1286/1610]: discriminator_loss = 0.55052, target_loss = 0.40529, acc = 0.50000\n",
            "Epoch [4/5] Step [1287/1610]: discriminator_loss = 0.55135, target_loss = 0.40634, acc = 0.50000\n",
            "Epoch [4/5] Step [1288/1610]: discriminator_loss = 0.55922, target_loss = 0.42684, acc = 0.50000\n",
            "Epoch [4/5] Step [1289/1610]: discriminator_loss = 0.55339, target_loss = 0.39514, acc = 0.50000\n",
            "Epoch [4/5] Step [1290/1610]: discriminator_loss = 0.55598, target_loss = 0.40183, acc = 0.50000\n",
            "Epoch [4/5] Step [1291/1610]: discriminator_loss = 0.54336, target_loss = 0.41542, acc = 0.50000\n",
            "Epoch [4/5] Step [1292/1610]: discriminator_loss = 0.54117, target_loss = 0.40667, acc = 0.50000\n",
            "Epoch [4/5] Step [1293/1610]: discriminator_loss = 0.56073, target_loss = 0.41205, acc = 0.50000\n",
            "Epoch [4/5] Step [1294/1610]: discriminator_loss = 0.55805, target_loss = 0.39889, acc = 0.50000\n",
            "Epoch [4/5] Step [1295/1610]: discriminator_loss = 0.55771, target_loss = 0.40666, acc = 0.50000\n",
            "Epoch [4/5] Step [1296/1610]: discriminator_loss = 0.55302, target_loss = 0.40189, acc = 0.50000\n",
            "Epoch [4/5] Step [1297/1610]: discriminator_loss = 0.55161, target_loss = 0.39814, acc = 0.50000\n",
            "Epoch [4/5] Step [1298/1610]: discriminator_loss = 0.54969, target_loss = 0.39966, acc = 0.50000\n",
            "Epoch [4/5] Step [1299/1610]: discriminator_loss = 0.60555, target_loss = 0.41980, acc = 0.50000\n",
            "Epoch [4/5] Step [1300/1610]: discriminator_loss = 0.55703, target_loss = 0.41067, acc = 0.50000\n",
            "Epoch [4/5] Step [1301/1610]: discriminator_loss = 0.57746, target_loss = 0.41800, acc = 0.50000\n",
            "Epoch [4/5] Step [1302/1610]: discriminator_loss = 0.54161, target_loss = 0.41396, acc = 0.50000\n",
            "Epoch [4/5] Step [1303/1610]: discriminator_loss = 0.55033, target_loss = 0.41214, acc = 0.50000\n",
            "Epoch [4/5] Step [1304/1610]: discriminator_loss = 0.56231, target_loss = 0.40985, acc = 0.50000\n",
            "Epoch [4/5] Step [1305/1610]: discriminator_loss = 0.55965, target_loss = 0.39915, acc = 0.50000\n",
            "Epoch [4/5] Step [1306/1610]: discriminator_loss = 0.55852, target_loss = 0.39636, acc = 0.50000\n",
            "Epoch [4/5] Step [1307/1610]: discriminator_loss = 0.56305, target_loss = 0.39760, acc = 0.50000\n",
            "Epoch [4/5] Step [1308/1610]: discriminator_loss = 0.56278, target_loss = 0.41383, acc = 0.50000\n",
            "Epoch [4/5] Step [1309/1610]: discriminator_loss = 0.56778, target_loss = 0.40928, acc = 0.50000\n",
            "Epoch [4/5] Step [1310/1610]: discriminator_loss = 0.57306, target_loss = 0.40466, acc = 0.50000\n",
            "Epoch [4/5] Step [1311/1610]: discriminator_loss = 0.55480, target_loss = 0.40740, acc = 0.50000\n",
            "Epoch [4/5] Step [1312/1610]: discriminator_loss = 0.55596, target_loss = 0.39504, acc = 0.50000\n",
            "Epoch [4/5] Step [1313/1610]: discriminator_loss = 0.54708, target_loss = 0.39048, acc = 0.50000\n",
            "Epoch [4/5] Step [1314/1610]: discriminator_loss = 0.57127, target_loss = 0.39922, acc = 0.50000\n",
            "Epoch [4/5] Step [1315/1610]: discriminator_loss = 0.57391, target_loss = 0.39531, acc = 0.50000\n",
            "Epoch [4/5] Step [1316/1610]: discriminator_loss = 0.54824, target_loss = 0.38863, acc = 0.50000\n",
            "Epoch [4/5] Step [1317/1610]: discriminator_loss = 0.56948, target_loss = 0.39226, acc = 0.50000\n",
            "Epoch [4/5] Step [1318/1610]: discriminator_loss = 0.55483, target_loss = 0.39567, acc = 0.50000\n",
            "Epoch [4/5] Step [1319/1610]: discriminator_loss = 0.56525, target_loss = 0.39381, acc = 0.50000\n",
            "Epoch [4/5] Step [1320/1610]: discriminator_loss = 0.56572, target_loss = 0.38732, acc = 0.50000\n",
            "Epoch [4/5] Step [1321/1610]: discriminator_loss = 0.57585, target_loss = 0.39111, acc = 0.50000\n",
            "Epoch [4/5] Step [1322/1610]: discriminator_loss = 0.59170, target_loss = 0.39151, acc = 0.50000\n",
            "Epoch [4/5] Step [1323/1610]: discriminator_loss = 0.56027, target_loss = 0.37676, acc = 0.50000\n",
            "Epoch [4/5] Step [1324/1610]: discriminator_loss = 0.56213, target_loss = 0.38579, acc = 0.50000\n",
            "Epoch [4/5] Step [1325/1610]: discriminator_loss = 0.59165, target_loss = 0.40315, acc = 0.50000\n",
            "Epoch [4/5] Step [1326/1610]: discriminator_loss = 0.56419, target_loss = 0.37440, acc = 0.50000\n",
            "Epoch [4/5] Step [1327/1610]: discriminator_loss = 0.56926, target_loss = 0.39223, acc = 0.50000\n",
            "Epoch [4/5] Step [1328/1610]: discriminator_loss = 0.56579, target_loss = 0.38445, acc = 0.50000\n",
            "Epoch [4/5] Step [1329/1610]: discriminator_loss = 0.55649, target_loss = 0.39079, acc = 0.50000\n",
            "Epoch [4/5] Step [1330/1610]: discriminator_loss = 0.57011, target_loss = 0.39224, acc = 0.50000\n",
            "Epoch [4/5] Step [1331/1610]: discriminator_loss = 0.54357, target_loss = 0.38825, acc = 0.50000\n",
            "Epoch [4/5] Step [1332/1610]: discriminator_loss = 0.54770, target_loss = 0.41139, acc = 0.50000\n",
            "Epoch [4/5] Step [1333/1610]: discriminator_loss = 0.55928, target_loss = 0.40874, acc = 0.50000\n",
            "Epoch [4/5] Step [1334/1610]: discriminator_loss = 0.55795, target_loss = 0.38510, acc = 0.50000\n",
            "Epoch [4/5] Step [1335/1610]: discriminator_loss = 0.60840, target_loss = 0.38917, acc = 0.50000\n",
            "Epoch [4/5] Step [1336/1610]: discriminator_loss = 0.65076, target_loss = 0.39430, acc = 0.50000\n",
            "Epoch [4/5] Step [1337/1610]: discriminator_loss = 0.65420, target_loss = 0.37739, acc = 0.50000\n",
            "Epoch [4/5] Step [1338/1610]: discriminator_loss = 0.54240, target_loss = 0.42236, acc = 0.50000\n",
            "Epoch [4/5] Step [1339/1610]: discriminator_loss = 0.56135, target_loss = 0.40035, acc = 0.50000\n",
            "Epoch [4/5] Step [1340/1610]: discriminator_loss = 0.54578, target_loss = 0.41125, acc = 0.50000\n",
            "Epoch [4/5] Step [1341/1610]: discriminator_loss = 0.55438, target_loss = 0.41318, acc = 0.50000\n",
            "Epoch [4/5] Step [1342/1610]: discriminator_loss = 0.55281, target_loss = 0.41732, acc = 0.50000\n",
            "Epoch [4/5] Step [1343/1610]: discriminator_loss = 0.54361, target_loss = 0.41861, acc = 0.50000\n",
            "Epoch [4/5] Step [1344/1610]: discriminator_loss = 0.55484, target_loss = 0.41754, acc = 0.50000\n",
            "Epoch [4/5] Step [1345/1610]: discriminator_loss = 0.55729, target_loss = 0.40886, acc = 0.50000\n",
            "Epoch [4/5] Step [1346/1610]: discriminator_loss = 0.54884, target_loss = 0.40912, acc = 0.50000\n",
            "Epoch [4/5] Step [1347/1610]: discriminator_loss = 0.55460, target_loss = 0.40655, acc = 0.50000\n",
            "Epoch [4/5] Step [1348/1610]: discriminator_loss = 0.56573, target_loss = 0.41448, acc = 0.50000\n",
            "Epoch [4/5] Step [1349/1610]: discriminator_loss = 0.56442, target_loss = 0.40549, acc = 0.50000\n",
            "Epoch [4/5] Step [1350/1610]: discriminator_loss = 0.56831, target_loss = 0.39888, acc = 0.50000\n",
            "Epoch [4/5] Step [1351/1610]: discriminator_loss = 0.55165, target_loss = 0.39310, acc = 0.50000\n",
            "Epoch [4/5] Step [1352/1610]: discriminator_loss = 0.58564, target_loss = 0.38668, acc = 0.50000\n",
            "Epoch [4/5] Step [1353/1610]: discriminator_loss = 0.56717, target_loss = 0.39506, acc = 0.50000\n",
            "Epoch [4/5] Step [1354/1610]: discriminator_loss = 0.56993, target_loss = 0.39076, acc = 0.50000\n",
            "Epoch [4/5] Step [1355/1610]: discriminator_loss = 0.56894, target_loss = 0.39322, acc = 0.50000\n",
            "Epoch [4/5] Step [1356/1610]: discriminator_loss = 0.55791, target_loss = 0.38532, acc = 0.50000\n",
            "Epoch [4/5] Step [1357/1610]: discriminator_loss = 0.57447, target_loss = 0.38556, acc = 0.50000\n",
            "Epoch [4/5] Step [1358/1610]: discriminator_loss = 0.59240, target_loss = 0.38456, acc = 0.50000\n",
            "Epoch [4/5] Step [1359/1610]: discriminator_loss = 0.58007, target_loss = 0.38334, acc = 0.50000\n",
            "Epoch [4/5] Step [1360/1610]: discriminator_loss = 0.59953, target_loss = 0.38608, acc = 0.50000\n",
            "Epoch [4/5] Step [1361/1610]: discriminator_loss = 0.56354, target_loss = 0.38275, acc = 0.50000\n",
            "Epoch [4/5] Step [1362/1610]: discriminator_loss = 0.57178, target_loss = 0.37770, acc = 0.50000\n",
            "Epoch [4/5] Step [1363/1610]: discriminator_loss = 0.60930, target_loss = 0.38480, acc = 0.50000\n",
            "Epoch [4/5] Step [1364/1610]: discriminator_loss = 0.57035, target_loss = 0.38672, acc = 0.50000\n",
            "Epoch [4/5] Step [1365/1610]: discriminator_loss = 0.54455, target_loss = 0.39960, acc = 0.50000\n",
            "Epoch [4/5] Step [1366/1610]: discriminator_loss = 0.55421, target_loss = 0.40099, acc = 0.50000\n",
            "Epoch [4/5] Step [1367/1610]: discriminator_loss = 0.56092, target_loss = 0.37336, acc = 0.50000\n",
            "Epoch [4/5] Step [1368/1610]: discriminator_loss = 0.60457, target_loss = 0.38226, acc = 0.50000\n",
            "Epoch [4/5] Step [1369/1610]: discriminator_loss = 0.55188, target_loss = 0.40860, acc = 0.50000\n",
            "Epoch [4/5] Step [1370/1610]: discriminator_loss = 0.54790, target_loss = 0.40348, acc = 0.50000\n",
            "Epoch [4/5] Step [1371/1610]: discriminator_loss = 0.54077, target_loss = 0.42279, acc = 0.50000\n",
            "Epoch [4/5] Step [1372/1610]: discriminator_loss = 0.55836, target_loss = 0.40709, acc = 0.50000\n",
            "Epoch [4/5] Step [1373/1610]: discriminator_loss = 0.55184, target_loss = 0.41193, acc = 0.50000\n",
            "Epoch [4/5] Step [1374/1610]: discriminator_loss = 0.53918, target_loss = 0.41357, acc = 0.50000\n",
            "Epoch [4/5] Step [1375/1610]: discriminator_loss = 0.53790, target_loss = 0.41382, acc = 0.50000\n",
            "Epoch [4/5] Step [1376/1610]: discriminator_loss = 0.53868, target_loss = 0.41058, acc = 0.50000\n",
            "Epoch [4/5] Step [1377/1610]: discriminator_loss = 0.53399, target_loss = 0.41516, acc = 0.50000\n",
            "Epoch [4/5] Step [1378/1610]: discriminator_loss = 0.54078, target_loss = 0.41699, acc = 0.50000\n",
            "Epoch [4/5] Step [1379/1610]: discriminator_loss = 0.54002, target_loss = 0.40538, acc = 0.50000\n",
            "Epoch [4/5] Step [1380/1610]: discriminator_loss = 0.54073, target_loss = 0.41045, acc = 0.50000\n",
            "Epoch [4/5] Step [1381/1610]: discriminator_loss = 0.54215, target_loss = 0.39823, acc = 0.50000\n",
            "Epoch [4/5] Step [1382/1610]: discriminator_loss = 0.55541, target_loss = 0.39951, acc = 0.50000\n",
            "Epoch [4/5] Step [1383/1610]: discriminator_loss = 0.54399, target_loss = 0.41170, acc = 0.50000\n",
            "Epoch [4/5] Step [1384/1610]: discriminator_loss = 0.53968, target_loss = 0.40729, acc = 0.50000\n",
            "Epoch [4/5] Step [1385/1610]: discriminator_loss = 0.56118, target_loss = 0.39548, acc = 0.50000\n",
            "Epoch [4/5] Step [1386/1610]: discriminator_loss = 0.53540, target_loss = 0.42537, acc = 0.50000\n",
            "Epoch [4/5] Step [1387/1610]: discriminator_loss = 0.53149, target_loss = 0.37590, acc = 0.50000\n",
            "Epoch [4/5] Step [1388/1610]: discriminator_loss = 0.59081, target_loss = 0.38602, acc = 0.50000\n",
            "Epoch [4/5] Step [1389/1610]: discriminator_loss = 0.56794, target_loss = 0.42332, acc = 0.50000\n",
            "Epoch [4/5] Step [1390/1610]: discriminator_loss = 0.67656, target_loss = 0.33047, acc = 0.50000\n",
            "Epoch [4/5] Step [1391/1610]: discriminator_loss = 0.54152, target_loss = 0.38202, acc = 0.50000\n",
            "Epoch [4/5] Step [1392/1610]: discriminator_loss = 0.56663, target_loss = 0.42318, acc = 0.50000\n",
            "Epoch [4/5] Step [1393/1610]: discriminator_loss = 0.55156, target_loss = 0.39877, acc = 0.50000\n",
            "Epoch [4/5] Step [1394/1610]: discriminator_loss = 0.52609, target_loss = 0.40130, acc = 0.50000\n",
            "Epoch [4/5] Step [1395/1610]: discriminator_loss = 0.51597, target_loss = 0.44450, acc = 0.50000\n",
            "Epoch [4/5] Step [1396/1610]: discriminator_loss = 0.54585, target_loss = 0.40668, acc = 0.50000\n",
            "Epoch [4/5] Step [1397/1610]: discriminator_loss = 0.53165, target_loss = 0.40024, acc = 0.50000\n",
            "Epoch [4/5] Step [1398/1610]: discriminator_loss = 0.57446, target_loss = 0.41076, acc = 0.50000\n",
            "Epoch [4/5] Step [1399/1610]: discriminator_loss = 0.55002, target_loss = 0.42880, acc = 0.50000\n",
            "Epoch [4/5] Step [1400/1610]: discriminator_loss = 0.52637, target_loss = 0.40619, acc = 0.50000\n",
            "Epoch [4/5] Step [1401/1610]: discriminator_loss = 0.52209, target_loss = 0.41630, acc = 0.50000\n",
            "Epoch [4/5] Step [1402/1610]: discriminator_loss = 0.52271, target_loss = 0.44119, acc = 0.50000\n",
            "Epoch [4/5] Step [1403/1610]: discriminator_loss = 0.59767, target_loss = 0.45188, acc = 0.50000\n",
            "Epoch [4/5] Step [1404/1610]: discriminator_loss = 0.56569, target_loss = 0.46214, acc = 0.50000\n",
            "Epoch [4/5] Step [1405/1610]: discriminator_loss = 0.52651, target_loss = 0.41626, acc = 0.50000\n",
            "Epoch [4/5] Step [1406/1610]: discriminator_loss = 0.53691, target_loss = 0.43199, acc = 0.50000\n",
            "Epoch [4/5] Step [1407/1610]: discriminator_loss = 0.53242, target_loss = 0.42866, acc = 0.50000\n",
            "Epoch [4/5] Step [1408/1610]: discriminator_loss = 0.54231, target_loss = 0.42709, acc = 0.50000\n",
            "Epoch [4/5] Step [1409/1610]: discriminator_loss = 0.54983, target_loss = 0.42763, acc = 0.50000\n",
            "Epoch [4/5] Step [1410/1610]: discriminator_loss = 0.55523, target_loss = 0.41009, acc = 0.50000\n",
            "Epoch [4/5] Step [1411/1610]: discriminator_loss = 0.56281, target_loss = 0.42296, acc = 0.50000\n",
            "Epoch [4/5] Step [1412/1610]: discriminator_loss = 0.54955, target_loss = 0.40839, acc = 0.50000\n",
            "Epoch [4/5] Step [1413/1610]: discriminator_loss = 0.56213, target_loss = 0.40263, acc = 0.50000\n",
            "Epoch [4/5] Step [1414/1610]: discriminator_loss = 0.56047, target_loss = 0.40765, acc = 0.50000\n",
            "Epoch [4/5] Step [1415/1610]: discriminator_loss = 0.57375, target_loss = 0.40411, acc = 0.50000\n",
            "Epoch [4/5] Step [1416/1610]: discriminator_loss = 0.57192, target_loss = 0.39169, acc = 0.50000\n",
            "Epoch [4/5] Step [1417/1610]: discriminator_loss = 0.58706, target_loss = 0.39149, acc = 0.50000\n",
            "Epoch [4/5] Step [1418/1610]: discriminator_loss = 0.56996, target_loss = 0.37670, acc = 0.50000\n",
            "Epoch [4/5] Step [1419/1610]: discriminator_loss = 0.55333, target_loss = 0.39000, acc = 0.50000\n",
            "Epoch [4/5] Step [1420/1610]: discriminator_loss = 0.57548, target_loss = 0.37845, acc = 0.50000\n",
            "Epoch [4/5] Step [1421/1610]: discriminator_loss = 0.56430, target_loss = 0.39946, acc = 0.50000\n",
            "Epoch [4/5] Step [1422/1610]: discriminator_loss = 0.56481, target_loss = 0.37323, acc = 0.50000\n",
            "Epoch [4/5] Step [1423/1610]: discriminator_loss = 0.57034, target_loss = 0.39057, acc = 0.50000\n",
            "Epoch [4/5] Step [1424/1610]: discriminator_loss = 0.59076, target_loss = 0.38665, acc = 0.50000\n",
            "Epoch [4/5] Step [1425/1610]: discriminator_loss = 0.57698, target_loss = 0.38693, acc = 0.50000\n",
            "Epoch [4/5] Step [1426/1610]: discriminator_loss = 0.56047, target_loss = 0.36264, acc = 0.50000\n",
            "Epoch [4/5] Step [1427/1610]: discriminator_loss = 0.57167, target_loss = 0.39138, acc = 0.50000\n",
            "Epoch [4/5] Step [1428/1610]: discriminator_loss = 0.56538, target_loss = 0.40435, acc = 0.50000\n",
            "Epoch [4/5] Step [1429/1610]: discriminator_loss = 0.59986, target_loss = 0.39892, acc = 0.50000\n",
            "Epoch [4/5] Step [1430/1610]: discriminator_loss = 0.59788, target_loss = 0.39723, acc = 0.50000\n",
            "Epoch [4/5] Step [1431/1610]: discriminator_loss = 0.54930, target_loss = 0.40399, acc = 0.50000\n",
            "Epoch [4/5] Step [1432/1610]: discriminator_loss = 0.54987, target_loss = 0.40425, acc = 0.50000\n",
            "Epoch [4/5] Step [1433/1610]: discriminator_loss = 0.54798, target_loss = 0.41843, acc = 0.50000\n",
            "Epoch [4/5] Step [1434/1610]: discriminator_loss = 0.55695, target_loss = 0.39234, acc = 0.50000\n",
            "Epoch [4/5] Step [1435/1610]: discriminator_loss = 0.55001, target_loss = 0.42471, acc = 0.50000\n",
            "Epoch [4/5] Step [1436/1610]: discriminator_loss = 0.56397, target_loss = 0.39900, acc = 0.50000\n",
            "Epoch [4/5] Step [1437/1610]: discriminator_loss = 0.55888, target_loss = 0.40855, acc = 0.50000\n",
            "Epoch [4/5] Step [1438/1610]: discriminator_loss = 0.55643, target_loss = 0.40166, acc = 0.50000\n",
            "Epoch [4/5] Step [1439/1610]: discriminator_loss = 0.56547, target_loss = 0.40511, acc = 0.50000\n",
            "Epoch [4/5] Step [1440/1610]: discriminator_loss = 0.56790, target_loss = 0.38514, acc = 0.50000\n",
            "Epoch [4/5] Step [1441/1610]: discriminator_loss = 0.55073, target_loss = 0.40614, acc = 0.50000\n",
            "Epoch [4/5] Step [1442/1610]: discriminator_loss = 0.56643, target_loss = 0.39503, acc = 0.50000\n",
            "Epoch [4/5] Step [1443/1610]: discriminator_loss = 0.54750, target_loss = 0.40398, acc = 0.50000\n",
            "Epoch [4/5] Step [1444/1610]: discriminator_loss = 0.56652, target_loss = 0.39101, acc = 0.50000\n",
            "Epoch [4/5] Step [1445/1610]: discriminator_loss = 0.55705, target_loss = 0.40449, acc = 0.50000\n",
            "Epoch [4/5] Step [1446/1610]: discriminator_loss = 0.56402, target_loss = 0.40170, acc = 0.50000\n",
            "Epoch [4/5] Step [1447/1610]: discriminator_loss = 0.56521, target_loss = 0.39280, acc = 0.50000\n",
            "Epoch [4/5] Step [1448/1610]: discriminator_loss = 0.55760, target_loss = 0.41630, acc = 0.50000\n",
            "Epoch [4/5] Step [1449/1610]: discriminator_loss = 0.56387, target_loss = 0.37429, acc = 0.50000\n",
            "Epoch [4/5] Step [1450/1610]: discriminator_loss = 0.56859, target_loss = 0.40324, acc = 0.50000\n",
            "Epoch [4/5] Step [1451/1610]: discriminator_loss = 0.55559, target_loss = 0.40729, acc = 0.50000\n",
            "Epoch [4/5] Step [1452/1610]: discriminator_loss = 0.55595, target_loss = 0.41059, acc = 0.50000\n",
            "Epoch [4/5] Step [1453/1610]: discriminator_loss = 0.55913, target_loss = 0.39642, acc = 0.50000\n",
            "Epoch [4/5] Step [1454/1610]: discriminator_loss = 0.61474, target_loss = 0.39525, acc = 0.50000\n",
            "Epoch [4/5] Step [1455/1610]: discriminator_loss = 0.54037, target_loss = 0.41957, acc = 0.50000\n",
            "Epoch [4/5] Step [1456/1610]: discriminator_loss = 0.57111, target_loss = 0.41062, acc = 0.50000\n",
            "Epoch [4/5] Step [1457/1610]: discriminator_loss = 0.56241, target_loss = 0.42445, acc = 0.50000\n",
            "Epoch [4/5] Step [1458/1610]: discriminator_loss = 0.52953, target_loss = 0.41616, acc = 0.50000\n",
            "Epoch [4/5] Step [1459/1610]: discriminator_loss = 0.57377, target_loss = 0.42876, acc = 0.50000\n",
            "Epoch [4/5] Step [1460/1610]: discriminator_loss = 0.54377, target_loss = 0.42416, acc = 0.50000\n",
            "Epoch [4/5] Step [1461/1610]: discriminator_loss = 0.55197, target_loss = 0.42440, acc = 0.50000\n",
            "Epoch [4/5] Step [1462/1610]: discriminator_loss = 0.52437, target_loss = 0.44213, acc = 0.50000\n",
            "Epoch [4/5] Step [1463/1610]: discriminator_loss = 0.54110, target_loss = 0.41750, acc = 0.50000\n",
            "Epoch [4/5] Step [1464/1610]: discriminator_loss = 0.56215, target_loss = 0.41084, acc = 0.50000\n",
            "Epoch [4/5] Step [1465/1610]: discriminator_loss = 0.55723, target_loss = 0.41502, acc = 0.50000\n",
            "Epoch [4/5] Step [1466/1610]: discriminator_loss = 0.54942, target_loss = 0.42364, acc = 0.50000\n",
            "Epoch [4/5] Step [1467/1610]: discriminator_loss = 0.56546, target_loss = 0.40278, acc = 0.50000\n",
            "Epoch [4/5] Step [1468/1610]: discriminator_loss = 0.55149, target_loss = 0.41065, acc = 0.50000\n",
            "Epoch [4/5] Step [1469/1610]: discriminator_loss = 0.55364, target_loss = 0.40126, acc = 0.50000\n",
            "Epoch [4/5] Step [1470/1610]: discriminator_loss = 0.54659, target_loss = 0.39746, acc = 0.50000\n",
            "Epoch [4/5] Step [1471/1610]: discriminator_loss = 0.55391, target_loss = 0.39973, acc = 0.50000\n",
            "Epoch [4/5] Step [1472/1610]: discriminator_loss = 0.55653, target_loss = 0.38483, acc = 0.50000\n",
            "Epoch [4/5] Step [1473/1610]: discriminator_loss = 0.56494, target_loss = 0.38456, acc = 0.50000\n",
            "Epoch [4/5] Step [1474/1610]: discriminator_loss = 0.54849, target_loss = 0.39653, acc = 0.50000\n",
            "Epoch [4/5] Step [1475/1610]: discriminator_loss = 0.56990, target_loss = 0.37015, acc = 0.50000\n",
            "Epoch [4/5] Step [1476/1610]: discriminator_loss = 0.57022, target_loss = 0.39208, acc = 0.50000\n",
            "Epoch [4/5] Step [1477/1610]: discriminator_loss = 0.57843, target_loss = 0.36701, acc = 0.50000\n",
            "Epoch [4/5] Step [1478/1610]: discriminator_loss = 0.56846, target_loss = 0.39152, acc = 0.50000\n",
            "Epoch [4/5] Step [1479/1610]: discriminator_loss = 0.58489, target_loss = 0.39426, acc = 0.50000\n",
            "Epoch [4/5] Step [1480/1610]: discriminator_loss = 0.57109, target_loss = 0.39864, acc = 0.50000\n",
            "Epoch [4/5] Step [1481/1610]: discriminator_loss = 0.57266, target_loss = 0.38153, acc = 0.50000\n",
            "Epoch [4/5] Step [1482/1610]: discriminator_loss = 0.58291, target_loss = 0.37547, acc = 0.50000\n",
            "Epoch [4/5] Step [1483/1610]: discriminator_loss = 0.58858, target_loss = 0.33778, acc = 0.50000\n",
            "Epoch [4/5] Step [1484/1610]: discriminator_loss = 0.56826, target_loss = 0.40614, acc = 0.50000\n",
            "Epoch [4/5] Step [1485/1610]: discriminator_loss = 0.57645, target_loss = 0.40736, acc = 0.50000\n",
            "Epoch [4/5] Step [1486/1610]: discriminator_loss = 0.57014, target_loss = 0.41942, acc = 0.50000\n",
            "Epoch [4/5] Step [1487/1610]: discriminator_loss = 0.53721, target_loss = 0.41460, acc = 0.50000\n",
            "Epoch [4/5] Step [1488/1610]: discriminator_loss = 0.54874, target_loss = 0.41180, acc = 0.50000\n",
            "Epoch [4/5] Step [1489/1610]: discriminator_loss = 0.54233, target_loss = 0.42375, acc = 0.50000\n",
            "Epoch [4/5] Step [1490/1610]: discriminator_loss = 0.52687, target_loss = 0.39804, acc = 0.50000\n",
            "Epoch [4/5] Step [1491/1610]: discriminator_loss = 0.57327, target_loss = 0.41596, acc = 0.50000\n",
            "Epoch [4/5] Step [1492/1610]: discriminator_loss = 0.54165, target_loss = 0.40838, acc = 0.50000\n",
            "Epoch [4/5] Step [1493/1610]: discriminator_loss = 0.53941, target_loss = 0.39185, acc = 0.50000\n",
            "Epoch [4/5] Step [1494/1610]: discriminator_loss = 0.56282, target_loss = 0.40206, acc = 0.50000\n",
            "Epoch [4/5] Step [1495/1610]: discriminator_loss = 0.54175, target_loss = 0.40589, acc = 0.50000\n",
            "Epoch [4/5] Step [1496/1610]: discriminator_loss = 0.55182, target_loss = 0.41719, acc = 0.50000\n",
            "Epoch [4/5] Step [1497/1610]: discriminator_loss = 0.57395, target_loss = 0.42003, acc = 0.50000\n",
            "Epoch [4/5] Step [1498/1610]: discriminator_loss = 0.55585, target_loss = 0.40759, acc = 0.50000\n",
            "Epoch [4/5] Step [1499/1610]: discriminator_loss = 0.54706, target_loss = 0.41318, acc = 0.50000\n",
            "Epoch [4/5] Step [1500/1610]: discriminator_loss = 0.55483, target_loss = 0.40512, acc = 0.50000\n",
            "Epoch [4/5] Step [1501/1610]: discriminator_loss = 0.54331, target_loss = 0.41585, acc = 0.50000\n",
            "Epoch [4/5] Step [1502/1610]: discriminator_loss = 0.53906, target_loss = 0.42357, acc = 0.50000\n",
            "Epoch [4/5] Step [1503/1610]: discriminator_loss = 0.55625, target_loss = 0.42266, acc = 0.50000\n",
            "Epoch [4/5] Step [1504/1610]: discriminator_loss = 0.53155, target_loss = 0.42738, acc = 0.50000\n",
            "Epoch [4/5] Step [1505/1610]: discriminator_loss = 0.54348, target_loss = 0.39572, acc = 0.50000\n",
            "Epoch [4/5] Step [1506/1610]: discriminator_loss = 0.55071, target_loss = 0.42347, acc = 0.50000\n",
            "Epoch [4/5] Step [1507/1610]: discriminator_loss = 0.54149, target_loss = 0.40260, acc = 0.50000\n",
            "Epoch [4/5] Step [1508/1610]: discriminator_loss = 0.54471, target_loss = 0.41311, acc = 0.50000\n",
            "Epoch [4/5] Step [1509/1610]: discriminator_loss = 0.54292, target_loss = 0.40899, acc = 0.50000\n",
            "Epoch [4/5] Step [1510/1610]: discriminator_loss = 0.55515, target_loss = 0.41654, acc = 0.50000\n",
            "Epoch [4/5] Step [1511/1610]: discriminator_loss = 0.56543, target_loss = 0.36986, acc = 0.50000\n",
            "Epoch [4/5] Step [1512/1610]: discriminator_loss = 0.57683, target_loss = 0.41117, acc = 0.50000\n",
            "Epoch [4/5] Step [1513/1610]: discriminator_loss = 0.57167, target_loss = 0.39781, acc = 0.50000\n",
            "Epoch [4/5] Step [1514/1610]: discriminator_loss = 0.56479, target_loss = 0.38711, acc = 0.50000\n",
            "Epoch [4/5] Step [1515/1610]: discriminator_loss = 0.54992, target_loss = 0.38058, acc = 0.50000\n",
            "Epoch [4/5] Step [1516/1610]: discriminator_loss = 0.59975, target_loss = 0.38040, acc = 0.50000\n",
            "Epoch [4/5] Step [1517/1610]: discriminator_loss = 0.73099, target_loss = 0.37280, acc = 0.50000\n",
            "Epoch [4/5] Step [1518/1610]: discriminator_loss = 0.57542, target_loss = 0.38737, acc = 0.50000\n",
            "Epoch [4/5] Step [1519/1610]: discriminator_loss = 0.58985, target_loss = 0.38877, acc = 0.50000\n",
            "Epoch [4/5] Step [1520/1610]: discriminator_loss = 0.57170, target_loss = 0.39445, acc = 0.50000\n",
            "Epoch [4/5] Step [1521/1610]: discriminator_loss = 0.56629, target_loss = 0.39674, acc = 0.50000\n",
            "Epoch [4/5] Step [1522/1610]: discriminator_loss = 0.62305, target_loss = 0.41054, acc = 0.50000\n",
            "Epoch [4/5] Step [1523/1610]: discriminator_loss = 0.54674, target_loss = 0.42738, acc = 0.50000\n",
            "Epoch [4/5] Step [1524/1610]: discriminator_loss = 0.54340, target_loss = 0.39969, acc = 0.50000\n",
            "Epoch [4/5] Step [1525/1610]: discriminator_loss = 0.54497, target_loss = 0.40515, acc = 0.50000\n",
            "Epoch [4/5] Step [1526/1610]: discriminator_loss = 0.53392, target_loss = 0.42966, acc = 0.50000\n",
            "Epoch [4/5] Step [1527/1610]: discriminator_loss = 0.55905, target_loss = 0.41602, acc = 0.50000\n",
            "Epoch [4/5] Step [1528/1610]: discriminator_loss = 0.56318, target_loss = 0.41052, acc = 0.50000\n",
            "Epoch [4/5] Step [1529/1610]: discriminator_loss = 0.53757, target_loss = 0.41552, acc = 0.50000\n",
            "Epoch [4/5] Step [1530/1610]: discriminator_loss = 0.53611, target_loss = 0.40982, acc = 0.50000\n",
            "Epoch [4/5] Step [1531/1610]: discriminator_loss = 0.54211, target_loss = 0.42345, acc = 0.50000\n",
            "Epoch [4/5] Step [1532/1610]: discriminator_loss = 0.54984, target_loss = 0.40901, acc = 0.50000\n",
            "Epoch [4/5] Step [1533/1610]: discriminator_loss = 0.54526, target_loss = 0.41735, acc = 0.50000\n",
            "Epoch [4/5] Step [1534/1610]: discriminator_loss = 0.63309, target_loss = 0.41844, acc = 0.50000\n",
            "Epoch [4/5] Step [1535/1610]: discriminator_loss = 0.53354, target_loss = 0.41661, acc = 0.50000\n",
            "Epoch [4/5] Step [1536/1610]: discriminator_loss = 0.54691, target_loss = 0.41891, acc = 0.50000\n",
            "Epoch [4/5] Step [1537/1610]: discriminator_loss = 0.54227, target_loss = 0.41519, acc = 0.50000\n",
            "Epoch [4/5] Step [1538/1610]: discriminator_loss = 0.54346, target_loss = 0.40442, acc = 0.50000\n",
            "Epoch [4/5] Step [1539/1610]: discriminator_loss = 0.54922, target_loss = 0.40306, acc = 0.50000\n",
            "Epoch [4/5] Step [1540/1610]: discriminator_loss = 0.59394, target_loss = 0.40502, acc = 0.50000\n",
            "Epoch [4/5] Step [1541/1610]: discriminator_loss = 0.55443, target_loss = 0.40922, acc = 0.50000\n",
            "Epoch [4/5] Step [1542/1610]: discriminator_loss = 0.54984, target_loss = 0.41096, acc = 0.50000\n",
            "Epoch [4/5] Step [1543/1610]: discriminator_loss = 0.56615, target_loss = 0.40034, acc = 0.50000\n",
            "Epoch [4/5] Step [1544/1610]: discriminator_loss = 0.54980, target_loss = 0.40559, acc = 0.50000\n",
            "Epoch [4/5] Step [1545/1610]: discriminator_loss = 0.55435, target_loss = 0.40696, acc = 0.50000\n",
            "Epoch [4/5] Step [1546/1610]: discriminator_loss = 0.56597, target_loss = 0.40026, acc = 0.50000\n",
            "Epoch [4/5] Step [1547/1610]: discriminator_loss = 0.58454, target_loss = 0.40552, acc = 0.50000\n",
            "Epoch [4/5] Step [1548/1610]: discriminator_loss = 0.56267, target_loss = 0.39802, acc = 0.50000\n",
            "Epoch [4/5] Step [1549/1610]: discriminator_loss = 0.55900, target_loss = 0.37890, acc = 0.50000\n",
            "Epoch [4/5] Step [1550/1610]: discriminator_loss = 0.57020, target_loss = 0.38959, acc = 0.50000\n",
            "Epoch [4/5] Step [1551/1610]: discriminator_loss = 0.56693, target_loss = 0.40329, acc = 0.50000\n",
            "Epoch [4/5] Step [1552/1610]: discriminator_loss = 0.60590, target_loss = 0.39236, acc = 0.50000\n",
            "Epoch [4/5] Step [1553/1610]: discriminator_loss = 0.56027, target_loss = 0.38752, acc = 0.50000\n",
            "Epoch [4/5] Step [1554/1610]: discriminator_loss = 0.56959, target_loss = 0.38568, acc = 0.50000\n",
            "Epoch [4/5] Step [1555/1610]: discriminator_loss = 0.54334, target_loss = 0.40204, acc = 0.50000\n",
            "Epoch [4/5] Step [1556/1610]: discriminator_loss = 0.56393, target_loss = 0.39405, acc = 0.50000\n",
            "Epoch [4/5] Step [1557/1610]: discriminator_loss = 0.54967, target_loss = 0.40667, acc = 0.50000\n",
            "Epoch [4/5] Step [1558/1610]: discriminator_loss = 0.62800, target_loss = 0.40260, acc = 0.50000\n",
            "Epoch [4/5] Step [1559/1610]: discriminator_loss = 0.56552, target_loss = 0.39237, acc = 0.50000\n",
            "Epoch [4/5] Step [1560/1610]: discriminator_loss = 0.56737, target_loss = 0.38711, acc = 0.50000\n",
            "Epoch [4/5] Step [1561/1610]: discriminator_loss = 0.57365, target_loss = 0.39939, acc = 0.50000\n",
            "Epoch [4/5] Step [1562/1610]: discriminator_loss = 0.55070, target_loss = 0.38045, acc = 0.50000\n",
            "Epoch [4/5] Step [1563/1610]: discriminator_loss = 0.55600, target_loss = 0.38750, acc = 0.50000\n",
            "Epoch [4/5] Step [1564/1610]: discriminator_loss = 0.56469, target_loss = 0.39140, acc = 0.50000\n",
            "Epoch [4/5] Step [1565/1610]: discriminator_loss = 0.54676, target_loss = 0.40672, acc = 0.50000\n",
            "Epoch [4/5] Step [1566/1610]: discriminator_loss = 0.54235, target_loss = 0.40750, acc = 0.50000\n",
            "Epoch [4/5] Step [1567/1610]: discriminator_loss = 0.56054, target_loss = 0.39168, acc = 0.50000\n",
            "Epoch [4/5] Step [1568/1610]: discriminator_loss = 0.55450, target_loss = 0.39960, acc = 0.50000\n",
            "Epoch [4/5] Step [1569/1610]: discriminator_loss = 0.56227, target_loss = 0.39665, acc = 0.50000\n",
            "Epoch [4/5] Step [1570/1610]: discriminator_loss = 0.55779, target_loss = 0.40479, acc = 0.50000\n",
            "Epoch [4/5] Step [1571/1610]: discriminator_loss = 0.55918, target_loss = 0.39353, acc = 0.50000\n",
            "Epoch [4/5] Step [1572/1610]: discriminator_loss = 0.55534, target_loss = 0.39452, acc = 0.50000\n",
            "Epoch [4/5] Step [1573/1610]: discriminator_loss = 0.57676, target_loss = 0.40843, acc = 0.50000\n",
            "Epoch [4/5] Step [1574/1610]: discriminator_loss = 0.56332, target_loss = 0.41047, acc = 0.50000\n",
            "Epoch [4/5] Step [1575/1610]: discriminator_loss = 0.54000, target_loss = 0.40650, acc = 0.50000\n",
            "Epoch [4/5] Step [1576/1610]: discriminator_loss = 0.54637, target_loss = 0.42465, acc = 0.50000\n",
            "Epoch [4/5] Step [1577/1610]: discriminator_loss = 0.53600, target_loss = 0.39798, acc = 0.50000\n",
            "Epoch [4/5] Step [1578/1610]: discriminator_loss = 0.53856, target_loss = 0.40286, acc = 0.50000\n",
            "Epoch [4/5] Step [1579/1610]: discriminator_loss = 0.54410, target_loss = 0.38980, acc = 0.50000\n",
            "Epoch [4/5] Step [1580/1610]: discriminator_loss = 0.58504, target_loss = 0.39421, acc = 0.50000\n",
            "Epoch [4/5] Step [1581/1610]: discriminator_loss = 0.55177, target_loss = 0.40533, acc = 0.50000\n",
            "Epoch [4/5] Step [1582/1610]: discriminator_loss = 0.54046, target_loss = 0.38209, acc = 0.50000\n",
            "Epoch [4/5] Step [1583/1610]: discriminator_loss = 0.56473, target_loss = 0.37229, acc = 0.50000\n",
            "Epoch [4/5] Step [1584/1610]: discriminator_loss = 0.53752, target_loss = 0.41775, acc = 0.50000\n",
            "Epoch [4/5] Step [1585/1610]: discriminator_loss = 0.56661, target_loss = 0.38923, acc = 0.50000\n",
            "Epoch [4/5] Step [1586/1610]: discriminator_loss = 0.55570, target_loss = 0.39790, acc = 0.50000\n",
            "Epoch [4/5] Step [1587/1610]: discriminator_loss = 0.58761, target_loss = 0.39973, acc = 0.50000\n",
            "Epoch [4/5] Step [1588/1610]: discriminator_loss = 0.59050, target_loss = 0.40763, acc = 0.50000\n",
            "Epoch [4/5] Step [1589/1610]: discriminator_loss = 0.54939, target_loss = 0.40449, acc = 0.50000\n",
            "Epoch [4/5] Step [1590/1610]: discriminator_loss = 0.58478, target_loss = 0.41147, acc = 0.50000\n",
            "Epoch [4/5] Step [1591/1610]: discriminator_loss = 0.54747, target_loss = 0.41728, acc = 0.50000\n",
            "Epoch [4/5] Step [1592/1610]: discriminator_loss = 0.54933, target_loss = 0.40783, acc = 0.50000\n",
            "Epoch [4/5] Step [1593/1610]: discriminator_loss = 0.56835, target_loss = 0.40098, acc = 0.50000\n",
            "Epoch [4/5] Step [1594/1610]: discriminator_loss = 0.56371, target_loss = 0.40964, acc = 0.50000\n",
            "Epoch [4/5] Step [1595/1610]: discriminator_loss = 0.56712, target_loss = 0.41261, acc = 0.50000\n",
            "Epoch [4/5] Step [1596/1610]: discriminator_loss = 0.55008, target_loss = 0.41552, acc = 0.50000\n",
            "Epoch [4/5] Step [1597/1610]: discriminator_loss = 0.53739, target_loss = 0.42798, acc = 0.50000\n",
            "Epoch [4/5] Step [1598/1610]: discriminator_loss = 0.53189, target_loss = 0.39796, acc = 0.50000\n",
            "Epoch [4/5] Step [1599/1610]: discriminator_loss = 0.56170, target_loss = 0.40486, acc = 0.50000\n",
            "Epoch [4/5] Step [1600/1610]: discriminator_loss = 0.55678, target_loss = 0.41501, acc = 0.50000\n",
            "Epoch [4/5] Step [1601/1610]: discriminator_loss = 0.55499, target_loss = 0.41218, acc = 0.50000\n",
            "Epoch [4/5] Step [1602/1610]: discriminator_loss = 0.54831, target_loss = 0.41433, acc = 0.50000\n",
            "Epoch [4/5] Step [1603/1610]: discriminator_loss = 0.54803, target_loss = 0.39726, acc = 0.50000\n",
            "Epoch [4/5] Step [1604/1610]: discriminator_loss = 0.55239, target_loss = 0.40697, acc = 0.50000\n",
            "Epoch [4/5] Step [1605/1610]: discriminator_loss = 0.55615, target_loss = 0.39142, acc = 0.50000\n",
            "Epoch [4/5] Step [1606/1610]: discriminator_loss = 0.55653, target_loss = 0.39451, acc = 0.50000\n",
            "Epoch [4/5] Step [1607/1610]: discriminator_loss = 0.55754, target_loss = 0.41530, acc = 0.50000\n",
            "Epoch [4/5] Step [1608/1610]: discriminator_loss = 0.53915, target_loss = 0.41171, acc = 0.50000\n",
            "Epoch [4/5] Step [1609/1610]: discriminator_loss = 0.56859, target_loss = 0.40850, acc = 0.50000\n",
            "Epoch [4/5] Step [1610/1610]: discriminator_loss = 0.55454, target_loss = 0.40862, acc = 0.50000\n",
            "Epoch [5/5] Step [1/1610]: discriminator_loss = 0.55273, target_loss = 0.41158, acc = 0.50000\n",
            "Epoch [5/5] Step [2/1610]: discriminator_loss = 0.55311, target_loss = 0.37808, acc = 0.50000\n",
            "Epoch [5/5] Step [3/1610]: discriminator_loss = 0.55317, target_loss = 0.40386, acc = 0.50000\n",
            "Epoch [5/5] Step [4/1610]: discriminator_loss = 0.54500, target_loss = 0.39633, acc = 0.50000\n",
            "Epoch [5/5] Step [5/1610]: discriminator_loss = 0.56687, target_loss = 0.38406, acc = 0.50000\n",
            "Epoch [5/5] Step [6/1610]: discriminator_loss = 0.55430, target_loss = 0.39668, acc = 0.50000\n",
            "Epoch [5/5] Step [7/1610]: discriminator_loss = 0.58512, target_loss = 0.40595, acc = 0.50000\n",
            "Epoch [5/5] Step [8/1610]: discriminator_loss = 0.56581, target_loss = 0.39521, acc = 0.50000\n",
            "Epoch [5/5] Step [9/1610]: discriminator_loss = 0.56308, target_loss = 0.39195, acc = 0.50000\n",
            "Epoch [5/5] Step [10/1610]: discriminator_loss = 0.56537, target_loss = 0.40542, acc = 0.50000\n",
            "Epoch [5/5] Step [11/1610]: discriminator_loss = 0.54868, target_loss = 0.39367, acc = 0.50000\n",
            "Epoch [5/5] Step [12/1610]: discriminator_loss = 0.56171, target_loss = 0.40117, acc = 0.50000\n",
            "Epoch [5/5] Step [13/1610]: discriminator_loss = 0.56361, target_loss = 0.39323, acc = 0.50000\n",
            "Epoch [5/5] Step [14/1610]: discriminator_loss = 0.54390, target_loss = 0.40888, acc = 0.50000\n",
            "Epoch [5/5] Step [15/1610]: discriminator_loss = 0.56550, target_loss = 0.38811, acc = 0.50000\n",
            "Epoch [5/5] Step [16/1610]: discriminator_loss = 0.55419, target_loss = 0.41071, acc = 0.50000\n",
            "Epoch [5/5] Step [17/1610]: discriminator_loss = 0.54715, target_loss = 0.39267, acc = 0.50000\n",
            "Epoch [5/5] Step [18/1610]: discriminator_loss = 0.63289, target_loss = 0.38910, acc = 0.50000\n",
            "Epoch [5/5] Step [19/1610]: discriminator_loss = 0.55366, target_loss = 0.40024, acc = 0.50000\n",
            "Epoch [5/5] Step [20/1610]: discriminator_loss = 0.53713, target_loss = 0.42195, acc = 0.50000\n",
            "Epoch [5/5] Step [21/1610]: discriminator_loss = 0.53800, target_loss = 0.39078, acc = 0.50000\n",
            "Epoch [5/5] Step [22/1610]: discriminator_loss = 0.54847, target_loss = 0.40064, acc = 0.50000\n",
            "Epoch [5/5] Step [23/1610]: discriminator_loss = 0.56415, target_loss = 0.41813, acc = 0.50000\n",
            "Epoch [5/5] Step [24/1610]: discriminator_loss = 0.52908, target_loss = 0.43895, acc = 0.50000\n",
            "Epoch [5/5] Step [25/1610]: discriminator_loss = 0.56791, target_loss = 0.41356, acc = 0.50000\n",
            "Epoch [5/5] Step [26/1610]: discriminator_loss = 0.54031, target_loss = 0.37251, acc = 0.50000\n",
            "Epoch [5/5] Step [27/1610]: discriminator_loss = 0.59558, target_loss = 0.42009, acc = 0.50000\n",
            "Epoch [5/5] Step [28/1610]: discriminator_loss = 0.53937, target_loss = 0.39081, acc = 0.50000\n",
            "Epoch [5/5] Step [29/1610]: discriminator_loss = 0.53285, target_loss = 0.42711, acc = 0.50000\n",
            "Epoch [5/5] Step [30/1610]: discriminator_loss = 0.54027, target_loss = 0.42480, acc = 0.50000\n",
            "Epoch [5/5] Step [31/1610]: discriminator_loss = 0.54641, target_loss = 0.41548, acc = 0.50000\n",
            "Epoch [5/5] Step [32/1610]: discriminator_loss = 0.54223, target_loss = 0.42415, acc = 0.50000\n",
            "Epoch [5/5] Step [33/1610]: discriminator_loss = 0.53598, target_loss = 0.41799, acc = 0.50000\n",
            "Epoch [5/5] Step [34/1610]: discriminator_loss = 0.55030, target_loss = 0.41320, acc = 0.50000\n",
            "Epoch [5/5] Step [35/1610]: discriminator_loss = 0.54598, target_loss = 0.41104, acc = 0.50000\n",
            "Epoch [5/5] Step [36/1610]: discriminator_loss = 0.53888, target_loss = 0.42500, acc = 0.50000\n",
            "Epoch [5/5] Step [37/1610]: discriminator_loss = 0.55975, target_loss = 0.40941, acc = 0.50000\n",
            "Epoch [5/5] Step [38/1610]: discriminator_loss = 0.55454, target_loss = 0.40934, acc = 0.50000\n",
            "Epoch [5/5] Step [39/1610]: discriminator_loss = 0.56649, target_loss = 0.40624, acc = 0.50000\n",
            "Epoch [5/5] Step [40/1610]: discriminator_loss = 0.54330, target_loss = 0.41859, acc = 0.50000\n",
            "Epoch [5/5] Step [41/1610]: discriminator_loss = 0.56403, target_loss = 0.39511, acc = 0.50000\n",
            "Epoch [5/5] Step [42/1610]: discriminator_loss = 0.55077, target_loss = 0.39174, acc = 0.50000\n",
            "Epoch [5/5] Step [43/1610]: discriminator_loss = 0.57278, target_loss = 0.38735, acc = 0.50000\n",
            "Epoch [5/5] Step [44/1610]: discriminator_loss = 0.56805, target_loss = 0.39731, acc = 0.50000\n",
            "Epoch [5/5] Step [45/1610]: discriminator_loss = 0.58341, target_loss = 0.39261, acc = 0.50000\n",
            "Epoch [5/5] Step [46/1610]: discriminator_loss = 0.56121, target_loss = 0.39311, acc = 0.50000\n",
            "Epoch [5/5] Step [47/1610]: discriminator_loss = 0.57378, target_loss = 0.39401, acc = 0.50000\n",
            "Epoch [5/5] Step [48/1610]: discriminator_loss = 0.58413, target_loss = 0.38358, acc = 0.50000\n",
            "Epoch [5/5] Step [49/1610]: discriminator_loss = 0.57903, target_loss = 0.37138, acc = 0.50000\n",
            "Epoch [5/5] Step [50/1610]: discriminator_loss = 0.56919, target_loss = 0.40718, acc = 0.50000\n",
            "Epoch [5/5] Step [51/1610]: discriminator_loss = 0.63183, target_loss = 0.39110, acc = 0.50000\n",
            "Epoch [5/5] Step [52/1610]: discriminator_loss = 0.58741, target_loss = 0.39051, acc = 0.50000\n",
            "Epoch [5/5] Step [53/1610]: discriminator_loss = 0.57152, target_loss = 0.37110, acc = 0.50000\n",
            "Epoch [5/5] Step [54/1610]: discriminator_loss = 0.56153, target_loss = 0.39342, acc = 0.50000\n",
            "Epoch [5/5] Step [55/1610]: discriminator_loss = 0.56511, target_loss = 0.38609, acc = 0.50000\n",
            "Epoch [5/5] Step [56/1610]: discriminator_loss = 0.55560, target_loss = 0.40089, acc = 0.50000\n",
            "Epoch [5/5] Step [57/1610]: discriminator_loss = 0.56359, target_loss = 0.39530, acc = 0.50000\n",
            "Epoch [5/5] Step [58/1610]: discriminator_loss = 0.56146, target_loss = 0.41361, acc = 0.50000\n",
            "Epoch [5/5] Step [59/1610]: discriminator_loss = 0.56029, target_loss = 0.39544, acc = 0.50000\n",
            "Epoch [5/5] Step [60/1610]: discriminator_loss = 0.55057, target_loss = 0.39762, acc = 0.50000\n",
            "Epoch [5/5] Step [61/1610]: discriminator_loss = 0.53832, target_loss = 0.40126, acc = 0.50000\n",
            "Epoch [5/5] Step [62/1610]: discriminator_loss = 0.54794, target_loss = 0.40981, acc = 0.50000\n",
            "Epoch [5/5] Step [63/1610]: discriminator_loss = 0.54617, target_loss = 0.40680, acc = 0.50000\n",
            "Epoch [5/5] Step [64/1610]: discriminator_loss = 0.55347, target_loss = 0.41686, acc = 0.50000\n",
            "Epoch [5/5] Step [65/1610]: discriminator_loss = 0.54696, target_loss = 0.40285, acc = 0.50000\n",
            "Epoch [5/5] Step [66/1610]: discriminator_loss = 0.54317, target_loss = 0.40964, acc = 0.50000\n",
            "Epoch [5/5] Step [67/1610]: discriminator_loss = 0.55687, target_loss = 0.41857, acc = 0.50000\n",
            "Epoch [5/5] Step [68/1610]: discriminator_loss = 0.55301, target_loss = 0.40765, acc = 0.50000\n",
            "Epoch [5/5] Step [69/1610]: discriminator_loss = 0.56405, target_loss = 0.40881, acc = 0.50000\n",
            "Epoch [5/5] Step [70/1610]: discriminator_loss = 0.54779, target_loss = 0.38437, acc = 0.50000\n",
            "Epoch [5/5] Step [71/1610]: discriminator_loss = 0.54901, target_loss = 0.40993, acc = 0.50000\n",
            "Epoch [5/5] Step [72/1610]: discriminator_loss = 0.56401, target_loss = 0.41396, acc = 0.50000\n",
            "Epoch [5/5] Step [73/1610]: discriminator_loss = 0.56236, target_loss = 0.40036, acc = 0.50000\n",
            "Epoch [5/5] Step [74/1610]: discriminator_loss = 0.57781, target_loss = 0.38693, acc = 0.50000\n",
            "Epoch [5/5] Step [75/1610]: discriminator_loss = 0.60035, target_loss = 0.38625, acc = 0.50000\n",
            "Epoch [5/5] Step [76/1610]: discriminator_loss = 0.57127, target_loss = 0.35176, acc = 0.50000\n",
            "Epoch [5/5] Step [77/1610]: discriminator_loss = 0.55803, target_loss = 0.40687, acc = 0.50000\n",
            "Epoch [5/5] Step [78/1610]: discriminator_loss = 0.56853, target_loss = 0.38463, acc = 0.50000\n",
            "Epoch [5/5] Step [79/1610]: discriminator_loss = 0.57980, target_loss = 0.39229, acc = 0.50000\n",
            "Epoch [5/5] Step [80/1610]: discriminator_loss = 0.55683, target_loss = 0.38854, acc = 0.50000\n",
            "Epoch [5/5] Step [81/1610]: discriminator_loss = 0.55380, target_loss = 0.42596, acc = 0.50000\n",
            "Epoch [5/5] Step [82/1610]: discriminator_loss = 0.53565, target_loss = 0.38386, acc = 0.50000\n",
            "Epoch [5/5] Step [83/1610]: discriminator_loss = 0.54233, target_loss = 0.41596, acc = 0.50000\n",
            "Epoch [5/5] Step [84/1610]: discriminator_loss = 0.53893, target_loss = 0.39957, acc = 0.50000\n",
            "Epoch [5/5] Step [85/1610]: discriminator_loss = 0.56489, target_loss = 0.41643, acc = 0.50000\n",
            "Epoch [5/5] Step [86/1610]: discriminator_loss = 0.56263, target_loss = 0.39713, acc = 0.50000\n",
            "Epoch [5/5] Step [87/1610]: discriminator_loss = 0.52940, target_loss = 0.43253, acc = 0.50000\n",
            "Epoch [5/5] Step [88/1610]: discriminator_loss = 0.53790, target_loss = 0.40554, acc = 0.50000\n",
            "Epoch [5/5] Step [89/1610]: discriminator_loss = 0.63904, target_loss = 0.39789, acc = 0.50000\n",
            "Epoch [5/5] Step [90/1610]: discriminator_loss = 0.54049, target_loss = 0.41330, acc = 0.50000\n",
            "Epoch [5/5] Step [91/1610]: discriminator_loss = 0.56071, target_loss = 0.42325, acc = 0.50000\n",
            "Epoch [5/5] Step [92/1610]: discriminator_loss = 0.53893, target_loss = 0.43933, acc = 0.50000\n",
            "Epoch [5/5] Step [93/1610]: discriminator_loss = 0.53279, target_loss = 0.41685, acc = 0.50000\n",
            "Epoch [5/5] Step [94/1610]: discriminator_loss = 0.55720, target_loss = 0.41929, acc = 0.50000\n",
            "Epoch [5/5] Step [95/1610]: discriminator_loss = 0.53878, target_loss = 0.42679, acc = 0.50000\n",
            "Epoch [5/5] Step [96/1610]: discriminator_loss = 0.54673, target_loss = 0.40522, acc = 0.50000\n",
            "Epoch [5/5] Step [97/1610]: discriminator_loss = 0.57687, target_loss = 0.41259, acc = 0.50000\n",
            "Epoch [5/5] Step [98/1610]: discriminator_loss = 0.53250, target_loss = 0.38876, acc = 0.50000\n",
            "Epoch [5/5] Step [99/1610]: discriminator_loss = 0.54807, target_loss = 0.42638, acc = 0.50000\n",
            "Epoch [5/5] Step [100/1610]: discriminator_loss = 0.57387, target_loss = 0.40480, acc = 0.50000\n",
            "Epoch [5/5] Step [101/1610]: discriminator_loss = 0.54759, target_loss = 0.39513, acc = 0.50000\n",
            "Epoch [5/5] Step [102/1610]: discriminator_loss = 0.57023, target_loss = 0.39153, acc = 0.50000\n",
            "Epoch [5/5] Step [103/1610]: discriminator_loss = 0.56766, target_loss = 0.39788, acc = 0.50000\n",
            "Epoch [5/5] Step [104/1610]: discriminator_loss = 0.55485, target_loss = 0.40108, acc = 0.50000\n",
            "Epoch [5/5] Step [105/1610]: discriminator_loss = 0.56815, target_loss = 0.39046, acc = 0.50000\n",
            "Epoch [5/5] Step [106/1610]: discriminator_loss = 0.58436, target_loss = 0.40081, acc = 0.50000\n",
            "Epoch [5/5] Step [107/1610]: discriminator_loss = 0.56421, target_loss = 0.40318, acc = 0.50000\n",
            "Epoch [5/5] Step [108/1610]: discriminator_loss = 0.55240, target_loss = 0.38967, acc = 0.50000\n",
            "Epoch [5/5] Step [109/1610]: discriminator_loss = 0.63394, target_loss = 0.39722, acc = 0.50000\n",
            "Epoch [5/5] Step [110/1610]: discriminator_loss = 0.61933, target_loss = 0.38476, acc = 0.50000\n",
            "Epoch [5/5] Step [111/1610]: discriminator_loss = 0.57903, target_loss = 0.40433, acc = 0.50000\n",
            "Epoch [5/5] Step [112/1610]: discriminator_loss = 0.55484, target_loss = 0.38405, acc = 0.50000\n",
            "Epoch [5/5] Step [113/1610]: discriminator_loss = 0.58192, target_loss = 0.40961, acc = 0.50000\n",
            "Epoch [5/5] Step [114/1610]: discriminator_loss = 0.56423, target_loss = 0.39884, acc = 0.50000\n",
            "Epoch [5/5] Step [115/1610]: discriminator_loss = 0.55674, target_loss = 0.41163, acc = 0.50000\n",
            "Epoch [5/5] Step [116/1610]: discriminator_loss = 0.55485, target_loss = 0.41862, acc = 0.50000\n",
            "Epoch [5/5] Step [117/1610]: discriminator_loss = 0.59298, target_loss = 0.37129, acc = 0.50000\n",
            "Epoch [5/5] Step [118/1610]: discriminator_loss = 0.56780, target_loss = 0.41345, acc = 0.50000\n",
            "Epoch [5/5] Step [119/1610]: discriminator_loss = 0.54772, target_loss = 0.39521, acc = 0.50000\n",
            "Epoch [5/5] Step [120/1610]: discriminator_loss = 0.54962, target_loss = 0.41255, acc = 0.50000\n",
            "Epoch [5/5] Step [121/1610]: discriminator_loss = 0.54540, target_loss = 0.39430, acc = 0.50000\n",
            "Epoch [5/5] Step [122/1610]: discriminator_loss = 0.56148, target_loss = 0.41147, acc = 0.50000\n",
            "Epoch [5/5] Step [123/1610]: discriminator_loss = 0.55252, target_loss = 0.36934, acc = 0.50000\n",
            "Epoch [5/5] Step [124/1610]: discriminator_loss = 0.55066, target_loss = 0.40236, acc = 0.50000\n",
            "Epoch [5/5] Step [125/1610]: discriminator_loss = 0.54368, target_loss = 0.39691, acc = 0.50000\n",
            "Epoch [5/5] Step [126/1610]: discriminator_loss = 0.59369, target_loss = 0.39641, acc = 0.50000\n",
            "Epoch [5/5] Step [127/1610]: discriminator_loss = 0.52617, target_loss = 0.41431, acc = 0.53125\n",
            "Epoch [5/5] Step [128/1610]: discriminator_loss = 0.55623, target_loss = 0.41567, acc = 0.50000\n",
            "Epoch [5/5] Step [129/1610]: discriminator_loss = 0.56124, target_loss = 0.38462, acc = 0.50000\n",
            "Epoch [5/5] Step [130/1610]: discriminator_loss = 0.54067, target_loss = 0.40712, acc = 0.50000\n",
            "Epoch [5/5] Step [131/1610]: discriminator_loss = 0.53897, target_loss = 0.39986, acc = 0.50000\n",
            "Epoch [5/5] Step [132/1610]: discriminator_loss = 0.52746, target_loss = 0.41254, acc = 0.50000\n",
            "Epoch [5/5] Step [133/1610]: discriminator_loss = 0.55148, target_loss = 0.41342, acc = 0.50000\n",
            "Epoch [5/5] Step [134/1610]: discriminator_loss = 0.55150, target_loss = 0.41899, acc = 0.50000\n",
            "Epoch [5/5] Step [135/1610]: discriminator_loss = 0.56506, target_loss = 0.42333, acc = 0.50000\n",
            "Epoch [5/5] Step [136/1610]: discriminator_loss = 0.54491, target_loss = 0.41189, acc = 0.50000\n",
            "Epoch [5/5] Step [137/1610]: discriminator_loss = 0.55475, target_loss = 0.40315, acc = 0.50000\n",
            "Epoch [5/5] Step [138/1610]: discriminator_loss = 0.54741, target_loss = 0.41196, acc = 0.50000\n",
            "Epoch [5/5] Step [139/1610]: discriminator_loss = 0.56529, target_loss = 0.40008, acc = 0.50000\n",
            "Epoch [5/5] Step [140/1610]: discriminator_loss = 0.58530, target_loss = 0.42370, acc = 0.50000\n",
            "Epoch [5/5] Step [141/1610]: discriminator_loss = 0.54063, target_loss = 0.40758, acc = 0.50000\n",
            "Epoch [5/5] Step [142/1610]: discriminator_loss = 0.55770, target_loss = 0.39601, acc = 0.50000\n",
            "Epoch [5/5] Step [143/1610]: discriminator_loss = 0.54102, target_loss = 0.40791, acc = 0.50000\n",
            "Epoch [5/5] Step [144/1610]: discriminator_loss = 0.54071, target_loss = 0.41104, acc = 0.50000\n",
            "Epoch [5/5] Step [145/1610]: discriminator_loss = 0.54928, target_loss = 0.41932, acc = 0.50000\n",
            "Epoch [5/5] Step [146/1610]: discriminator_loss = 0.65047, target_loss = 0.38271, acc = 0.50000\n",
            "Epoch [5/5] Step [147/1610]: discriminator_loss = 0.57117, target_loss = 0.37270, acc = 0.50000\n",
            "Epoch [5/5] Step [148/1610]: discriminator_loss = 0.59244, target_loss = 0.40833, acc = 0.50000\n",
            "Epoch [5/5] Step [149/1610]: discriminator_loss = 0.54708, target_loss = 0.42012, acc = 0.50000\n",
            "Epoch [5/5] Step [150/1610]: discriminator_loss = 0.55605, target_loss = 0.40947, acc = 0.50000\n",
            "Epoch [5/5] Step [151/1610]: discriminator_loss = 0.54798, target_loss = 0.41732, acc = 0.50000\n",
            "Epoch [5/5] Step [152/1610]: discriminator_loss = 0.55373, target_loss = 0.41488, acc = 0.50000\n",
            "Epoch [5/5] Step [153/1610]: discriminator_loss = 0.55064, target_loss = 0.42385, acc = 0.50000\n",
            "Epoch [5/5] Step [154/1610]: discriminator_loss = 0.53179, target_loss = 0.41041, acc = 0.50000\n",
            "Epoch [5/5] Step [155/1610]: discriminator_loss = 0.54157, target_loss = 0.42529, acc = 0.50000\n",
            "Epoch [5/5] Step [156/1610]: discriminator_loss = 0.56825, target_loss = 0.41185, acc = 0.50000\n",
            "Epoch [5/5] Step [157/1610]: discriminator_loss = 0.54410, target_loss = 0.43114, acc = 0.50000\n",
            "Epoch [5/5] Step [158/1610]: discriminator_loss = 0.56560, target_loss = 0.41561, acc = 0.50000\n",
            "Epoch [5/5] Step [159/1610]: discriminator_loss = 0.54132, target_loss = 0.40605, acc = 0.50000\n",
            "Epoch [5/5] Step [160/1610]: discriminator_loss = 0.55082, target_loss = 0.38771, acc = 0.50000\n",
            "Epoch [5/5] Step [161/1610]: discriminator_loss = 0.54629, target_loss = 0.39192, acc = 0.50000\n",
            "Epoch [5/5] Step [162/1610]: discriminator_loss = 0.57625, target_loss = 0.39373, acc = 0.50000\n",
            "Epoch [5/5] Step [163/1610]: discriminator_loss = 0.55677, target_loss = 0.37945, acc = 0.50000\n",
            "Epoch [5/5] Step [164/1610]: discriminator_loss = 0.56984, target_loss = 0.38160, acc = 0.50000\n",
            "Epoch [5/5] Step [165/1610]: discriminator_loss = 0.57025, target_loss = 0.38347, acc = 0.50000\n",
            "Epoch [5/5] Step [166/1610]: discriminator_loss = 0.58950, target_loss = 0.37646, acc = 0.50000\n",
            "Epoch [5/5] Step [167/1610]: discriminator_loss = 0.60240, target_loss = 0.37704, acc = 0.50000\n",
            "Epoch [5/5] Step [168/1610]: discriminator_loss = 0.57574, target_loss = 0.34727, acc = 0.50000\n",
            "Epoch [5/5] Step [169/1610]: discriminator_loss = 0.57395, target_loss = 0.37857, acc = 0.50000\n",
            "Epoch [5/5] Step [170/1610]: discriminator_loss = 0.55238, target_loss = 0.39455, acc = 0.50000\n",
            "Epoch [5/5] Step [171/1610]: discriminator_loss = 0.59475, target_loss = 0.39686, acc = 0.50000\n",
            "Epoch [5/5] Step [172/1610]: discriminator_loss = 0.56574, target_loss = 0.37606, acc = 0.50000\n",
            "Epoch [5/5] Step [173/1610]: discriminator_loss = 0.58858, target_loss = 0.37032, acc = 0.50000\n",
            "Epoch [5/5] Step [174/1610]: discriminator_loss = 0.55148, target_loss = 0.40309, acc = 0.50000\n",
            "Epoch [5/5] Step [175/1610]: discriminator_loss = 0.56120, target_loss = 0.39126, acc = 0.50000\n",
            "Epoch [5/5] Step [176/1610]: discriminator_loss = 0.54908, target_loss = 0.41529, acc = 0.50000\n",
            "Epoch [5/5] Step [177/1610]: discriminator_loss = 0.54147, target_loss = 0.41836, acc = 0.50000\n",
            "Epoch [5/5] Step [178/1610]: discriminator_loss = 0.54960, target_loss = 0.41529, acc = 0.50000\n",
            "Epoch [5/5] Step [179/1610]: discriminator_loss = 0.54533, target_loss = 0.43308, acc = 0.50000\n",
            "Epoch [5/5] Step [180/1610]: discriminator_loss = 0.54463, target_loss = 0.37405, acc = 0.50000\n",
            "Epoch [5/5] Step [181/1610]: discriminator_loss = 0.56807, target_loss = 0.41916, acc = 0.50000\n",
            "Epoch [5/5] Step [182/1610]: discriminator_loss = 0.54240, target_loss = 0.41245, acc = 0.50000\n",
            "Epoch [5/5] Step [183/1610]: discriminator_loss = 0.54845, target_loss = 0.38349, acc = 0.50000\n",
            "Epoch [5/5] Step [184/1610]: discriminator_loss = 0.55157, target_loss = 0.37633, acc = 0.50000\n",
            "Epoch [5/5] Step [185/1610]: discriminator_loss = 0.57557, target_loss = 0.40559, acc = 0.50000\n",
            "Epoch [5/5] Step [186/1610]: discriminator_loss = 0.55165, target_loss = 0.40564, acc = 0.50000\n",
            "Epoch [5/5] Step [187/1610]: discriminator_loss = 0.58158, target_loss = 0.37424, acc = 0.50000\n",
            "Epoch [5/5] Step [188/1610]: discriminator_loss = 0.55131, target_loss = 0.42849, acc = 0.50000\n",
            "Epoch [5/5] Step [189/1610]: discriminator_loss = 0.62265, target_loss = 0.41551, acc = 0.50000\n",
            "Epoch [5/5] Step [190/1610]: discriminator_loss = 0.55971, target_loss = 0.40138, acc = 0.50000\n",
            "Epoch [5/5] Step [191/1610]: discriminator_loss = 0.55468, target_loss = 0.42130, acc = 0.50000\n",
            "Epoch [5/5] Step [192/1610]: discriminator_loss = 0.52724, target_loss = 0.42450, acc = 0.50000\n",
            "Epoch [5/5] Step [193/1610]: discriminator_loss = 0.53850, target_loss = 0.42025, acc = 0.50000\n",
            "Epoch [5/5] Step [194/1610]: discriminator_loss = 0.52631, target_loss = 0.42154, acc = 0.50000\n",
            "Epoch [5/5] Step [195/1610]: discriminator_loss = 0.54412, target_loss = 0.43153, acc = 0.50000\n",
            "Epoch [5/5] Step [196/1610]: discriminator_loss = 0.58080, target_loss = 0.42424, acc = 0.50000\n",
            "Epoch [5/5] Step [197/1610]: discriminator_loss = 0.55111, target_loss = 0.40767, acc = 0.50000\n",
            "Epoch [5/5] Step [198/1610]: discriminator_loss = 0.57024, target_loss = 0.39277, acc = 0.50000\n",
            "Epoch [5/5] Step [199/1610]: discriminator_loss = 0.56576, target_loss = 0.39109, acc = 0.50000\n",
            "Epoch [5/5] Step [200/1610]: discriminator_loss = 0.55271, target_loss = 0.38893, acc = 0.50000\n",
            "Epoch [5/5] Step [201/1610]: discriminator_loss = 0.56058, target_loss = 0.39579, acc = 0.50000\n",
            "Epoch [5/5] Step [202/1610]: discriminator_loss = 0.59074, target_loss = 0.38621, acc = 0.50000\n",
            "Epoch [5/5] Step [203/1610]: discriminator_loss = 0.57412, target_loss = 0.40045, acc = 0.50000\n",
            "Epoch [5/5] Step [204/1610]: discriminator_loss = 0.55284, target_loss = 0.38309, acc = 0.50000\n",
            "Epoch [5/5] Step [205/1610]: discriminator_loss = 0.58048, target_loss = 0.39318, acc = 0.50000\n",
            "Epoch [5/5] Step [206/1610]: discriminator_loss = 0.57510, target_loss = 0.37656, acc = 0.50000\n",
            "Epoch [5/5] Step [207/1610]: discriminator_loss = 0.57958, target_loss = 0.37263, acc = 0.50000\n",
            "Epoch [5/5] Step [208/1610]: discriminator_loss = 0.58682, target_loss = 0.38419, acc = 0.50000\n",
            "Epoch [5/5] Step [209/1610]: discriminator_loss = 0.60574, target_loss = 0.38249, acc = 0.50000\n",
            "Epoch [5/5] Step [210/1610]: discriminator_loss = 0.56858, target_loss = 0.37721, acc = 0.50000\n",
            "Epoch [5/5] Step [211/1610]: discriminator_loss = 0.58503, target_loss = 0.39284, acc = 0.50000\n",
            "Epoch [5/5] Step [212/1610]: discriminator_loss = 0.60253, target_loss = 0.37741, acc = 0.50000\n",
            "Epoch [5/5] Step [213/1610]: discriminator_loss = 0.56158, target_loss = 0.37544, acc = 0.50000\n",
            "Epoch [5/5] Step [214/1610]: discriminator_loss = 0.56259, target_loss = 0.38391, acc = 0.50000\n",
            "Epoch [5/5] Step [215/1610]: discriminator_loss = 0.56012, target_loss = 0.39227, acc = 0.50000\n",
            "Epoch [5/5] Step [216/1610]: discriminator_loss = 0.56841, target_loss = 0.39057, acc = 0.50000\n",
            "Epoch [5/5] Step [217/1610]: discriminator_loss = 0.59639, target_loss = 0.40367, acc = 0.50000\n",
            "Epoch [5/5] Step [218/1610]: discriminator_loss = 0.57942, target_loss = 0.39764, acc = 0.50000\n",
            "Epoch [5/5] Step [219/1610]: discriminator_loss = 0.56108, target_loss = 0.42135, acc = 0.50000\n",
            "Epoch [5/5] Step [220/1610]: discriminator_loss = 0.56208, target_loss = 0.40146, acc = 0.50000\n",
            "Epoch [5/5] Step [221/1610]: discriminator_loss = 0.56370, target_loss = 0.40829, acc = 0.50000\n",
            "Epoch [5/5] Step [222/1610]: discriminator_loss = 0.54947, target_loss = 0.41222, acc = 0.50000\n",
            "Epoch [5/5] Step [223/1610]: discriminator_loss = 0.54041, target_loss = 0.39691, acc = 0.50000\n",
            "Epoch [5/5] Step [224/1610]: discriminator_loss = 0.53755, target_loss = 0.40537, acc = 0.50000\n",
            "Epoch [5/5] Step [225/1610]: discriminator_loss = 0.56995, target_loss = 0.38977, acc = 0.50000\n",
            "Epoch [5/5] Step [226/1610]: discriminator_loss = 0.55846, target_loss = 0.38394, acc = 0.50000\n",
            "Epoch [5/5] Step [227/1610]: discriminator_loss = 0.54999, target_loss = 0.41711, acc = 0.50000\n",
            "Epoch [5/5] Step [228/1610]: discriminator_loss = 0.54814, target_loss = 0.40005, acc = 0.50000\n",
            "Epoch [5/5] Step [229/1610]: discriminator_loss = 0.55726, target_loss = 0.41288, acc = 0.50000\n",
            "Epoch [5/5] Step [230/1610]: discriminator_loss = 0.65228, target_loss = 0.39956, acc = 0.50000\n",
            "Epoch [5/5] Step [231/1610]: discriminator_loss = 0.56205, target_loss = 0.40894, acc = 0.50000\n",
            "Epoch [5/5] Step [232/1610]: discriminator_loss = 0.57867, target_loss = 0.43794, acc = 0.50000\n",
            "Epoch [5/5] Step [233/1610]: discriminator_loss = 0.55159, target_loss = 0.41740, acc = 0.50000\n",
            "Epoch [5/5] Step [234/1610]: discriminator_loss = 0.55696, target_loss = 0.40389, acc = 0.50000\n",
            "Epoch [5/5] Step [235/1610]: discriminator_loss = 0.56185, target_loss = 0.40462, acc = 0.50000\n",
            "Epoch [5/5] Step [236/1610]: discriminator_loss = 0.55961, target_loss = 0.40871, acc = 0.50000\n",
            "Epoch [5/5] Step [237/1610]: discriminator_loss = 0.56734, target_loss = 0.40626, acc = 0.50000\n",
            "Epoch [5/5] Step [238/1610]: discriminator_loss = 0.53837, target_loss = 0.41310, acc = 0.50000\n",
            "Epoch [5/5] Step [239/1610]: discriminator_loss = 0.56641, target_loss = 0.40427, acc = 0.50000\n",
            "Epoch [5/5] Step [240/1610]: discriminator_loss = 0.57523, target_loss = 0.40626, acc = 0.50000\n",
            "Epoch [5/5] Step [241/1610]: discriminator_loss = 0.56337, target_loss = 0.41121, acc = 0.50000\n",
            "Epoch [5/5] Step [242/1610]: discriminator_loss = 0.54864, target_loss = 0.41077, acc = 0.50000\n",
            "Epoch [5/5] Step [243/1610]: discriminator_loss = 0.54237, target_loss = 0.39833, acc = 0.50000\n",
            "Epoch [5/5] Step [244/1610]: discriminator_loss = 0.56747, target_loss = 0.39248, acc = 0.50000\n",
            "Epoch [5/5] Step [245/1610]: discriminator_loss = 0.55832, target_loss = 0.40151, acc = 0.50000\n",
            "Epoch [5/5] Step [246/1610]: discriminator_loss = 0.57413, target_loss = 0.40201, acc = 0.50000\n",
            "Epoch [5/5] Step [247/1610]: discriminator_loss = 0.55950, target_loss = 0.40422, acc = 0.50000\n",
            "Epoch [5/5] Step [248/1610]: discriminator_loss = 0.56975, target_loss = 0.41310, acc = 0.50000\n",
            "Epoch [5/5] Step [249/1610]: discriminator_loss = 0.54298, target_loss = 0.41019, acc = 0.50000\n",
            "Epoch [5/5] Step [250/1610]: discriminator_loss = 0.56235, target_loss = 0.40749, acc = 0.50000\n",
            "Epoch [5/5] Step [251/1610]: discriminator_loss = 0.53284, target_loss = 0.39118, acc = 0.50000\n",
            "Epoch [5/5] Step [252/1610]: discriminator_loss = 0.56321, target_loss = 0.39310, acc = 0.50000\n",
            "Epoch [5/5] Step [253/1610]: discriminator_loss = 0.55873, target_loss = 0.38360, acc = 0.50000\n",
            "Epoch [5/5] Step [254/1610]: discriminator_loss = 0.63314, target_loss = 0.40347, acc = 0.50000\n",
            "Epoch [5/5] Step [255/1610]: discriminator_loss = 0.55301, target_loss = 0.40855, acc = 0.50000\n",
            "Epoch [5/5] Step [256/1610]: discriminator_loss = 0.54849, target_loss = 0.40535, acc = 0.50000\n",
            "Epoch [5/5] Step [257/1610]: discriminator_loss = 0.55565, target_loss = 0.40024, acc = 0.50000\n",
            "Epoch [5/5] Step [258/1610]: discriminator_loss = 0.55287, target_loss = 0.40863, acc = 0.50000\n",
            "Epoch [5/5] Step [259/1610]: discriminator_loss = 0.53575, target_loss = 0.43265, acc = 0.50000\n",
            "Epoch [5/5] Step [260/1610]: discriminator_loss = 0.53650, target_loss = 0.38783, acc = 0.50000\n",
            "Epoch [5/5] Step [261/1610]: discriminator_loss = 0.54314, target_loss = 0.40938, acc = 0.50000\n",
            "Epoch [5/5] Step [262/1610]: discriminator_loss = 0.53703, target_loss = 0.42066, acc = 0.50000\n",
            "Epoch [5/5] Step [263/1610]: discriminator_loss = 0.54514, target_loss = 0.41336, acc = 0.50000\n",
            "Epoch [5/5] Step [264/1610]: discriminator_loss = 0.53853, target_loss = 0.42672, acc = 0.50000\n",
            "Epoch [5/5] Step [265/1610]: discriminator_loss = 0.57955, target_loss = 0.41436, acc = 0.50000\n",
            "Epoch [5/5] Step [266/1610]: discriminator_loss = 0.55809, target_loss = 0.38864, acc = 0.50000\n",
            "Epoch [5/5] Step [267/1610]: discriminator_loss = 0.55162, target_loss = 0.41061, acc = 0.50000\n",
            "Epoch [5/5] Step [268/1610]: discriminator_loss = 0.54284, target_loss = 0.41245, acc = 0.50000\n",
            "Epoch [5/5] Step [269/1610]: discriminator_loss = 0.53169, target_loss = 0.39669, acc = 0.50000\n",
            "Epoch [5/5] Step [270/1610]: discriminator_loss = 0.57700, target_loss = 0.41178, acc = 0.50000\n",
            "Epoch [5/5] Step [271/1610]: discriminator_loss = 0.56877, target_loss = 0.40880, acc = 0.50000\n",
            "Epoch [5/5] Step [272/1610]: discriminator_loss = 0.53839, target_loss = 0.40967, acc = 0.50000\n",
            "Epoch [5/5] Step [273/1610]: discriminator_loss = 0.54690, target_loss = 0.39409, acc = 0.50000\n",
            "Epoch [5/5] Step [274/1610]: discriminator_loss = 0.55376, target_loss = 0.41313, acc = 0.50000\n",
            "Epoch [5/5] Step [275/1610]: discriminator_loss = 0.54958, target_loss = 0.39774, acc = 0.50000\n",
            "Epoch [5/5] Step [276/1610]: discriminator_loss = 0.56211, target_loss = 0.40799, acc = 0.50000\n",
            "Epoch [5/5] Step [277/1610]: discriminator_loss = 0.53592, target_loss = 0.41560, acc = 0.50000\n",
            "Epoch [5/5] Step [278/1610]: discriminator_loss = 0.54619, target_loss = 0.38745, acc = 0.50000\n",
            "Epoch [5/5] Step [279/1610]: discriminator_loss = 0.56557, target_loss = 0.40505, acc = 0.50000\n",
            "Epoch [5/5] Step [280/1610]: discriminator_loss = 0.59830, target_loss = 0.40143, acc = 0.50000\n",
            "Epoch [5/5] Step [281/1610]: discriminator_loss = 0.57925, target_loss = 0.39711, acc = 0.50000\n",
            "Epoch [5/5] Step [282/1610]: discriminator_loss = 0.54686, target_loss = 0.38725, acc = 0.50000\n",
            "Epoch [5/5] Step [283/1610]: discriminator_loss = 0.57930, target_loss = 0.42232, acc = 0.50000\n",
            "Epoch [5/5] Step [284/1610]: discriminator_loss = 0.53886, target_loss = 0.42248, acc = 0.50000\n",
            "Epoch [5/5] Step [285/1610]: discriminator_loss = 0.56007, target_loss = 0.41410, acc = 0.50000\n",
            "Epoch [5/5] Step [286/1610]: discriminator_loss = 0.54110, target_loss = 0.40269, acc = 0.50000\n",
            "Epoch [5/5] Step [287/1610]: discriminator_loss = 0.54184, target_loss = 0.40891, acc = 0.50000\n",
            "Epoch [5/5] Step [288/1610]: discriminator_loss = 0.55619, target_loss = 0.41508, acc = 0.50000\n",
            "Epoch [5/5] Step [289/1610]: discriminator_loss = 0.54360, target_loss = 0.42559, acc = 0.50000\n",
            "Epoch [5/5] Step [290/1610]: discriminator_loss = 0.55142, target_loss = 0.41563, acc = 0.50000\n",
            "Epoch [5/5] Step [291/1610]: discriminator_loss = 0.55075, target_loss = 0.41595, acc = 0.50000\n",
            "Epoch [5/5] Step [292/1610]: discriminator_loss = 0.54858, target_loss = 0.40936, acc = 0.50000\n",
            "Epoch [5/5] Step [293/1610]: discriminator_loss = 0.55356, target_loss = 0.41217, acc = 0.50000\n",
            "Epoch [5/5] Step [294/1610]: discriminator_loss = 0.54926, target_loss = 0.39978, acc = 0.50000\n",
            "Epoch [5/5] Step [295/1610]: discriminator_loss = 0.56280, target_loss = 0.41061, acc = 0.50000\n",
            "Epoch [5/5] Step [296/1610]: discriminator_loss = 0.55568, target_loss = 0.42164, acc = 0.50000\n",
            "Epoch [5/5] Step [297/1610]: discriminator_loss = 0.55449, target_loss = 0.39352, acc = 0.50000\n",
            "Epoch [5/5] Step [298/1610]: discriminator_loss = 0.55637, target_loss = 0.38865, acc = 0.50000\n",
            "Epoch [5/5] Step [299/1610]: discriminator_loss = 0.58342, target_loss = 0.40428, acc = 0.50000\n",
            "Epoch [5/5] Step [300/1610]: discriminator_loss = 0.53860, target_loss = 0.42117, acc = 0.50000\n",
            "Epoch [5/5] Step [301/1610]: discriminator_loss = 0.53985, target_loss = 0.40475, acc = 0.50000\n",
            "Epoch [5/5] Step [302/1610]: discriminator_loss = 0.54689, target_loss = 0.39771, acc = 0.50000\n",
            "Epoch [5/5] Step [303/1610]: discriminator_loss = 0.54053, target_loss = 0.41101, acc = 0.50000\n",
            "Epoch [5/5] Step [304/1610]: discriminator_loss = 0.54962, target_loss = 0.42771, acc = 0.50000\n",
            "Epoch [5/5] Step [305/1610]: discriminator_loss = 0.55430, target_loss = 0.43277, acc = 0.50000\n",
            "Epoch [5/5] Step [306/1610]: discriminator_loss = 0.56791, target_loss = 0.40331, acc = 0.50000\n",
            "Epoch [5/5] Step [307/1610]: discriminator_loss = 0.55688, target_loss = 0.38847, acc = 0.50000\n",
            "Epoch [5/5] Step [308/1610]: discriminator_loss = 0.55948, target_loss = 0.37774, acc = 0.50000\n",
            "Epoch [5/5] Step [309/1610]: discriminator_loss = 0.54902, target_loss = 0.41072, acc = 0.50000\n",
            "Epoch [5/5] Step [310/1610]: discriminator_loss = 0.56847, target_loss = 0.42753, acc = 0.50000\n",
            "Epoch [5/5] Step [311/1610]: discriminator_loss = 0.55119, target_loss = 0.39683, acc = 0.50000\n",
            "Epoch [5/5] Step [312/1610]: discriminator_loss = 0.54837, target_loss = 0.41979, acc = 0.50000\n",
            "Epoch [5/5] Step [313/1610]: discriminator_loss = 0.57088, target_loss = 0.40122, acc = 0.50000\n",
            "Epoch [5/5] Step [314/1610]: discriminator_loss = 0.54320, target_loss = 0.41106, acc = 0.50000\n",
            "Epoch [5/5] Step [315/1610]: discriminator_loss = 0.54838, target_loss = 0.41172, acc = 0.50000\n",
            "Epoch [5/5] Step [316/1610]: discriminator_loss = 0.54849, target_loss = 0.41197, acc = 0.50000\n",
            "Epoch [5/5] Step [317/1610]: discriminator_loss = 0.54576, target_loss = 0.40856, acc = 0.50000\n",
            "Epoch [5/5] Step [318/1610]: discriminator_loss = 0.53656, target_loss = 0.42783, acc = 0.50000\n",
            "Epoch [5/5] Step [319/1610]: discriminator_loss = 0.60038, target_loss = 0.40320, acc = 0.50000\n",
            "Epoch [5/5] Step [320/1610]: discriminator_loss = 0.57006, target_loss = 0.39421, acc = 0.50000\n",
            "Epoch [5/5] Step [321/1610]: discriminator_loss = 0.55266, target_loss = 0.40279, acc = 0.50000\n",
            "Epoch [5/5] Step [322/1610]: discriminator_loss = 0.54665, target_loss = 0.40067, acc = 0.50000\n",
            "Epoch [5/5] Step [323/1610]: discriminator_loss = 0.56242, target_loss = 0.40104, acc = 0.50000\n",
            "Epoch [5/5] Step [324/1610]: discriminator_loss = 0.55707, target_loss = 0.37317, acc = 0.50000\n",
            "Epoch [5/5] Step [325/1610]: discriminator_loss = 0.59285, target_loss = 0.39601, acc = 0.50000\n",
            "Epoch [5/5] Step [326/1610]: discriminator_loss = 0.57225, target_loss = 0.39186, acc = 0.50000\n",
            "Epoch [5/5] Step [327/1610]: discriminator_loss = 0.57286, target_loss = 0.39871, acc = 0.50000\n",
            "Epoch [5/5] Step [328/1610]: discriminator_loss = 0.56268, target_loss = 0.38803, acc = 0.50000\n",
            "Epoch [5/5] Step [329/1610]: discriminator_loss = 0.57031, target_loss = 0.40484, acc = 0.50000\n",
            "Epoch [5/5] Step [330/1610]: discriminator_loss = 0.57681, target_loss = 0.39956, acc = 0.50000\n",
            "Epoch [5/5] Step [331/1610]: discriminator_loss = 0.56335, target_loss = 0.39637, acc = 0.50000\n",
            "Epoch [5/5] Step [332/1610]: discriminator_loss = 0.55604, target_loss = 0.40325, acc = 0.50000\n",
            "Epoch [5/5] Step [333/1610]: discriminator_loss = 0.56389, target_loss = 0.40908, acc = 0.50000\n",
            "Epoch [5/5] Step [334/1610]: discriminator_loss = 0.57032, target_loss = 0.39915, acc = 0.50000\n",
            "Epoch [5/5] Step [335/1610]: discriminator_loss = 0.54330, target_loss = 0.40442, acc = 0.50000\n",
            "Epoch [5/5] Step [336/1610]: discriminator_loss = 0.57447, target_loss = 0.39767, acc = 0.50000\n",
            "Epoch [5/5] Step [337/1610]: discriminator_loss = 0.54869, target_loss = 0.40674, acc = 0.50000\n",
            "Epoch [5/5] Step [338/1610]: discriminator_loss = 0.59209, target_loss = 0.41344, acc = 0.50000\n",
            "Epoch [5/5] Step [339/1610]: discriminator_loss = 0.55666, target_loss = 0.40200, acc = 0.50000\n",
            "Epoch [5/5] Step [340/1610]: discriminator_loss = 0.55800, target_loss = 0.40805, acc = 0.50000\n",
            "Epoch [5/5] Step [341/1610]: discriminator_loss = 0.54991, target_loss = 0.42667, acc = 0.50000\n",
            "Epoch [5/5] Step [342/1610]: discriminator_loss = 0.55452, target_loss = 0.41837, acc = 0.50000\n",
            "Epoch [5/5] Step [343/1610]: discriminator_loss = 0.53953, target_loss = 0.40696, acc = 0.50000\n",
            "Epoch [5/5] Step [344/1610]: discriminator_loss = 0.56972, target_loss = 0.40499, acc = 0.50000\n",
            "Epoch [5/5] Step [345/1610]: discriminator_loss = 0.53502, target_loss = 0.42282, acc = 0.53125\n",
            "Epoch [5/5] Step [346/1610]: discriminator_loss = 0.54445, target_loss = 0.39433, acc = 0.50000\n",
            "Epoch [5/5] Step [347/1610]: discriminator_loss = 0.57314, target_loss = 0.41007, acc = 0.50000\n",
            "Epoch [5/5] Step [348/1610]: discriminator_loss = 0.56481, target_loss = 0.39782, acc = 0.50000\n",
            "Epoch [5/5] Step [349/1610]: discriminator_loss = 0.56273, target_loss = 0.39724, acc = 0.50000\n",
            "Epoch [5/5] Step [350/1610]: discriminator_loss = 0.55977, target_loss = 0.40803, acc = 0.50000\n",
            "Epoch [5/5] Step [351/1610]: discriminator_loss = 0.56232, target_loss = 0.40027, acc = 0.50000\n",
            "Epoch [5/5] Step [352/1610]: discriminator_loss = 0.57353, target_loss = 0.39081, acc = 0.50000\n",
            "Epoch [5/5] Step [353/1610]: discriminator_loss = 0.56997, target_loss = 0.37779, acc = 0.50000\n",
            "Epoch [5/5] Step [354/1610]: discriminator_loss = 0.55786, target_loss = 0.39407, acc = 0.50000\n",
            "Epoch [5/5] Step [355/1610]: discriminator_loss = 0.56994, target_loss = 0.40005, acc = 0.50000\n",
            "Epoch [5/5] Step [356/1610]: discriminator_loss = 0.59393, target_loss = 0.38387, acc = 0.50000\n",
            "Epoch [5/5] Step [357/1610]: discriminator_loss = 0.56769, target_loss = 0.37888, acc = 0.50000\n",
            "Epoch [5/5] Step [358/1610]: discriminator_loss = 0.56640, target_loss = 0.38042, acc = 0.50000\n",
            "Epoch [5/5] Step [359/1610]: discriminator_loss = 0.56716, target_loss = 0.39637, acc = 0.50000\n",
            "Epoch [5/5] Step [360/1610]: discriminator_loss = 0.57323, target_loss = 0.39227, acc = 0.50000\n",
            "Epoch [5/5] Step [361/1610]: discriminator_loss = 0.59600, target_loss = 0.39082, acc = 0.50000\n",
            "Epoch [5/5] Step [362/1610]: discriminator_loss = 0.56258, target_loss = 0.39429, acc = 0.50000\n",
            "Epoch [5/5] Step [363/1610]: discriminator_loss = 0.54991, target_loss = 0.41132, acc = 0.50000\n",
            "Epoch [5/5] Step [364/1610]: discriminator_loss = 0.54675, target_loss = 0.41193, acc = 0.50000\n",
            "Epoch [5/5] Step [365/1610]: discriminator_loss = 0.56327, target_loss = 0.41200, acc = 0.50000\n",
            "Epoch [5/5] Step [366/1610]: discriminator_loss = 0.55062, target_loss = 0.38941, acc = 0.50000\n",
            "Epoch [5/5] Step [367/1610]: discriminator_loss = 0.55252, target_loss = 0.41472, acc = 0.50000\n",
            "Epoch [5/5] Step [368/1610]: discriminator_loss = 0.55169, target_loss = 0.41264, acc = 0.50000\n",
            "Epoch [5/5] Step [369/1610]: discriminator_loss = 0.54455, target_loss = 0.40966, acc = 0.50000\n",
            "Epoch [5/5] Step [370/1610]: discriminator_loss = 0.53603, target_loss = 0.40239, acc = 0.50000\n",
            "Epoch [5/5] Step [371/1610]: discriminator_loss = 0.57302, target_loss = 0.41387, acc = 0.50000\n",
            "Epoch [5/5] Step [372/1610]: discriminator_loss = 0.54508, target_loss = 0.41760, acc = 0.50000\n",
            "Epoch [5/5] Step [373/1610]: discriminator_loss = 0.55685, target_loss = 0.40452, acc = 0.50000\n",
            "Epoch [5/5] Step [374/1610]: discriminator_loss = 0.56640, target_loss = 0.40192, acc = 0.50000\n",
            "Epoch [5/5] Step [375/1610]: discriminator_loss = 0.59844, target_loss = 0.41590, acc = 0.50000\n",
            "Epoch [5/5] Step [376/1610]: discriminator_loss = 0.55967, target_loss = 0.41886, acc = 0.50000\n",
            "Epoch [5/5] Step [377/1610]: discriminator_loss = 0.54252, target_loss = 0.40633, acc = 0.50000\n",
            "Epoch [5/5] Step [378/1610]: discriminator_loss = 0.57442, target_loss = 0.41169, acc = 0.50000\n",
            "Epoch [5/5] Step [379/1610]: discriminator_loss = 0.56203, target_loss = 0.40688, acc = 0.50000\n",
            "Epoch [5/5] Step [380/1610]: discriminator_loss = 0.54979, target_loss = 0.41436, acc = 0.50000\n",
            "Epoch [5/5] Step [381/1610]: discriminator_loss = 0.55718, target_loss = 0.40100, acc = 0.50000\n",
            "Epoch [5/5] Step [382/1610]: discriminator_loss = 0.55247, target_loss = 0.40485, acc = 0.50000\n",
            "Epoch [5/5] Step [383/1610]: discriminator_loss = 0.53150, target_loss = 0.40631, acc = 0.50000\n",
            "Epoch [5/5] Step [384/1610]: discriminator_loss = 0.54255, target_loss = 0.39906, acc = 0.50000\n",
            "Epoch [5/5] Step [385/1610]: discriminator_loss = 0.54818, target_loss = 0.40408, acc = 0.50000\n",
            "Epoch [5/5] Step [386/1610]: discriminator_loss = 0.55474, target_loss = 0.41187, acc = 0.50000\n",
            "Epoch [5/5] Step [387/1610]: discriminator_loss = 0.55785, target_loss = 0.40387, acc = 0.50000\n",
            "Epoch [5/5] Step [388/1610]: discriminator_loss = 0.56249, target_loss = 0.40900, acc = 0.50000\n",
            "Epoch [5/5] Step [389/1610]: discriminator_loss = 0.56692, target_loss = 0.39973, acc = 0.50000\n",
            "Epoch [5/5] Step [390/1610]: discriminator_loss = 0.55329, target_loss = 0.40493, acc = 0.50000\n",
            "Epoch [5/5] Step [391/1610]: discriminator_loss = 0.55161, target_loss = 0.37821, acc = 0.50000\n",
            "Epoch [5/5] Step [392/1610]: discriminator_loss = 0.56144, target_loss = 0.41359, acc = 0.50000\n",
            "Epoch [5/5] Step [393/1610]: discriminator_loss = 0.54690, target_loss = 0.39052, acc = 0.50000\n",
            "Epoch [5/5] Step [394/1610]: discriminator_loss = 0.54233, target_loss = 0.39538, acc = 0.50000\n",
            "Epoch [5/5] Step [395/1610]: discriminator_loss = 0.54560, target_loss = 0.39603, acc = 0.50000\n",
            "Epoch [5/5] Step [396/1610]: discriminator_loss = 0.56269, target_loss = 0.40217, acc = 0.50000\n",
            "Epoch [5/5] Step [397/1610]: discriminator_loss = 0.59633, target_loss = 0.37608, acc = 0.50000\n",
            "Epoch [5/5] Step [398/1610]: discriminator_loss = 0.58127, target_loss = 0.40615, acc = 0.50000\n",
            "Epoch [5/5] Step [399/1610]: discriminator_loss = 0.57116, target_loss = 0.39829, acc = 0.50000\n",
            "Epoch [5/5] Step [400/1610]: discriminator_loss = 0.56026, target_loss = 0.40431, acc = 0.50000\n",
            "Epoch [5/5] Step [401/1610]: discriminator_loss = 0.54935, target_loss = 0.41241, acc = 0.50000\n",
            "Epoch [5/5] Step [402/1610]: discriminator_loss = 0.55867, target_loss = 0.39742, acc = 0.50000\n",
            "Epoch [5/5] Step [403/1610]: discriminator_loss = 0.62000, target_loss = 0.40455, acc = 0.50000\n",
            "Epoch [5/5] Step [404/1610]: discriminator_loss = 0.54675, target_loss = 0.40330, acc = 0.50000\n",
            "Epoch [5/5] Step [405/1610]: discriminator_loss = 0.55131, target_loss = 0.41136, acc = 0.50000\n",
            "Epoch [5/5] Step [406/1610]: discriminator_loss = 0.54615, target_loss = 0.40825, acc = 0.50000\n",
            "Epoch [5/5] Step [407/1610]: discriminator_loss = 0.55817, target_loss = 0.40242, acc = 0.50000\n",
            "Epoch [5/5] Step [408/1610]: discriminator_loss = 0.55705, target_loss = 0.40471, acc = 0.50000\n",
            "Epoch [5/5] Step [409/1610]: discriminator_loss = 0.55232, target_loss = 0.38669, acc = 0.50000\n",
            "Epoch [5/5] Step [410/1610]: discriminator_loss = 0.56323, target_loss = 0.40356, acc = 0.50000\n",
            "Epoch [5/5] Step [411/1610]: discriminator_loss = 0.55447, target_loss = 0.40844, acc = 0.50000\n",
            "Epoch [5/5] Step [412/1610]: discriminator_loss = 0.57534, target_loss = 0.41225, acc = 0.50000\n",
            "Epoch [5/5] Step [413/1610]: discriminator_loss = 0.56161, target_loss = 0.40280, acc = 0.50000\n",
            "Epoch [5/5] Step [414/1610]: discriminator_loss = 0.55248, target_loss = 0.40897, acc = 0.50000\n",
            "Epoch [5/5] Step [415/1610]: discriminator_loss = 0.56405, target_loss = 0.40202, acc = 0.50000\n",
            "Epoch [5/5] Step [416/1610]: discriminator_loss = 0.55956, target_loss = 0.38908, acc = 0.50000\n",
            "Epoch [5/5] Step [417/1610]: discriminator_loss = 0.57846, target_loss = 0.39363, acc = 0.50000\n",
            "Epoch [5/5] Step [418/1610]: discriminator_loss = 0.57629, target_loss = 0.39879, acc = 0.50000\n",
            "Epoch [5/5] Step [419/1610]: discriminator_loss = 0.56261, target_loss = 0.39361, acc = 0.50000\n",
            "Epoch [5/5] Step [420/1610]: discriminator_loss = 0.56493, target_loss = 0.40162, acc = 0.50000\n",
            "Epoch [5/5] Step [421/1610]: discriminator_loss = 0.55539, target_loss = 0.38819, acc = 0.50000\n",
            "Epoch [5/5] Step [422/1610]: discriminator_loss = 0.54714, target_loss = 0.39718, acc = 0.50000\n",
            "Epoch [5/5] Step [423/1610]: discriminator_loss = 0.56541, target_loss = 0.39943, acc = 0.50000\n",
            "Epoch [5/5] Step [424/1610]: discriminator_loss = 0.56079, target_loss = 0.40175, acc = 0.50000\n",
            "Epoch [5/5] Step [425/1610]: discriminator_loss = 0.56595, target_loss = 0.37737, acc = 0.50000\n",
            "Epoch [5/5] Step [426/1610]: discriminator_loss = 0.56803, target_loss = 0.40967, acc = 0.50000\n",
            "Epoch [5/5] Step [427/1610]: discriminator_loss = 0.55484, target_loss = 0.39314, acc = 0.50000\n",
            "Epoch [5/5] Step [428/1610]: discriminator_loss = 0.56868, target_loss = 0.39890, acc = 0.50000\n",
            "Epoch [5/5] Step [429/1610]: discriminator_loss = 0.56644, target_loss = 0.41168, acc = 0.50000\n",
            "Epoch [5/5] Step [430/1610]: discriminator_loss = 0.65745, target_loss = 0.38138, acc = 0.50000\n",
            "Epoch [5/5] Step [431/1610]: discriminator_loss = 0.59184, target_loss = 0.40235, acc = 0.50000\n",
            "Epoch [5/5] Step [432/1610]: discriminator_loss = 0.53714, target_loss = 0.41458, acc = 0.50000\n",
            "Epoch [5/5] Step [433/1610]: discriminator_loss = 0.55402, target_loss = 0.40677, acc = 0.50000\n",
            "Epoch [5/5] Step [434/1610]: discriminator_loss = 0.55837, target_loss = 0.41384, acc = 0.50000\n",
            "Epoch [5/5] Step [435/1610]: discriminator_loss = 0.54428, target_loss = 0.41024, acc = 0.50000\n",
            "Epoch [5/5] Step [436/1610]: discriminator_loss = 0.54770, target_loss = 0.39850, acc = 0.50000\n",
            "Epoch [5/5] Step [437/1610]: discriminator_loss = 0.55584, target_loss = 0.41798, acc = 0.50000\n",
            "Epoch [5/5] Step [438/1610]: discriminator_loss = 0.56184, target_loss = 0.39766, acc = 0.50000\n",
            "Epoch [5/5] Step [439/1610]: discriminator_loss = 0.56228, target_loss = 0.41732, acc = 0.50000\n",
            "Epoch [5/5] Step [440/1610]: discriminator_loss = 0.54710, target_loss = 0.40732, acc = 0.50000\n",
            "Epoch [5/5] Step [441/1610]: discriminator_loss = 0.54679, target_loss = 0.41358, acc = 0.50000\n",
            "Epoch [5/5] Step [442/1610]: discriminator_loss = 0.55013, target_loss = 0.40718, acc = 0.50000\n",
            "Epoch [5/5] Step [443/1610]: discriminator_loss = 0.54040, target_loss = 0.41373, acc = 0.50000\n",
            "Epoch [5/5] Step [444/1610]: discriminator_loss = 0.55604, target_loss = 0.40161, acc = 0.50000\n",
            "Epoch [5/5] Step [445/1610]: discriminator_loss = 0.55288, target_loss = 0.40918, acc = 0.50000\n",
            "Epoch [5/5] Step [446/1610]: discriminator_loss = 0.53966, target_loss = 0.39716, acc = 0.50000\n",
            "Epoch [5/5] Step [447/1610]: discriminator_loss = 0.54957, target_loss = 0.40855, acc = 0.50000\n",
            "Epoch [5/5] Step [448/1610]: discriminator_loss = 0.56052, target_loss = 0.40651, acc = 0.50000\n",
            "Epoch [5/5] Step [449/1610]: discriminator_loss = 0.56302, target_loss = 0.39761, acc = 0.50000\n",
            "Epoch [5/5] Step [450/1610]: discriminator_loss = 0.54556, target_loss = 0.39971, acc = 0.50000\n",
            "Epoch [5/5] Step [451/1610]: discriminator_loss = 0.55329, target_loss = 0.37810, acc = 0.50000\n",
            "Epoch [5/5] Step [452/1610]: discriminator_loss = 0.54286, target_loss = 0.38837, acc = 0.50000\n",
            "Epoch [5/5] Step [453/1610]: discriminator_loss = 0.54766, target_loss = 0.37624, acc = 0.50000\n",
            "Epoch [5/5] Step [454/1610]: discriminator_loss = 0.57131, target_loss = 0.36834, acc = 0.50000\n",
            "Epoch [5/5] Step [455/1610]: discriminator_loss = 0.58221, target_loss = 0.39574, acc = 0.50000\n",
            "Epoch [5/5] Step [456/1610]: discriminator_loss = 0.56028, target_loss = 0.39724, acc = 0.50000\n",
            "Epoch [5/5] Step [457/1610]: discriminator_loss = 0.56996, target_loss = 0.42022, acc = 0.50000\n",
            "Epoch [5/5] Step [458/1610]: discriminator_loss = 0.55011, target_loss = 0.41942, acc = 0.50000\n",
            "Epoch [5/5] Step [459/1610]: discriminator_loss = 0.61339, target_loss = 0.41079, acc = 0.50000\n",
            "Epoch [5/5] Step [460/1610]: discriminator_loss = 0.53454, target_loss = 0.40925, acc = 0.50000\n",
            "Epoch [5/5] Step [461/1610]: discriminator_loss = 0.55596, target_loss = 0.40352, acc = 0.50000\n",
            "Epoch [5/5] Step [462/1610]: discriminator_loss = 0.53953, target_loss = 0.42388, acc = 0.50000\n",
            "Epoch [5/5] Step [463/1610]: discriminator_loss = 0.55375, target_loss = 0.39529, acc = 0.50000\n",
            "Epoch [5/5] Step [464/1610]: discriminator_loss = 0.53407, target_loss = 0.40556, acc = 0.50000\n",
            "Epoch [5/5] Step [465/1610]: discriminator_loss = 0.55744, target_loss = 0.39973, acc = 0.50000\n",
            "Epoch [5/5] Step [466/1610]: discriminator_loss = 0.56247, target_loss = 0.41442, acc = 0.50000\n",
            "Epoch [5/5] Step [467/1610]: discriminator_loss = 0.54128, target_loss = 0.40344, acc = 0.50000\n",
            "Epoch [5/5] Step [468/1610]: discriminator_loss = 0.54950, target_loss = 0.41716, acc = 0.50000\n",
            "Epoch [5/5] Step [469/1610]: discriminator_loss = 0.55630, target_loss = 0.41688, acc = 0.50000\n",
            "Epoch [5/5] Step [470/1610]: discriminator_loss = 0.56596, target_loss = 0.41019, acc = 0.50000\n",
            "Epoch [5/5] Step [471/1610]: discriminator_loss = 0.55932, target_loss = 0.41320, acc = 0.50000\n",
            "Epoch [5/5] Step [472/1610]: discriminator_loss = 0.55148, target_loss = 0.38932, acc = 0.50000\n",
            "Epoch [5/5] Step [473/1610]: discriminator_loss = 0.54664, target_loss = 0.41318, acc = 0.50000\n",
            "Epoch [5/5] Step [474/1610]: discriminator_loss = 0.55000, target_loss = 0.40869, acc = 0.50000\n",
            "Epoch [5/5] Step [475/1610]: discriminator_loss = 0.54620, target_loss = 0.39277, acc = 0.50000\n",
            "Epoch [5/5] Step [476/1610]: discriminator_loss = 0.55688, target_loss = 0.40807, acc = 0.50000\n",
            "Epoch [5/5] Step [477/1610]: discriminator_loss = 0.55597, target_loss = 0.38473, acc = 0.50000\n",
            "Epoch [5/5] Step [478/1610]: discriminator_loss = 0.56271, target_loss = 0.39932, acc = 0.50000\n",
            "Epoch [5/5] Step [479/1610]: discriminator_loss = 0.55596, target_loss = 0.38315, acc = 0.50000\n",
            "Epoch [5/5] Step [480/1610]: discriminator_loss = 0.55635, target_loss = 0.39621, acc = 0.50000\n",
            "Epoch [5/5] Step [481/1610]: discriminator_loss = 0.58605, target_loss = 0.41387, acc = 0.50000\n",
            "Epoch [5/5] Step [482/1610]: discriminator_loss = 0.56311, target_loss = 0.39121, acc = 0.50000\n",
            "Epoch [5/5] Step [483/1610]: discriminator_loss = 0.54952, target_loss = 0.36714, acc = 0.50000\n",
            "Epoch [5/5] Step [484/1610]: discriminator_loss = 0.54084, target_loss = 0.40869, acc = 0.50000\n",
            "Epoch [5/5] Step [485/1610]: discriminator_loss = 0.57071, target_loss = 0.41401, acc = 0.50000\n",
            "Epoch [5/5] Step [486/1610]: discriminator_loss = 0.63163, target_loss = 0.41297, acc = 0.50000\n",
            "Epoch [5/5] Step [487/1610]: discriminator_loss = 0.54868, target_loss = 0.42508, acc = 0.50000\n",
            "Epoch [5/5] Step [488/1610]: discriminator_loss = 0.55204, target_loss = 0.40939, acc = 0.50000\n",
            "Epoch [5/5] Step [489/1610]: discriminator_loss = 0.54929, target_loss = 0.43241, acc = 0.50000\n",
            "Epoch [5/5] Step [490/1610]: discriminator_loss = 0.56847, target_loss = 0.41467, acc = 0.50000\n",
            "Epoch [5/5] Step [491/1610]: discriminator_loss = 0.59774, target_loss = 0.40366, acc = 0.50000\n",
            "Epoch [5/5] Step [492/1610]: discriminator_loss = 0.55462, target_loss = 0.43589, acc = 0.50000\n",
            "Epoch [5/5] Step [493/1610]: discriminator_loss = 0.56232, target_loss = 0.41787, acc = 0.50000\n",
            "Epoch [5/5] Step [494/1610]: discriminator_loss = 0.55013, target_loss = 0.40078, acc = 0.50000\n",
            "Epoch [5/5] Step [495/1610]: discriminator_loss = 0.56423, target_loss = 0.40851, acc = 0.50000\n",
            "Epoch [5/5] Step [496/1610]: discriminator_loss = 0.57058, target_loss = 0.40143, acc = 0.50000\n",
            "Epoch [5/5] Step [497/1610]: discriminator_loss = 0.56116, target_loss = 0.40206, acc = 0.50000\n",
            "Epoch [5/5] Step [498/1610]: discriminator_loss = 0.58322, target_loss = 0.39694, acc = 0.50000\n",
            "Epoch [5/5] Step [499/1610]: discriminator_loss = 0.56892, target_loss = 0.40741, acc = 0.50000\n",
            "Epoch [5/5] Step [500/1610]: discriminator_loss = 0.57676, target_loss = 0.40609, acc = 0.50000\n",
            "Epoch [5/5] Step [501/1610]: discriminator_loss = 0.57348, target_loss = 0.39116, acc = 0.50000\n",
            "Epoch [5/5] Step [502/1610]: discriminator_loss = 0.57120, target_loss = 0.38995, acc = 0.50000\n",
            "Epoch [5/5] Step [503/1610]: discriminator_loss = 0.55188, target_loss = 0.39854, acc = 0.50000\n",
            "Epoch [5/5] Step [504/1610]: discriminator_loss = 0.57912, target_loss = 0.38779, acc = 0.50000\n",
            "Epoch [5/5] Step [505/1610]: discriminator_loss = 0.58406, target_loss = 0.39647, acc = 0.50000\n",
            "Epoch [5/5] Step [506/1610]: discriminator_loss = 0.59121, target_loss = 0.38416, acc = 0.50000\n",
            "Epoch [5/5] Step [507/1610]: discriminator_loss = 0.56613, target_loss = 0.39164, acc = 0.50000\n",
            "Epoch [5/5] Step [508/1610]: discriminator_loss = 0.57502, target_loss = 0.37347, acc = 0.50000\n",
            "Epoch [5/5] Step [509/1610]: discriminator_loss = 0.56786, target_loss = 0.39980, acc = 0.50000\n",
            "Epoch [5/5] Step [510/1610]: discriminator_loss = 0.56505, target_loss = 0.37732, acc = 0.50000\n",
            "Epoch [5/5] Step [511/1610]: discriminator_loss = 0.57095, target_loss = 0.38675, acc = 0.50000\n",
            "Epoch [5/5] Step [512/1610]: discriminator_loss = 0.55870, target_loss = 0.39468, acc = 0.50000\n",
            "Epoch [5/5] Step [513/1610]: discriminator_loss = 0.58777, target_loss = 0.39810, acc = 0.50000\n",
            "Epoch [5/5] Step [514/1610]: discriminator_loss = 0.58247, target_loss = 0.40810, acc = 0.50000\n",
            "Epoch [5/5] Step [515/1610]: discriminator_loss = 0.58013, target_loss = 0.38930, acc = 0.50000\n",
            "Epoch [5/5] Step [516/1610]: discriminator_loss = 0.55696, target_loss = 0.39629, acc = 0.50000\n",
            "Epoch [5/5] Step [517/1610]: discriminator_loss = 0.54630, target_loss = 0.40805, acc = 0.50000\n",
            "Epoch [5/5] Step [518/1610]: discriminator_loss = 0.55987, target_loss = 0.40849, acc = 0.50000\n",
            "Epoch [5/5] Step [519/1610]: discriminator_loss = 0.56172, target_loss = 0.40123, acc = 0.50000\n",
            "Epoch [5/5] Step [520/1610]: discriminator_loss = 0.54935, target_loss = 0.41630, acc = 0.50000\n",
            "Epoch [5/5] Step [521/1610]: discriminator_loss = 0.56389, target_loss = 0.40889, acc = 0.50000\n",
            "Epoch [5/5] Step [522/1610]: discriminator_loss = 0.57111, target_loss = 0.39939, acc = 0.50000\n",
            "Epoch [5/5] Step [523/1610]: discriminator_loss = 0.55331, target_loss = 0.39657, acc = 0.50000\n",
            "Epoch [5/5] Step [524/1610]: discriminator_loss = 0.56593, target_loss = 0.40048, acc = 0.50000\n",
            "Epoch [5/5] Step [525/1610]: discriminator_loss = 0.55731, target_loss = 0.41063, acc = 0.50000\n",
            "Epoch [5/5] Step [526/1610]: discriminator_loss = 0.57010, target_loss = 0.40835, acc = 0.50000\n",
            "Epoch [5/5] Step [527/1610]: discriminator_loss = 0.57156, target_loss = 0.40260, acc = 0.50000\n",
            "Epoch [5/5] Step [528/1610]: discriminator_loss = 0.55288, target_loss = 0.39870, acc = 0.50000\n",
            "Epoch [5/5] Step [529/1610]: discriminator_loss = 0.55571, target_loss = 0.40347, acc = 0.50000\n",
            "Epoch [5/5] Step [530/1610]: discriminator_loss = 0.55628, target_loss = 0.38196, acc = 0.50000\n",
            "Epoch [5/5] Step [531/1610]: discriminator_loss = 0.55924, target_loss = 0.40192, acc = 0.50000\n",
            "Epoch [5/5] Step [532/1610]: discriminator_loss = 0.55229, target_loss = 0.39397, acc = 0.50000\n",
            "Epoch [5/5] Step [533/1610]: discriminator_loss = 0.57278, target_loss = 0.39500, acc = 0.50000\n",
            "Epoch [5/5] Step [534/1610]: discriminator_loss = 0.56636, target_loss = 0.40340, acc = 0.50000\n",
            "Epoch [5/5] Step [535/1610]: discriminator_loss = 0.56997, target_loss = 0.39160, acc = 0.50000\n",
            "Epoch [5/5] Step [536/1610]: discriminator_loss = 0.55943, target_loss = 0.40222, acc = 0.50000\n",
            "Epoch [5/5] Step [537/1610]: discriminator_loss = 0.57036, target_loss = 0.39706, acc = 0.50000\n",
            "Epoch [5/5] Step [538/1610]: discriminator_loss = 0.58441, target_loss = 0.37612, acc = 0.50000\n",
            "Epoch [5/5] Step [539/1610]: discriminator_loss = 0.56865, target_loss = 0.39574, acc = 0.50000\n",
            "Epoch [5/5] Step [540/1610]: discriminator_loss = 0.55172, target_loss = 0.38031, acc = 0.50000\n",
            "Epoch [5/5] Step [541/1610]: discriminator_loss = 0.59311, target_loss = 0.41079, acc = 0.50000\n",
            "Epoch [5/5] Step [542/1610]: discriminator_loss = 0.59871, target_loss = 0.41458, acc = 0.50000\n",
            "Epoch [5/5] Step [543/1610]: discriminator_loss = 0.61445, target_loss = 0.42106, acc = 0.50000\n",
            "Epoch [5/5] Step [544/1610]: discriminator_loss = 0.53300, target_loss = 0.42299, acc = 0.50000\n",
            "Epoch [5/5] Step [545/1610]: discriminator_loss = 0.54019, target_loss = 0.39515, acc = 0.50000\n",
            "Epoch [5/5] Step [546/1610]: discriminator_loss = 0.54341, target_loss = 0.40474, acc = 0.50000\n",
            "Epoch [5/5] Step [547/1610]: discriminator_loss = 0.54284, target_loss = 0.43071, acc = 0.50000\n",
            "Epoch [5/5] Step [548/1610]: discriminator_loss = 0.54986, target_loss = 0.41156, acc = 0.50000\n",
            "Epoch [5/5] Step [549/1610]: discriminator_loss = 0.55364, target_loss = 0.41557, acc = 0.50000\n",
            "Epoch [5/5] Step [550/1610]: discriminator_loss = 0.55959, target_loss = 0.41820, acc = 0.50000\n",
            "Epoch [5/5] Step [551/1610]: discriminator_loss = 0.55190, target_loss = 0.40841, acc = 0.50000\n",
            "Epoch [5/5] Step [552/1610]: discriminator_loss = 0.56268, target_loss = 0.40930, acc = 0.50000\n",
            "Epoch [5/5] Step [553/1610]: discriminator_loss = 0.55473, target_loss = 0.42097, acc = 0.50000\n",
            "Epoch [5/5] Step [554/1610]: discriminator_loss = 0.55171, target_loss = 0.40814, acc = 0.50000\n",
            "Epoch [5/5] Step [555/1610]: discriminator_loss = 0.57929, target_loss = 0.39909, acc = 0.50000\n",
            "Epoch [5/5] Step [556/1610]: discriminator_loss = 0.56384, target_loss = 0.41215, acc = 0.50000\n",
            "Epoch [5/5] Step [557/1610]: discriminator_loss = 0.57975, target_loss = 0.38713, acc = 0.50000\n",
            "Epoch [5/5] Step [558/1610]: discriminator_loss = 0.55493, target_loss = 0.41018, acc = 0.50000\n",
            "Epoch [5/5] Step [559/1610]: discriminator_loss = 0.56153, target_loss = 0.39705, acc = 0.50000\n",
            "Epoch [5/5] Step [560/1610]: discriminator_loss = 0.57774, target_loss = 0.39716, acc = 0.50000\n",
            "Epoch [5/5] Step [561/1610]: discriminator_loss = 0.55973, target_loss = 0.40930, acc = 0.50000\n",
            "Epoch [5/5] Step [562/1610]: discriminator_loss = 0.55391, target_loss = 0.39857, acc = 0.50000\n",
            "Epoch [5/5] Step [563/1610]: discriminator_loss = 0.61015, target_loss = 0.39571, acc = 0.50000\n",
            "Epoch [5/5] Step [564/1610]: discriminator_loss = 0.57831, target_loss = 0.39274, acc = 0.50000\n",
            "Epoch [5/5] Step [565/1610]: discriminator_loss = 0.55807, target_loss = 0.40218, acc = 0.50000\n",
            "Epoch [5/5] Step [566/1610]: discriminator_loss = 0.57625, target_loss = 0.39281, acc = 0.50000\n",
            "Epoch [5/5] Step [567/1610]: discriminator_loss = 0.56339, target_loss = 0.38976, acc = 0.50000\n",
            "Epoch [5/5] Step [568/1610]: discriminator_loss = 0.56405, target_loss = 0.39345, acc = 0.50000\n",
            "Epoch [5/5] Step [569/1610]: discriminator_loss = 0.56525, target_loss = 0.39592, acc = 0.50000\n",
            "Epoch [5/5] Step [570/1610]: discriminator_loss = 0.62481, target_loss = 0.37032, acc = 0.50000\n",
            "Epoch [5/5] Step [571/1610]: discriminator_loss = 0.57659, target_loss = 0.38315, acc = 0.50000\n",
            "Epoch [5/5] Step [572/1610]: discriminator_loss = 0.59475, target_loss = 0.38602, acc = 0.50000\n",
            "Epoch [5/5] Step [573/1610]: discriminator_loss = 0.56912, target_loss = 0.39931, acc = 0.50000\n",
            "Epoch [5/5] Step [574/1610]: discriminator_loss = 0.56282, target_loss = 0.41054, acc = 0.50000\n",
            "Epoch [5/5] Step [575/1610]: discriminator_loss = 0.55104, target_loss = 0.40031, acc = 0.50000\n",
            "Epoch [5/5] Step [576/1610]: discriminator_loss = 0.64410, target_loss = 0.39348, acc = 0.50000\n",
            "Epoch [5/5] Step [577/1610]: discriminator_loss = 0.54702, target_loss = 0.42733, acc = 0.53125\n",
            "Epoch [5/5] Step [578/1610]: discriminator_loss = 0.53894, target_loss = 0.42522, acc = 0.50000\n",
            "Epoch [5/5] Step [579/1610]: discriminator_loss = 0.54182, target_loss = 0.41766, acc = 0.50000\n",
            "Epoch [5/5] Step [580/1610]: discriminator_loss = 0.54966, target_loss = 0.41737, acc = 0.50000\n",
            "Epoch [5/5] Step [581/1610]: discriminator_loss = 0.54019, target_loss = 0.41291, acc = 0.50000\n",
            "Epoch [5/5] Step [582/1610]: discriminator_loss = 0.56036, target_loss = 0.39430, acc = 0.50000\n",
            "Epoch [5/5] Step [583/1610]: discriminator_loss = 0.56107, target_loss = 0.41797, acc = 0.50000\n",
            "Epoch [5/5] Step [584/1610]: discriminator_loss = 0.55158, target_loss = 0.40919, acc = 0.50000\n",
            "Epoch [5/5] Step [585/1610]: discriminator_loss = 0.55161, target_loss = 0.40984, acc = 0.50000\n",
            "Epoch [5/5] Step [586/1610]: discriminator_loss = 0.56461, target_loss = 0.41577, acc = 0.50000\n",
            "Epoch [5/5] Step [587/1610]: discriminator_loss = 0.55155, target_loss = 0.42010, acc = 0.50000\n",
            "Epoch [5/5] Step [588/1610]: discriminator_loss = 0.55077, target_loss = 0.41462, acc = 0.50000\n",
            "Epoch [5/5] Step [589/1610]: discriminator_loss = 0.53902, target_loss = 0.41254, acc = 0.50000\n",
            "Epoch [5/5] Step [590/1610]: discriminator_loss = 0.55747, target_loss = 0.41707, acc = 0.50000\n",
            "Epoch [5/5] Step [591/1610]: discriminator_loss = 0.53304, target_loss = 0.42904, acc = 0.50000\n",
            "Epoch [5/5] Step [592/1610]: discriminator_loss = 0.56000, target_loss = 0.41042, acc = 0.50000\n",
            "Epoch [5/5] Step [593/1610]: discriminator_loss = 0.56449, target_loss = 0.40241, acc = 0.50000\n",
            "Epoch [5/5] Step [594/1610]: discriminator_loss = 0.54354, target_loss = 0.39906, acc = 0.50000\n",
            "Epoch [5/5] Step [595/1610]: discriminator_loss = 0.54918, target_loss = 0.40161, acc = 0.50000\n",
            "Epoch [5/5] Step [596/1610]: discriminator_loss = 0.56065, target_loss = 0.39231, acc = 0.50000\n",
            "Epoch [5/5] Step [597/1610]: discriminator_loss = 0.55286, target_loss = 0.39600, acc = 0.50000\n",
            "Epoch [5/5] Step [598/1610]: discriminator_loss = 0.55913, target_loss = 0.40103, acc = 0.50000\n",
            "Epoch [5/5] Step [599/1610]: discriminator_loss = 0.55700, target_loss = 0.38239, acc = 0.50000\n",
            "Epoch [5/5] Step [600/1610]: discriminator_loss = 0.55779, target_loss = 0.39966, acc = 0.50000\n",
            "Epoch [5/5] Step [601/1610]: discriminator_loss = 0.55964, target_loss = 0.39311, acc = 0.50000\n",
            "Epoch [5/5] Step [602/1610]: discriminator_loss = 0.55266, target_loss = 0.39028, acc = 0.50000\n",
            "Epoch [5/5] Step [603/1610]: discriminator_loss = 0.55843, target_loss = 0.37717, acc = 0.50000\n",
            "Epoch [5/5] Step [604/1610]: discriminator_loss = 0.58779, target_loss = 0.38767, acc = 0.50000\n",
            "Epoch [5/5] Step [605/1610]: discriminator_loss = 0.56735, target_loss = 0.37878, acc = 0.50000\n",
            "Epoch [5/5] Step [606/1610]: discriminator_loss = 0.66722, target_loss = 0.35796, acc = 0.50000\n",
            "Epoch [5/5] Step [607/1610]: discriminator_loss = 0.60374, target_loss = 0.38308, acc = 0.50000\n",
            "Epoch [5/5] Step [608/1610]: discriminator_loss = 0.56335, target_loss = 0.41122, acc = 0.50000\n",
            "Epoch [5/5] Step [609/1610]: discriminator_loss = 0.58440, target_loss = 0.39973, acc = 0.50000\n",
            "Epoch [5/5] Step [610/1610]: discriminator_loss = 0.62090, target_loss = 0.43081, acc = 0.50000\n",
            "Epoch [5/5] Step [611/1610]: discriminator_loss = 0.54353, target_loss = 0.42966, acc = 0.50000\n",
            "Epoch [5/5] Step [612/1610]: discriminator_loss = 0.52584, target_loss = 0.41061, acc = 0.50000\n",
            "Epoch [5/5] Step [613/1610]: discriminator_loss = 0.58995, target_loss = 0.43429, acc = 0.50000\n",
            "Epoch [5/5] Step [614/1610]: discriminator_loss = 0.53313, target_loss = 0.43395, acc = 0.50000\n",
            "Epoch [5/5] Step [615/1610]: discriminator_loss = 0.52129, target_loss = 0.41901, acc = 0.50000\n",
            "Epoch [5/5] Step [616/1610]: discriminator_loss = 0.53933, target_loss = 0.42913, acc = 0.50000\n",
            "Epoch [5/5] Step [617/1610]: discriminator_loss = 0.52692, target_loss = 0.42286, acc = 0.50000\n",
            "Epoch [5/5] Step [618/1610]: discriminator_loss = 0.53236, target_loss = 0.40488, acc = 0.50000\n",
            "Epoch [5/5] Step [619/1610]: discriminator_loss = 0.55595, target_loss = 0.42021, acc = 0.50000\n",
            "Epoch [5/5] Step [620/1610]: discriminator_loss = 0.53885, target_loss = 0.39952, acc = 0.50000\n",
            "Epoch [5/5] Step [621/1610]: discriminator_loss = 0.54844, target_loss = 0.41192, acc = 0.50000\n",
            "Epoch [5/5] Step [622/1610]: discriminator_loss = 0.55126, target_loss = 0.41409, acc = 0.50000\n",
            "Epoch [5/5] Step [623/1610]: discriminator_loss = 0.53369, target_loss = 0.41851, acc = 0.50000\n",
            "Epoch [5/5] Step [624/1610]: discriminator_loss = 0.57268, target_loss = 0.42109, acc = 0.50000\n",
            "Epoch [5/5] Step [625/1610]: discriminator_loss = 0.56281, target_loss = 0.40018, acc = 0.50000\n",
            "Epoch [5/5] Step [626/1610]: discriminator_loss = 0.54461, target_loss = 0.39417, acc = 0.50000\n",
            "Epoch [5/5] Step [627/1610]: discriminator_loss = 0.55390, target_loss = 0.41389, acc = 0.50000\n",
            "Epoch [5/5] Step [628/1610]: discriminator_loss = 0.56613, target_loss = 0.40883, acc = 0.50000\n",
            "Epoch [5/5] Step [629/1610]: discriminator_loss = 0.55113, target_loss = 0.40726, acc = 0.50000\n",
            "Epoch [5/5] Step [630/1610]: discriminator_loss = 0.57549, target_loss = 0.38919, acc = 0.50000\n",
            "Epoch [5/5] Step [631/1610]: discriminator_loss = 0.57561, target_loss = 0.40267, acc = 0.50000\n",
            "Epoch [5/5] Step [632/1610]: discriminator_loss = 0.56285, target_loss = 0.39572, acc = 0.50000\n",
            "Epoch [5/5] Step [633/1610]: discriminator_loss = 0.55564, target_loss = 0.38565, acc = 0.50000\n",
            "Epoch [5/5] Step [634/1610]: discriminator_loss = 0.55501, target_loss = 0.39999, acc = 0.50000\n",
            "Epoch [5/5] Step [635/1610]: discriminator_loss = 0.57797, target_loss = 0.40078, acc = 0.50000\n",
            "Epoch [5/5] Step [636/1610]: discriminator_loss = 0.56284, target_loss = 0.39527, acc = 0.50000\n",
            "Epoch [5/5] Step [637/1610]: discriminator_loss = 0.55353, target_loss = 0.40069, acc = 0.50000\n",
            "Epoch [5/5] Step [638/1610]: discriminator_loss = 0.56369, target_loss = 0.40349, acc = 0.50000\n",
            "Epoch [5/5] Step [639/1610]: discriminator_loss = 0.54861, target_loss = 0.41507, acc = 0.50000\n",
            "Epoch [5/5] Step [640/1610]: discriminator_loss = 0.55932, target_loss = 0.39905, acc = 0.50000\n",
            "Epoch [5/5] Step [641/1610]: discriminator_loss = 0.56311, target_loss = 0.39723, acc = 0.50000\n",
            "Epoch [5/5] Step [642/1610]: discriminator_loss = 0.55606, target_loss = 0.40411, acc = 0.50000\n",
            "Epoch [5/5] Step [643/1610]: discriminator_loss = 0.56850, target_loss = 0.40564, acc = 0.50000\n",
            "Epoch [5/5] Step [644/1610]: discriminator_loss = 0.56134, target_loss = 0.41261, acc = 0.50000\n",
            "Epoch [5/5] Step [645/1610]: discriminator_loss = 0.56288, target_loss = 0.40344, acc = 0.50000\n",
            "Epoch [5/5] Step [646/1610]: discriminator_loss = 0.55024, target_loss = 0.40827, acc = 0.50000\n",
            "Epoch [5/5] Step [647/1610]: discriminator_loss = 0.55622, target_loss = 0.40069, acc = 0.50000\n",
            "Epoch [5/5] Step [648/1610]: discriminator_loss = 0.55225, target_loss = 0.38845, acc = 0.50000\n",
            "Epoch [5/5] Step [649/1610]: discriminator_loss = 0.54780, target_loss = 0.39715, acc = 0.50000\n",
            "Epoch [5/5] Step [650/1610]: discriminator_loss = 0.54790, target_loss = 0.36744, acc = 0.50000\n",
            "Epoch [5/5] Step [651/1610]: discriminator_loss = 0.58343, target_loss = 0.38042, acc = 0.50000\n",
            "Epoch [5/5] Step [652/1610]: discriminator_loss = 0.59033, target_loss = 0.40837, acc = 0.50000\n",
            "Epoch [5/5] Step [653/1610]: discriminator_loss = 0.54178, target_loss = 0.40482, acc = 0.50000\n",
            "Epoch [5/5] Step [654/1610]: discriminator_loss = 0.56496, target_loss = 0.39250, acc = 0.50000\n",
            "Epoch [5/5] Step [655/1610]: discriminator_loss = 0.63330, target_loss = 0.39489, acc = 0.50000\n",
            "Epoch [5/5] Step [656/1610]: discriminator_loss = 0.55168, target_loss = 0.40283, acc = 0.50000\n",
            "Epoch [5/5] Step [657/1610]: discriminator_loss = 0.60361, target_loss = 0.39918, acc = 0.50000\n",
            "Epoch [5/5] Step [658/1610]: discriminator_loss = 0.56591, target_loss = 0.41106, acc = 0.50000\n",
            "Epoch [5/5] Step [659/1610]: discriminator_loss = 0.54180, target_loss = 0.41041, acc = 0.50000\n",
            "Epoch [5/5] Step [660/1610]: discriminator_loss = 0.55335, target_loss = 0.41934, acc = 0.50000\n",
            "Epoch [5/5] Step [661/1610]: discriminator_loss = 0.55495, target_loss = 0.40200, acc = 0.50000\n",
            "Epoch [5/5] Step [662/1610]: discriminator_loss = 0.54104, target_loss = 0.41425, acc = 0.50000\n",
            "Epoch [5/5] Step [663/1610]: discriminator_loss = 0.54326, target_loss = 0.41743, acc = 0.50000\n",
            "Epoch [5/5] Step [664/1610]: discriminator_loss = 0.55216, target_loss = 0.40564, acc = 0.50000\n",
            "Epoch [5/5] Step [665/1610]: discriminator_loss = 0.56515, target_loss = 0.41860, acc = 0.50000\n",
            "Epoch [5/5] Step [666/1610]: discriminator_loss = 0.54664, target_loss = 0.40063, acc = 0.50000\n",
            "Epoch [5/5] Step [667/1610]: discriminator_loss = 0.56134, target_loss = 0.40729, acc = 0.50000\n",
            "Epoch [5/5] Step [668/1610]: discriminator_loss = 0.54967, target_loss = 0.40747, acc = 0.50000\n",
            "Epoch [5/5] Step [669/1610]: discriminator_loss = 0.54245, target_loss = 0.41167, acc = 0.50000\n",
            "Epoch [5/5] Step [670/1610]: discriminator_loss = 0.55971, target_loss = 0.40481, acc = 0.50000\n",
            "Epoch [5/5] Step [671/1610]: discriminator_loss = 0.56114, target_loss = 0.39290, acc = 0.50000\n",
            "Epoch [5/5] Step [672/1610]: discriminator_loss = 0.55044, target_loss = 0.40992, acc = 0.50000\n",
            "Epoch [5/5] Step [673/1610]: discriminator_loss = 0.56210, target_loss = 0.40739, acc = 0.50000\n",
            "Epoch [5/5] Step [674/1610]: discriminator_loss = 0.54858, target_loss = 0.39992, acc = 0.50000\n",
            "Epoch [5/5] Step [675/1610]: discriminator_loss = 0.55283, target_loss = 0.39860, acc = 0.50000\n",
            "Epoch [5/5] Step [676/1610]: discriminator_loss = 0.56975, target_loss = 0.39548, acc = 0.50000\n",
            "Epoch [5/5] Step [677/1610]: discriminator_loss = 0.56445, target_loss = 0.39457, acc = 0.50000\n",
            "Epoch [5/5] Step [678/1610]: discriminator_loss = 0.56392, target_loss = 0.38892, acc = 0.50000\n",
            "Epoch [5/5] Step [679/1610]: discriminator_loss = 0.55943, target_loss = 0.35947, acc = 0.50000\n",
            "Epoch [5/5] Step [680/1610]: discriminator_loss = 0.56458, target_loss = 0.40115, acc = 0.50000\n",
            "Epoch [5/5] Step [681/1610]: discriminator_loss = 0.55857, target_loss = 0.38771, acc = 0.50000\n",
            "Epoch [5/5] Step [682/1610]: discriminator_loss = 0.59920, target_loss = 0.38537, acc = 0.50000\n",
            "Epoch [5/5] Step [683/1610]: discriminator_loss = 0.56770, target_loss = 0.37280, acc = 0.50000\n",
            "Epoch [5/5] Step [684/1610]: discriminator_loss = 0.58616, target_loss = 0.39165, acc = 0.50000\n",
            "Epoch [5/5] Step [685/1610]: discriminator_loss = 0.57302, target_loss = 0.38718, acc = 0.50000\n",
            "Epoch [5/5] Step [686/1610]: discriminator_loss = 0.56691, target_loss = 0.39777, acc = 0.50000\n",
            "Epoch [5/5] Step [687/1610]: discriminator_loss = 0.55394, target_loss = 0.40805, acc = 0.50000\n",
            "Epoch [5/5] Step [688/1610]: discriminator_loss = 0.58059, target_loss = 0.38822, acc = 0.50000\n",
            "Epoch [5/5] Step [689/1610]: discriminator_loss = 0.57012, target_loss = 0.40390, acc = 0.50000\n",
            "Epoch [5/5] Step [690/1610]: discriminator_loss = 0.56003, target_loss = 0.39582, acc = 0.50000\n",
            "Epoch [5/5] Step [691/1610]: discriminator_loss = 0.55165, target_loss = 0.40287, acc = 0.50000\n",
            "Epoch [5/5] Step [692/1610]: discriminator_loss = 0.54837, target_loss = 0.41354, acc = 0.50000\n",
            "Epoch [5/5] Step [693/1610]: discriminator_loss = 0.56288, target_loss = 0.40637, acc = 0.50000\n",
            "Epoch [5/5] Step [694/1610]: discriminator_loss = 0.56653, target_loss = 0.40405, acc = 0.50000\n",
            "Epoch [5/5] Step [695/1610]: discriminator_loss = 0.56520, target_loss = 0.40795, acc = 0.50000\n",
            "Epoch [5/5] Step [696/1610]: discriminator_loss = 0.53653, target_loss = 0.41664, acc = 0.50000\n",
            "Epoch [5/5] Step [697/1610]: discriminator_loss = 0.54776, target_loss = 0.41046, acc = 0.50000\n",
            "Epoch [5/5] Step [698/1610]: discriminator_loss = 0.55338, target_loss = 0.41502, acc = 0.50000\n",
            "Epoch [5/5] Step [699/1610]: discriminator_loss = 0.55065, target_loss = 0.41802, acc = 0.50000\n",
            "Epoch [5/5] Step [700/1610]: discriminator_loss = 0.55801, target_loss = 0.37649, acc = 0.50000\n",
            "Epoch [5/5] Step [701/1610]: discriminator_loss = 0.57750, target_loss = 0.39101, acc = 0.50000\n",
            "Epoch [5/5] Step [702/1610]: discriminator_loss = 0.53637, target_loss = 0.41616, acc = 0.50000\n",
            "Epoch [5/5] Step [703/1610]: discriminator_loss = 0.54583, target_loss = 0.41697, acc = 0.50000\n",
            "Epoch [5/5] Step [704/1610]: discriminator_loss = 0.56327, target_loss = 0.40913, acc = 0.50000\n",
            "Epoch [5/5] Step [705/1610]: discriminator_loss = 0.55329, target_loss = 0.39830, acc = 0.50000\n",
            "Epoch [5/5] Step [706/1610]: discriminator_loss = 0.54379, target_loss = 0.42877, acc = 0.50000\n",
            "Epoch [5/5] Step [707/1610]: discriminator_loss = 0.55417, target_loss = 0.41327, acc = 0.50000\n",
            "Epoch [5/5] Step [708/1610]: discriminator_loss = 0.55039, target_loss = 0.39737, acc = 0.50000\n",
            "Epoch [5/5] Step [709/1610]: discriminator_loss = 0.56212, target_loss = 0.39668, acc = 0.50000\n",
            "Epoch [5/5] Step [710/1610]: discriminator_loss = 0.56695, target_loss = 0.38486, acc = 0.50000\n",
            "Epoch [5/5] Step [711/1610]: discriminator_loss = 0.56507, target_loss = 0.39625, acc = 0.50000\n",
            "Epoch [5/5] Step [712/1610]: discriminator_loss = 0.58125, target_loss = 0.40518, acc = 0.50000\n",
            "Epoch [5/5] Step [713/1610]: discriminator_loss = 0.55070, target_loss = 0.40052, acc = 0.50000\n",
            "Epoch [5/5] Step [714/1610]: discriminator_loss = 0.56951, target_loss = 0.40202, acc = 0.50000\n",
            "Epoch [5/5] Step [715/1610]: discriminator_loss = 0.56666, target_loss = 0.40205, acc = 0.50000\n",
            "Epoch [5/5] Step [716/1610]: discriminator_loss = 0.55851, target_loss = 0.40729, acc = 0.50000\n",
            "Epoch [5/5] Step [717/1610]: discriminator_loss = 0.54737, target_loss = 0.35467, acc = 0.50000\n",
            "Epoch [5/5] Step [718/1610]: discriminator_loss = 0.55367, target_loss = 0.39563, acc = 0.50000\n",
            "Epoch [5/5] Step [719/1610]: discriminator_loss = 0.55219, target_loss = 0.37910, acc = 0.50000\n",
            "Epoch [5/5] Step [720/1610]: discriminator_loss = 0.56157, target_loss = 0.40889, acc = 0.50000\n",
            "Epoch [5/5] Step [721/1610]: discriminator_loss = 0.57430, target_loss = 0.38279, acc = 0.50000\n",
            "Epoch [5/5] Step [722/1610]: discriminator_loss = 0.72155, target_loss = 0.40928, acc = 0.50000\n",
            "Epoch [5/5] Step [723/1610]: discriminator_loss = 0.55561, target_loss = 0.39915, acc = 0.50000\n",
            "Epoch [5/5] Step [724/1610]: discriminator_loss = 0.55280, target_loss = 0.40080, acc = 0.50000\n",
            "Epoch [5/5] Step [725/1610]: discriminator_loss = 0.54747, target_loss = 0.41276, acc = 0.50000\n",
            "Epoch [5/5] Step [726/1610]: discriminator_loss = 0.54354, target_loss = 0.41152, acc = 0.50000\n",
            "Epoch [5/5] Step [727/1610]: discriminator_loss = 0.56224, target_loss = 0.42381, acc = 0.50000\n",
            "Epoch [5/5] Step [728/1610]: discriminator_loss = 0.54077, target_loss = 0.41847, acc = 0.50000\n",
            "Epoch [5/5] Step [729/1610]: discriminator_loss = 0.53866, target_loss = 0.41097, acc = 0.50000\n",
            "Epoch [5/5] Step [730/1610]: discriminator_loss = 0.56060, target_loss = 0.42175, acc = 0.50000\n",
            "Epoch [5/5] Step [731/1610]: discriminator_loss = 0.54953, target_loss = 0.41448, acc = 0.50000\n",
            "Epoch [5/5] Step [732/1610]: discriminator_loss = 0.54823, target_loss = 0.43754, acc = 0.50000\n",
            "Epoch [5/5] Step [733/1610]: discriminator_loss = 0.54036, target_loss = 0.42721, acc = 0.50000\n",
            "Epoch [5/5] Step [734/1610]: discriminator_loss = 0.55130, target_loss = 0.41407, acc = 0.50000\n",
            "Epoch [5/5] Step [735/1610]: discriminator_loss = 0.56018, target_loss = 0.42259, acc = 0.50000\n",
            "Epoch [5/5] Step [736/1610]: discriminator_loss = 0.53069, target_loss = 0.42370, acc = 0.50000\n",
            "Epoch [5/5] Step [737/1610]: discriminator_loss = 0.52945, target_loss = 0.41581, acc = 0.50000\n",
            "Epoch [5/5] Step [738/1610]: discriminator_loss = 0.54576, target_loss = 0.41035, acc = 0.50000\n",
            "Epoch [5/5] Step [739/1610]: discriminator_loss = 0.54692, target_loss = 0.41932, acc = 0.50000\n",
            "Epoch [5/5] Step [740/1610]: discriminator_loss = 0.55211, target_loss = 0.41396, acc = 0.50000\n",
            "Epoch [5/5] Step [741/1610]: discriminator_loss = 0.56205, target_loss = 0.40728, acc = 0.50000\n",
            "Epoch [5/5] Step [742/1610]: discriminator_loss = 0.55900, target_loss = 0.39212, acc = 0.50000\n",
            "Epoch [5/5] Step [743/1610]: discriminator_loss = 0.55847, target_loss = 0.41113, acc = 0.50000\n",
            "Epoch [5/5] Step [744/1610]: discriminator_loss = 0.55823, target_loss = 0.40687, acc = 0.50000\n",
            "Epoch [5/5] Step [745/1610]: discriminator_loss = 0.57021, target_loss = 0.42261, acc = 0.50000\n",
            "Epoch [5/5] Step [746/1610]: discriminator_loss = 0.57124, target_loss = 0.38847, acc = 0.50000\n",
            "Epoch [5/5] Step [747/1610]: discriminator_loss = 0.55735, target_loss = 0.39599, acc = 0.50000\n",
            "Epoch [5/5] Step [748/1610]: discriminator_loss = 0.56166, target_loss = 0.40387, acc = 0.50000\n",
            "Epoch [5/5] Step [749/1610]: discriminator_loss = 0.56336, target_loss = 0.39037, acc = 0.50000\n",
            "Epoch [5/5] Step [750/1610]: discriminator_loss = 0.57585, target_loss = 0.39230, acc = 0.50000\n",
            "Epoch [5/5] Step [751/1610]: discriminator_loss = 0.57234, target_loss = 0.38254, acc = 0.50000\n",
            "Epoch [5/5] Step [752/1610]: discriminator_loss = 0.61759, target_loss = 0.39270, acc = 0.50000\n",
            "Epoch [5/5] Step [753/1610]: discriminator_loss = 0.58444, target_loss = 0.39067, acc = 0.50000\n",
            "Epoch [5/5] Step [754/1610]: discriminator_loss = 0.58326, target_loss = 0.37540, acc = 0.50000\n",
            "Epoch [5/5] Step [755/1610]: discriminator_loss = 0.57184, target_loss = 0.39988, acc = 0.50000\n",
            "Epoch [5/5] Step [756/1610]: discriminator_loss = 0.58149, target_loss = 0.39333, acc = 0.50000\n",
            "Epoch [5/5] Step [757/1610]: discriminator_loss = 0.56000, target_loss = 0.38731, acc = 0.50000\n",
            "Epoch [5/5] Step [758/1610]: discriminator_loss = 0.55954, target_loss = 0.39631, acc = 0.50000\n",
            "Epoch [5/5] Step [759/1610]: discriminator_loss = 0.57232, target_loss = 0.39024, acc = 0.50000\n",
            "Epoch [5/5] Step [760/1610]: discriminator_loss = 0.56776, target_loss = 0.38143, acc = 0.50000\n",
            "Epoch [5/5] Step [761/1610]: discriminator_loss = 0.55628, target_loss = 0.39377, acc = 0.50000\n",
            "Epoch [5/5] Step [762/1610]: discriminator_loss = 0.55379, target_loss = 0.39617, acc = 0.50000\n",
            "Epoch [5/5] Step [763/1610]: discriminator_loss = 0.56323, target_loss = 0.40345, acc = 0.50000\n",
            "Epoch [5/5] Step [764/1610]: discriminator_loss = 0.56173, target_loss = 0.40340, acc = 0.50000\n",
            "Epoch [5/5] Step [765/1610]: discriminator_loss = 0.56759, target_loss = 0.40306, acc = 0.50000\n",
            "Epoch [5/5] Step [766/1610]: discriminator_loss = 0.56736, target_loss = 0.39220, acc = 0.50000\n",
            "Epoch [5/5] Step [767/1610]: discriminator_loss = 0.56314, target_loss = 0.41202, acc = 0.50000\n",
            "Epoch [5/5] Step [768/1610]: discriminator_loss = 0.57229, target_loss = 0.38584, acc = 0.50000\n",
            "Epoch [5/5] Step [769/1610]: discriminator_loss = 0.56194, target_loss = 0.38913, acc = 0.50000\n",
            "Epoch [5/5] Step [770/1610]: discriminator_loss = 0.54956, target_loss = 0.38918, acc = 0.50000\n",
            "Epoch [5/5] Step [771/1610]: discriminator_loss = 0.54937, target_loss = 0.39796, acc = 0.50000\n",
            "Epoch [5/5] Step [772/1610]: discriminator_loss = 0.54645, target_loss = 0.41168, acc = 0.50000\n",
            "Epoch [5/5] Step [773/1610]: discriminator_loss = 0.55213, target_loss = 0.40515, acc = 0.50000\n",
            "Epoch [5/5] Step [774/1610]: discriminator_loss = 0.54900, target_loss = 0.41542, acc = 0.50000\n",
            "Epoch [5/5] Step [775/1610]: discriminator_loss = 0.55434, target_loss = 0.40305, acc = 0.50000\n",
            "Epoch [5/5] Step [776/1610]: discriminator_loss = 0.60381, target_loss = 0.38922, acc = 0.50000\n",
            "Epoch [5/5] Step [777/1610]: discriminator_loss = 0.54587, target_loss = 0.41608, acc = 0.50000\n",
            "Epoch [5/5] Step [778/1610]: discriminator_loss = 0.56057, target_loss = 0.41060, acc = 0.50000\n",
            "Epoch [5/5] Step [779/1610]: discriminator_loss = 0.54010, target_loss = 0.41194, acc = 0.50000\n",
            "Epoch [5/5] Step [780/1610]: discriminator_loss = 0.54677, target_loss = 0.40393, acc = 0.50000\n",
            "Epoch [5/5] Step [781/1610]: discriminator_loss = 0.54104, target_loss = 0.41739, acc = 0.50000\n",
            "Epoch [5/5] Step [782/1610]: discriminator_loss = 0.55185, target_loss = 0.41026, acc = 0.50000\n",
            "Epoch [5/5] Step [783/1610]: discriminator_loss = 0.54797, target_loss = 0.41857, acc = 0.50000\n",
            "Epoch [5/5] Step [784/1610]: discriminator_loss = 0.54336, target_loss = 0.41541, acc = 0.50000\n",
            "Epoch [5/5] Step [785/1610]: discriminator_loss = 0.55104, target_loss = 0.41783, acc = 0.50000\n",
            "Epoch [5/5] Step [786/1610]: discriminator_loss = 0.55013, target_loss = 0.40953, acc = 0.50000\n",
            "Epoch [5/5] Step [787/1610]: discriminator_loss = 0.54304, target_loss = 0.40562, acc = 0.50000\n",
            "Epoch [5/5] Step [788/1610]: discriminator_loss = 0.54801, target_loss = 0.41144, acc = 0.50000\n",
            "Epoch [5/5] Step [789/1610]: discriminator_loss = 0.54962, target_loss = 0.39732, acc = 0.50000\n",
            "Epoch [5/5] Step [790/1610]: discriminator_loss = 0.54576, target_loss = 0.40847, acc = 0.50000\n",
            "Epoch [5/5] Step [791/1610]: discriminator_loss = 0.55446, target_loss = 0.40307, acc = 0.50000\n",
            "Epoch [5/5] Step [792/1610]: discriminator_loss = 0.55481, target_loss = 0.41029, acc = 0.50000\n",
            "Epoch [5/5] Step [793/1610]: discriminator_loss = 0.55957, target_loss = 0.39845, acc = 0.50000\n",
            "Epoch [5/5] Step [794/1610]: discriminator_loss = 0.54742, target_loss = 0.40889, acc = 0.50000\n",
            "Epoch [5/5] Step [795/1610]: discriminator_loss = 0.57013, target_loss = 0.40347, acc = 0.50000\n",
            "Epoch [5/5] Step [796/1610]: discriminator_loss = 0.55951, target_loss = 0.41502, acc = 0.50000\n",
            "Epoch [5/5] Step [797/1610]: discriminator_loss = 0.53242, target_loss = 0.39117, acc = 0.50000\n",
            "Epoch [5/5] Step [798/1610]: discriminator_loss = 0.55832, target_loss = 0.39179, acc = 0.50000\n",
            "Epoch [5/5] Step [799/1610]: discriminator_loss = 0.55576, target_loss = 0.41096, acc = 0.50000\n",
            "Epoch [5/5] Step [800/1610]: discriminator_loss = 0.54909, target_loss = 0.39915, acc = 0.50000\n",
            "Epoch [5/5] Step [801/1610]: discriminator_loss = 0.57493, target_loss = 0.37207, acc = 0.50000\n",
            "Epoch [5/5] Step [802/1610]: discriminator_loss = 0.53665, target_loss = 0.40295, acc = 0.50000\n",
            "Epoch [5/5] Step [803/1610]: discriminator_loss = 0.53841, target_loss = 0.38721, acc = 0.50000\n",
            "Epoch [5/5] Step [804/1610]: discriminator_loss = 0.56718, target_loss = 0.37669, acc = 0.50000\n",
            "Epoch [5/5] Step [805/1610]: discriminator_loss = 0.55191, target_loss = 0.38490, acc = 0.50000\n",
            "Epoch [5/5] Step [806/1610]: discriminator_loss = 0.58469, target_loss = 0.39685, acc = 0.50000\n",
            "Epoch [5/5] Step [807/1610]: discriminator_loss = 0.56195, target_loss = 0.37491, acc = 0.50000\n",
            "Epoch [5/5] Step [808/1610]: discriminator_loss = 0.55908, target_loss = 0.39900, acc = 0.50000\n",
            "Epoch [5/5] Step [809/1610]: discriminator_loss = 0.63543, target_loss = 0.40349, acc = 0.50000\n",
            "Epoch [5/5] Step [810/1610]: discriminator_loss = 0.58057, target_loss = 0.41469, acc = 0.50000\n",
            "Epoch [5/5] Step [811/1610]: discriminator_loss = 0.55281, target_loss = 0.40993, acc = 0.50000\n",
            "Epoch [5/5] Step [812/1610]: discriminator_loss = 0.56817, target_loss = 0.41367, acc = 0.50000\n",
            "Epoch [5/5] Step [813/1610]: discriminator_loss = 0.54221, target_loss = 0.42617, acc = 0.50000\n",
            "Epoch [5/5] Step [814/1610]: discriminator_loss = 0.53728, target_loss = 0.42364, acc = 0.50000\n",
            "Epoch [5/5] Step [815/1610]: discriminator_loss = 0.53605, target_loss = 0.43331, acc = 0.50000\n",
            "Epoch [5/5] Step [816/1610]: discriminator_loss = 0.55741, target_loss = 0.41488, acc = 0.50000\n",
            "Epoch [5/5] Step [817/1610]: discriminator_loss = 0.54724, target_loss = 0.41502, acc = 0.50000\n",
            "Epoch [5/5] Step [818/1610]: discriminator_loss = 0.55651, target_loss = 0.42330, acc = 0.50000\n",
            "Epoch [5/5] Step [819/1610]: discriminator_loss = 0.54771, target_loss = 0.40073, acc = 0.50000\n",
            "Epoch [5/5] Step [820/1610]: discriminator_loss = 0.55522, target_loss = 0.40817, acc = 0.50000\n",
            "Epoch [5/5] Step [821/1610]: discriminator_loss = 0.55604, target_loss = 0.41305, acc = 0.50000\n",
            "Epoch [5/5] Step [822/1610]: discriminator_loss = 0.56467, target_loss = 0.40884, acc = 0.50000\n",
            "Epoch [5/5] Step [823/1610]: discriminator_loss = 0.56925, target_loss = 0.38694, acc = 0.50000\n",
            "Epoch [5/5] Step [824/1610]: discriminator_loss = 0.56103, target_loss = 0.40414, acc = 0.50000\n",
            "Epoch [5/5] Step [825/1610]: discriminator_loss = 0.56182, target_loss = 0.40464, acc = 0.50000\n",
            "Epoch [5/5] Step [826/1610]: discriminator_loss = 0.55260, target_loss = 0.37924, acc = 0.50000\n",
            "Epoch [5/5] Step [827/1610]: discriminator_loss = 0.55823, target_loss = 0.38515, acc = 0.50000\n",
            "Epoch [5/5] Step [828/1610]: discriminator_loss = 0.57793, target_loss = 0.40448, acc = 0.50000\n",
            "Epoch [5/5] Step [829/1610]: discriminator_loss = 0.55364, target_loss = 0.38893, acc = 0.50000\n",
            "Epoch [5/5] Step [830/1610]: discriminator_loss = 0.56167, target_loss = 0.38859, acc = 0.50000\n",
            "Epoch [5/5] Step [831/1610]: discriminator_loss = 0.57847, target_loss = 0.38357, acc = 0.50000\n",
            "Epoch [5/5] Step [832/1610]: discriminator_loss = 0.57607, target_loss = 0.38955, acc = 0.50000\n",
            "Epoch [5/5] Step [833/1610]: discriminator_loss = 0.56459, target_loss = 0.38731, acc = 0.50000\n",
            "Epoch [5/5] Step [834/1610]: discriminator_loss = 0.58291, target_loss = 0.36998, acc = 0.50000\n",
            "Epoch [5/5] Step [835/1610]: discriminator_loss = 0.56365, target_loss = 0.38626, acc = 0.50000\n",
            "Epoch [5/5] Step [836/1610]: discriminator_loss = 0.56500, target_loss = 0.40359, acc = 0.50000\n",
            "Epoch [5/5] Step [837/1610]: discriminator_loss = 0.58554, target_loss = 0.38330, acc = 0.50000\n",
            "Epoch [5/5] Step [838/1610]: discriminator_loss = 0.56061, target_loss = 0.40108, acc = 0.50000\n",
            "Epoch [5/5] Step [839/1610]: discriminator_loss = 0.55818, target_loss = 0.40098, acc = 0.50000\n",
            "Epoch [5/5] Step [840/1610]: discriminator_loss = 0.56574, target_loss = 0.40193, acc = 0.50000\n",
            "Epoch [5/5] Step [841/1610]: discriminator_loss = 0.58819, target_loss = 0.39415, acc = 0.50000\n",
            "Epoch [5/5] Step [842/1610]: discriminator_loss = 0.56549, target_loss = 0.38975, acc = 0.50000\n",
            "Epoch [5/5] Step [843/1610]: discriminator_loss = 0.61791, target_loss = 0.39495, acc = 0.50000\n",
            "Epoch [5/5] Step [844/1610]: discriminator_loss = 0.55543, target_loss = 0.39823, acc = 0.50000\n",
            "Epoch [5/5] Step [845/1610]: discriminator_loss = 0.58836, target_loss = 0.39202, acc = 0.50000\n",
            "Epoch [5/5] Step [846/1610]: discriminator_loss = 0.60541, target_loss = 0.39309, acc = 0.50000\n",
            "Epoch [5/5] Step [847/1610]: discriminator_loss = 0.54582, target_loss = 0.41362, acc = 0.50000\n",
            "Epoch [5/5] Step [848/1610]: discriminator_loss = 0.55123, target_loss = 0.42365, acc = 0.50000\n",
            "Epoch [5/5] Step [849/1610]: discriminator_loss = 0.54608, target_loss = 0.40623, acc = 0.50000\n",
            "Epoch [5/5] Step [850/1610]: discriminator_loss = 0.55329, target_loss = 0.42427, acc = 0.50000\n",
            "Epoch [5/5] Step [851/1610]: discriminator_loss = 0.56997, target_loss = 0.40299, acc = 0.50000\n",
            "Epoch [5/5] Step [852/1610]: discriminator_loss = 0.55260, target_loss = 0.40714, acc = 0.50000\n",
            "Epoch [5/5] Step [853/1610]: discriminator_loss = 0.55102, target_loss = 0.42210, acc = 0.50000\n",
            "Epoch [5/5] Step [854/1610]: discriminator_loss = 0.56492, target_loss = 0.40662, acc = 0.50000\n",
            "Epoch [5/5] Step [855/1610]: discriminator_loss = 0.54707, target_loss = 0.41012, acc = 0.50000\n",
            "Epoch [5/5] Step [856/1610]: discriminator_loss = 0.56578, target_loss = 0.41595, acc = 0.50000\n",
            "Epoch [5/5] Step [857/1610]: discriminator_loss = 0.55696, target_loss = 0.42110, acc = 0.50000\n",
            "Epoch [5/5] Step [858/1610]: discriminator_loss = 0.56190, target_loss = 0.44014, acc = 0.50000\n",
            "Epoch [5/5] Step [859/1610]: discriminator_loss = 0.53654, target_loss = 0.42104, acc = 0.50000\n",
            "Epoch [5/5] Step [860/1610]: discriminator_loss = 0.56362, target_loss = 0.40498, acc = 0.50000\n",
            "Epoch [5/5] Step [861/1610]: discriminator_loss = 0.54174, target_loss = 0.40712, acc = 0.50000\n",
            "Epoch [5/5] Step [862/1610]: discriminator_loss = 0.54212, target_loss = 0.42473, acc = 0.50000\n",
            "Epoch [5/5] Step [863/1610]: discriminator_loss = 0.56206, target_loss = 0.41055, acc = 0.50000\n",
            "Epoch [5/5] Step [864/1610]: discriminator_loss = 0.55719, target_loss = 0.39047, acc = 0.50000\n",
            "Epoch [5/5] Step [865/1610]: discriminator_loss = 0.56727, target_loss = 0.40384, acc = 0.50000\n",
            "Epoch [5/5] Step [866/1610]: discriminator_loss = 0.58104, target_loss = 0.39628, acc = 0.50000\n",
            "Epoch [5/5] Step [867/1610]: discriminator_loss = 0.56149, target_loss = 0.39556, acc = 0.50000\n",
            "Epoch [5/5] Step [868/1610]: discriminator_loss = 0.57026, target_loss = 0.39494, acc = 0.50000\n",
            "Epoch [5/5] Step [869/1610]: discriminator_loss = 0.56430, target_loss = 0.41080, acc = 0.50000\n",
            "Epoch [5/5] Step [870/1610]: discriminator_loss = 0.56544, target_loss = 0.39537, acc = 0.50000\n",
            "Epoch [5/5] Step [871/1610]: discriminator_loss = 0.56935, target_loss = 0.39017, acc = 0.50000\n",
            "Epoch [5/5] Step [872/1610]: discriminator_loss = 0.57026, target_loss = 0.38289, acc = 0.50000\n",
            "Epoch [5/5] Step [873/1610]: discriminator_loss = 0.57087, target_loss = 0.35438, acc = 0.50000\n",
            "Epoch [5/5] Step [874/1610]: discriminator_loss = 0.56722, target_loss = 0.38335, acc = 0.50000\n",
            "Epoch [5/5] Step [875/1610]: discriminator_loss = 0.56516, target_loss = 0.39206, acc = 0.50000\n",
            "Epoch [5/5] Step [876/1610]: discriminator_loss = 0.56846, target_loss = 0.38128, acc = 0.50000\n",
            "Epoch [5/5] Step [877/1610]: discriminator_loss = 0.55805, target_loss = 0.39049, acc = 0.50000\n",
            "Epoch [5/5] Step [878/1610]: discriminator_loss = 0.59138, target_loss = 0.38269, acc = 0.50000\n",
            "Epoch [5/5] Step [879/1610]: discriminator_loss = 0.56739, target_loss = 0.36625, acc = 0.50000\n",
            "Epoch [5/5] Step [880/1610]: discriminator_loss = 0.65802, target_loss = 0.38750, acc = 0.50000\n",
            "Epoch [5/5] Step [881/1610]: discriminator_loss = 0.59292, target_loss = 0.41127, acc = 0.50000\n",
            "Epoch [5/5] Step [882/1610]: discriminator_loss = 0.56354, target_loss = 0.38273, acc = 0.50000\n",
            "Epoch [5/5] Step [883/1610]: discriminator_loss = 0.56501, target_loss = 0.39924, acc = 0.50000\n",
            "Epoch [5/5] Step [884/1610]: discriminator_loss = 0.54115, target_loss = 0.41979, acc = 0.50000\n",
            "Epoch [5/5] Step [885/1610]: discriminator_loss = 0.55494, target_loss = 0.40451, acc = 0.50000\n",
            "Epoch [5/5] Step [886/1610]: discriminator_loss = 0.54405, target_loss = 0.41158, acc = 0.50000\n",
            "Epoch [5/5] Step [887/1610]: discriminator_loss = 0.55327, target_loss = 0.41070, acc = 0.50000\n",
            "Epoch [5/5] Step [888/1610]: discriminator_loss = 0.54235, target_loss = 0.40809, acc = 0.50000\n",
            "Epoch [5/5] Step [889/1610]: discriminator_loss = 0.53730, target_loss = 0.42570, acc = 0.50000\n",
            "Epoch [5/5] Step [890/1610]: discriminator_loss = 0.53922, target_loss = 0.41975, acc = 0.50000\n",
            "Epoch [5/5] Step [891/1610]: discriminator_loss = 0.54813, target_loss = 0.40878, acc = 0.50000\n",
            "Epoch [5/5] Step [892/1610]: discriminator_loss = 0.53885, target_loss = 0.41044, acc = 0.50000\n",
            "Epoch [5/5] Step [893/1610]: discriminator_loss = 0.54525, target_loss = 0.41749, acc = 0.50000\n",
            "Epoch [5/5] Step [894/1610]: discriminator_loss = 0.57109, target_loss = 0.41170, acc = 0.50000\n",
            "Epoch [5/5] Step [895/1610]: discriminator_loss = 0.53865, target_loss = 0.41047, acc = 0.50000\n",
            "Epoch [5/5] Step [896/1610]: discriminator_loss = 0.54559, target_loss = 0.40440, acc = 0.50000\n",
            "Epoch [5/5] Step [897/1610]: discriminator_loss = 0.55914, target_loss = 0.41431, acc = 0.50000\n",
            "Epoch [5/5] Step [898/1610]: discriminator_loss = 0.53876, target_loss = 0.40358, acc = 0.50000\n",
            "Epoch [5/5] Step [899/1610]: discriminator_loss = 0.56034, target_loss = 0.40393, acc = 0.50000\n",
            "Epoch [5/5] Step [900/1610]: discriminator_loss = 0.55452, target_loss = 0.41005, acc = 0.50000\n",
            "Epoch [5/5] Step [901/1610]: discriminator_loss = 0.55581, target_loss = 0.37190, acc = 0.50000\n",
            "Epoch [5/5] Step [902/1610]: discriminator_loss = 0.56419, target_loss = 0.38400, acc = 0.50000\n",
            "Epoch [5/5] Step [903/1610]: discriminator_loss = 0.55278, target_loss = 0.34940, acc = 0.50000\n",
            "Epoch [5/5] Step [904/1610]: discriminator_loss = 0.56456, target_loss = 0.34673, acc = 0.50000\n",
            "Epoch [5/5] Step [905/1610]: discriminator_loss = 0.57935, target_loss = 0.38163, acc = 0.50000\n",
            "Epoch [5/5] Step [906/1610]: discriminator_loss = 0.56267, target_loss = 0.38232, acc = 0.50000\n",
            "Epoch [5/5] Step [907/1610]: discriminator_loss = 0.56031, target_loss = 0.38724, acc = 0.50000\n",
            "Epoch [5/5] Step [908/1610]: discriminator_loss = 0.54702, target_loss = 0.35645, acc = 0.50000\n",
            "Epoch [5/5] Step [909/1610]: discriminator_loss = 0.64119, target_loss = 0.38804, acc = 0.50000\n",
            "Epoch [5/5] Step [910/1610]: discriminator_loss = 0.62652, target_loss = 0.39836, acc = 0.50000\n",
            "Epoch [5/5] Step [911/1610]: discriminator_loss = 0.63180, target_loss = 0.38821, acc = 0.50000\n",
            "Epoch [5/5] Step [912/1610]: discriminator_loss = 0.53487, target_loss = 0.40896, acc = 0.50000\n",
            "Epoch [5/5] Step [913/1610]: discriminator_loss = 0.57134, target_loss = 0.43848, acc = 0.50000\n",
            "Epoch [5/5] Step [914/1610]: discriminator_loss = 0.58145, target_loss = 0.42113, acc = 0.50000\n",
            "Epoch [5/5] Step [915/1610]: discriminator_loss = 0.53443, target_loss = 0.43870, acc = 0.50000\n",
            "Epoch [5/5] Step [916/1610]: discriminator_loss = 0.53678, target_loss = 0.41997, acc = 0.50000\n",
            "Epoch [5/5] Step [917/1610]: discriminator_loss = 0.54564, target_loss = 0.40982, acc = 0.50000\n",
            "Epoch [5/5] Step [918/1610]: discriminator_loss = 0.53657, target_loss = 0.44233, acc = 0.50000\n",
            "Epoch [5/5] Step [919/1610]: discriminator_loss = 0.54440, target_loss = 0.42141, acc = 0.50000\n",
            "Epoch [5/5] Step [920/1610]: discriminator_loss = 0.53535, target_loss = 0.43066, acc = 0.50000\n",
            "Epoch [5/5] Step [921/1610]: discriminator_loss = 0.52126, target_loss = 0.40906, acc = 0.50000\n",
            "Epoch [5/5] Step [922/1610]: discriminator_loss = 0.55945, target_loss = 0.41693, acc = 0.50000\n",
            "Epoch [5/5] Step [923/1610]: discriminator_loss = 0.54679, target_loss = 0.41764, acc = 0.50000\n",
            "Epoch [5/5] Step [924/1610]: discriminator_loss = 0.55576, target_loss = 0.42945, acc = 0.50000\n",
            "Epoch [5/5] Step [925/1610]: discriminator_loss = 0.55711, target_loss = 0.42207, acc = 0.50000\n",
            "Epoch [5/5] Step [926/1610]: discriminator_loss = 0.57035, target_loss = 0.40815, acc = 0.50000\n",
            "Epoch [5/5] Step [927/1610]: discriminator_loss = 0.54323, target_loss = 0.40954, acc = 0.50000\n",
            "Epoch [5/5] Step [928/1610]: discriminator_loss = 0.55389, target_loss = 0.40105, acc = 0.50000\n",
            "Epoch [5/5] Step [929/1610]: discriminator_loss = 0.56659, target_loss = 0.39252, acc = 0.50000\n",
            "Epoch [5/5] Step [930/1610]: discriminator_loss = 0.56122, target_loss = 0.40119, acc = 0.50000\n",
            "Epoch [5/5] Step [931/1610]: discriminator_loss = 0.57049, target_loss = 0.39278, acc = 0.50000\n",
            "Epoch [5/5] Step [932/1610]: discriminator_loss = 0.58086, target_loss = 0.38209, acc = 0.50000\n",
            "Epoch [5/5] Step [933/1610]: discriminator_loss = 0.56818, target_loss = 0.38560, acc = 0.50000\n",
            "Epoch [5/5] Step [934/1610]: discriminator_loss = 0.56161, target_loss = 0.40147, acc = 0.50000\n",
            "Epoch [5/5] Step [935/1610]: discriminator_loss = 0.56521, target_loss = 0.38685, acc = 0.50000\n",
            "Epoch [5/5] Step [936/1610]: discriminator_loss = 0.56910, target_loss = 0.39251, acc = 0.50000\n",
            "Epoch [5/5] Step [937/1610]: discriminator_loss = 0.61525, target_loss = 0.39233, acc = 0.50000\n",
            "Epoch [5/5] Step [938/1610]: discriminator_loss = 0.58112, target_loss = 0.38564, acc = 0.50000\n",
            "Epoch [5/5] Step [939/1610]: discriminator_loss = 0.58049, target_loss = 0.38664, acc = 0.50000\n",
            "Epoch [5/5] Step [940/1610]: discriminator_loss = 0.56640, target_loss = 0.38910, acc = 0.50000\n",
            "Epoch [5/5] Step [941/1610]: discriminator_loss = 0.56559, target_loss = 0.39719, acc = 0.50000\n",
            "Epoch [5/5] Step [942/1610]: discriminator_loss = 0.56384, target_loss = 0.39535, acc = 0.50000\n",
            "Epoch [5/5] Step [943/1610]: discriminator_loss = 0.56910, target_loss = 0.38758, acc = 0.50000\n",
            "Epoch [5/5] Step [944/1610]: discriminator_loss = 0.57965, target_loss = 0.39738, acc = 0.50000\n",
            "Epoch [5/5] Step [945/1610]: discriminator_loss = 0.57559, target_loss = 0.38315, acc = 0.50000\n",
            "Epoch [5/5] Step [946/1610]: discriminator_loss = 0.57277, target_loss = 0.37660, acc = 0.50000\n",
            "Epoch [5/5] Step [947/1610]: discriminator_loss = 0.55393, target_loss = 0.39269, acc = 0.50000\n",
            "Epoch [5/5] Step [948/1610]: discriminator_loss = 0.63940, target_loss = 0.39613, acc = 0.50000\n",
            "Epoch [5/5] Step [949/1610]: discriminator_loss = 0.56041, target_loss = 0.40176, acc = 0.50000\n",
            "Epoch [5/5] Step [950/1610]: discriminator_loss = 0.54042, target_loss = 0.39687, acc = 0.50000\n",
            "Epoch [5/5] Step [951/1610]: discriminator_loss = 0.56788, target_loss = 0.40990, acc = 0.50000\n",
            "Epoch [5/5] Step [952/1610]: discriminator_loss = 0.58146, target_loss = 0.41941, acc = 0.50000\n",
            "Epoch [5/5] Step [953/1610]: discriminator_loss = 0.54822, target_loss = 0.41053, acc = 0.50000\n",
            "Epoch [5/5] Step [954/1610]: discriminator_loss = 0.54001, target_loss = 0.40804, acc = 0.50000\n",
            "Epoch [5/5] Step [955/1610]: discriminator_loss = 0.54903, target_loss = 0.40380, acc = 0.50000\n",
            "Epoch [5/5] Step [956/1610]: discriminator_loss = 0.55617, target_loss = 0.41318, acc = 0.50000\n",
            "Epoch [5/5] Step [957/1610]: discriminator_loss = 0.54922, target_loss = 0.41753, acc = 0.50000\n",
            "Epoch [5/5] Step [958/1610]: discriminator_loss = 0.57164, target_loss = 0.39511, acc = 0.50000\n",
            "Epoch [5/5] Step [959/1610]: discriminator_loss = 0.56310, target_loss = 0.40566, acc = 0.50000\n",
            "Epoch [5/5] Step [960/1610]: discriminator_loss = 0.53705, target_loss = 0.42044, acc = 0.50000\n",
            "Epoch [5/5] Step [961/1610]: discriminator_loss = 0.53432, target_loss = 0.42986, acc = 0.50000\n",
            "Epoch [5/5] Step [962/1610]: discriminator_loss = 0.53295, target_loss = 0.42313, acc = 0.50000\n",
            "Epoch [5/5] Step [963/1610]: discriminator_loss = 0.54341, target_loss = 0.43070, acc = 0.50000\n",
            "Epoch [5/5] Step [964/1610]: discriminator_loss = 0.55191, target_loss = 0.40588, acc = 0.50000\n",
            "Epoch [5/5] Step [965/1610]: discriminator_loss = 0.55069, target_loss = 0.42646, acc = 0.50000\n",
            "Epoch [5/5] Step [966/1610]: discriminator_loss = 0.54248, target_loss = 0.41410, acc = 0.50000\n",
            "Epoch [5/5] Step [967/1610]: discriminator_loss = 0.56535, target_loss = 0.41704, acc = 0.50000\n",
            "Epoch [5/5] Step [968/1610]: discriminator_loss = 0.53463, target_loss = 0.41726, acc = 0.50000\n",
            "Epoch [5/5] Step [969/1610]: discriminator_loss = 0.54756, target_loss = 0.41263, acc = 0.50000\n",
            "Epoch [5/5] Step [970/1610]: discriminator_loss = 0.58779, target_loss = 0.43126, acc = 0.50000\n",
            "Epoch [5/5] Step [971/1610]: discriminator_loss = 0.54672, target_loss = 0.40515, acc = 0.50000\n",
            "Epoch [5/5] Step [972/1610]: discriminator_loss = 0.54877, target_loss = 0.40823, acc = 0.50000\n",
            "Epoch [5/5] Step [973/1610]: discriminator_loss = 0.54757, target_loss = 0.41032, acc = 0.50000\n",
            "Epoch [5/5] Step [974/1610]: discriminator_loss = 0.55994, target_loss = 0.40520, acc = 0.50000\n",
            "Epoch [5/5] Step [975/1610]: discriminator_loss = 0.55046, target_loss = 0.39831, acc = 0.50000\n",
            "Epoch [5/5] Step [976/1610]: discriminator_loss = 0.55345, target_loss = 0.40004, acc = 0.50000\n",
            "Epoch [5/5] Step [977/1610]: discriminator_loss = 0.55885, target_loss = 0.40640, acc = 0.50000\n",
            "Epoch [5/5] Step [978/1610]: discriminator_loss = 0.54502, target_loss = 0.40068, acc = 0.50000\n",
            "Epoch [5/5] Step [979/1610]: discriminator_loss = 0.55813, target_loss = 0.40177, acc = 0.50000\n",
            "Epoch [5/5] Step [980/1610]: discriminator_loss = 0.56608, target_loss = 0.39706, acc = 0.50000\n",
            "Epoch [5/5] Step [981/1610]: discriminator_loss = 0.55805, target_loss = 0.40702, acc = 0.50000\n",
            "Epoch [5/5] Step [982/1610]: discriminator_loss = 0.55268, target_loss = 0.40781, acc = 0.50000\n",
            "Epoch [5/5] Step [983/1610]: discriminator_loss = 0.56291, target_loss = 0.40684, acc = 0.50000\n",
            "Epoch [5/5] Step [984/1610]: discriminator_loss = 0.54772, target_loss = 0.40496, acc = 0.50000\n",
            "Epoch [5/5] Step [985/1610]: discriminator_loss = 0.56969, target_loss = 0.38996, acc = 0.50000\n",
            "Epoch [5/5] Step [986/1610]: discriminator_loss = 0.56181, target_loss = 0.40406, acc = 0.50000\n",
            "Epoch [5/5] Step [987/1610]: discriminator_loss = 0.55429, target_loss = 0.40005, acc = 0.50000\n",
            "Epoch [5/5] Step [988/1610]: discriminator_loss = 0.58147, target_loss = 0.40131, acc = 0.50000\n",
            "Epoch [5/5] Step [989/1610]: discriminator_loss = 0.56001, target_loss = 0.39462, acc = 0.50000\n",
            "Epoch [5/5] Step [990/1610]: discriminator_loss = 0.55578, target_loss = 0.41084, acc = 0.50000\n",
            "Epoch [5/5] Step [991/1610]: discriminator_loss = 0.55252, target_loss = 0.41216, acc = 0.50000\n",
            "Epoch [5/5] Step [992/1610]: discriminator_loss = 0.56190, target_loss = 0.40256, acc = 0.50000\n",
            "Epoch [5/5] Step [993/1610]: discriminator_loss = 0.54854, target_loss = 0.37402, acc = 0.50000\n",
            "Epoch [5/5] Step [994/1610]: discriminator_loss = 0.56211, target_loss = 0.39289, acc = 0.50000\n",
            "Epoch [5/5] Step [995/1610]: discriminator_loss = 0.54772, target_loss = 0.39591, acc = 0.50000\n",
            "Epoch [5/5] Step [996/1610]: discriminator_loss = 0.55698, target_loss = 0.40430, acc = 0.50000\n",
            "Epoch [5/5] Step [997/1610]: discriminator_loss = 0.55285, target_loss = 0.39980, acc = 0.50000\n",
            "Epoch [5/5] Step [998/1610]: discriminator_loss = 0.55015, target_loss = 0.40422, acc = 0.50000\n",
            "Epoch [5/5] Step [999/1610]: discriminator_loss = 0.62346, target_loss = 0.40335, acc = 0.50000\n",
            "Epoch [5/5] Step [1000/1610]: discriminator_loss = 0.55527, target_loss = 0.41496, acc = 0.50000\n",
            "Epoch [5/5] Step [1001/1610]: discriminator_loss = 0.57404, target_loss = 0.40449, acc = 0.50000\n",
            "Epoch [5/5] Step [1002/1610]: discriminator_loss = 0.54604, target_loss = 0.40609, acc = 0.50000\n",
            "Epoch [5/5] Step [1003/1610]: discriminator_loss = 0.55474, target_loss = 0.39918, acc = 0.50000\n",
            "Epoch [5/5] Step [1004/1610]: discriminator_loss = 0.55802, target_loss = 0.39309, acc = 0.50000\n",
            "Epoch [5/5] Step [1005/1610]: discriminator_loss = 0.59958, target_loss = 0.40056, acc = 0.50000\n",
            "Epoch [5/5] Step [1006/1610]: discriminator_loss = 0.55959, target_loss = 0.40003, acc = 0.50000\n",
            "Epoch [5/5] Step [1007/1610]: discriminator_loss = 0.55811, target_loss = 0.40128, acc = 0.50000\n",
            "Epoch [5/5] Step [1008/1610]: discriminator_loss = 0.54566, target_loss = 0.41008, acc = 0.50000\n",
            "Epoch [5/5] Step [1009/1610]: discriminator_loss = 0.54866, target_loss = 0.41481, acc = 0.50000\n",
            "Epoch [5/5] Step [1010/1610]: discriminator_loss = 0.55622, target_loss = 0.40234, acc = 0.50000\n",
            "Epoch [5/5] Step [1011/1610]: discriminator_loss = 0.54493, target_loss = 0.41711, acc = 0.50000\n",
            "Epoch [5/5] Step [1012/1610]: discriminator_loss = 0.56622, target_loss = 0.39445, acc = 0.50000\n",
            "Epoch [5/5] Step [1013/1610]: discriminator_loss = 0.56038, target_loss = 0.40144, acc = 0.50000\n",
            "Epoch [5/5] Step [1014/1610]: discriminator_loss = 0.55916, target_loss = 0.42148, acc = 0.50000\n",
            "Epoch [5/5] Step [1015/1610]: discriminator_loss = 0.54473, target_loss = 0.37976, acc = 0.50000\n",
            "Epoch [5/5] Step [1016/1610]: discriminator_loss = 0.54878, target_loss = 0.40517, acc = 0.50000\n",
            "Epoch [5/5] Step [1017/1610]: discriminator_loss = 0.55761, target_loss = 0.40668, acc = 0.50000\n",
            "Epoch [5/5] Step [1018/1610]: discriminator_loss = 0.55896, target_loss = 0.39938, acc = 0.50000\n",
            "Epoch [5/5] Step [1019/1610]: discriminator_loss = 0.55871, target_loss = 0.40668, acc = 0.50000\n",
            "Epoch [5/5] Step [1020/1610]: discriminator_loss = 0.56832, target_loss = 0.40030, acc = 0.50000\n",
            "Epoch [5/5] Step [1021/1610]: discriminator_loss = 0.55994, target_loss = 0.40150, acc = 0.50000\n",
            "Epoch [5/5] Step [1022/1610]: discriminator_loss = 0.56081, target_loss = 0.38812, acc = 0.50000\n",
            "Epoch [5/5] Step [1023/1610]: discriminator_loss = 0.57169, target_loss = 0.39924, acc = 0.50000\n",
            "Epoch [5/5] Step [1024/1610]: discriminator_loss = 0.56234, target_loss = 0.41395, acc = 0.50000\n",
            "Epoch [5/5] Step [1025/1610]: discriminator_loss = 0.56553, target_loss = 0.40744, acc = 0.50000\n",
            "Epoch [5/5] Step [1026/1610]: discriminator_loss = 0.55933, target_loss = 0.39840, acc = 0.50000\n",
            "Epoch [5/5] Step [1027/1610]: discriminator_loss = 0.55577, target_loss = 0.40812, acc = 0.50000\n",
            "Epoch [5/5] Step [1028/1610]: discriminator_loss = 0.55617, target_loss = 0.39815, acc = 0.50000\n",
            "Epoch [5/5] Step [1029/1610]: discriminator_loss = 0.54939, target_loss = 0.39034, acc = 0.50000\n",
            "Epoch [5/5] Step [1030/1610]: discriminator_loss = 0.55665, target_loss = 0.39155, acc = 0.50000\n",
            "Epoch [5/5] Step [1031/1610]: discriminator_loss = 0.57223, target_loss = 0.38071, acc = 0.50000\n",
            "Epoch [5/5] Step [1032/1610]: discriminator_loss = 0.54967, target_loss = 0.40088, acc = 0.50000\n",
            "Epoch [5/5] Step [1033/1610]: discriminator_loss = 0.56001, target_loss = 0.39696, acc = 0.50000\n",
            "Epoch [5/5] Step [1034/1610]: discriminator_loss = 0.57072, target_loss = 0.40662, acc = 0.50000\n",
            "Epoch [5/5] Step [1035/1610]: discriminator_loss = 0.55955, target_loss = 0.40814, acc = 0.50000\n",
            "Epoch [5/5] Step [1036/1610]: discriminator_loss = 0.55454, target_loss = 0.39920, acc = 0.50000\n",
            "Epoch [5/5] Step [1037/1610]: discriminator_loss = 0.55106, target_loss = 0.41232, acc = 0.50000\n",
            "Epoch [5/5] Step [1038/1610]: discriminator_loss = 0.66112, target_loss = 0.39227, acc = 0.50000\n",
            "Epoch [5/5] Step [1039/1610]: discriminator_loss = 0.54615, target_loss = 0.41066, acc = 0.50000\n",
            "Epoch [5/5] Step [1040/1610]: discriminator_loss = 0.55823, target_loss = 0.40340, acc = 0.50000\n",
            "Epoch [5/5] Step [1041/1610]: discriminator_loss = 0.56343, target_loss = 0.39809, acc = 0.50000\n",
            "Epoch [5/5] Step [1042/1610]: discriminator_loss = 0.55424, target_loss = 0.41491, acc = 0.50000\n",
            "Epoch [5/5] Step [1043/1610]: discriminator_loss = 0.55366, target_loss = 0.39879, acc = 0.50000\n",
            "Epoch [5/5] Step [1044/1610]: discriminator_loss = 0.54202, target_loss = 0.41060, acc = 0.50000\n",
            "Epoch [5/5] Step [1045/1610]: discriminator_loss = 0.54913, target_loss = 0.40831, acc = 0.50000\n",
            "Epoch [5/5] Step [1046/1610]: discriminator_loss = 0.55610, target_loss = 0.40020, acc = 0.50000\n",
            "Epoch [5/5] Step [1047/1610]: discriminator_loss = 0.56054, target_loss = 0.40999, acc = 0.50000\n",
            "Epoch [5/5] Step [1048/1610]: discriminator_loss = 0.54585, target_loss = 0.41629, acc = 0.50000\n",
            "Epoch [5/5] Step [1049/1610]: discriminator_loss = 0.54515, target_loss = 0.41534, acc = 0.50000\n",
            "Epoch [5/5] Step [1050/1610]: discriminator_loss = 0.55385, target_loss = 0.40849, acc = 0.50000\n",
            "Epoch [5/5] Step [1051/1610]: discriminator_loss = 0.58291, target_loss = 0.41285, acc = 0.50000\n",
            "Epoch [5/5] Step [1052/1610]: discriminator_loss = 0.54742, target_loss = 0.41579, acc = 0.50000\n",
            "Epoch [5/5] Step [1053/1610]: discriminator_loss = 0.56256, target_loss = 0.39533, acc = 0.50000\n",
            "Epoch [5/5] Step [1054/1610]: discriminator_loss = 0.54483, target_loss = 0.39650, acc = 0.50000\n",
            "Epoch [5/5] Step [1055/1610]: discriminator_loss = 0.54840, target_loss = 0.39197, acc = 0.50000\n",
            "Epoch [5/5] Step [1056/1610]: discriminator_loss = 0.55507, target_loss = 0.37475, acc = 0.50000\n",
            "Epoch [5/5] Step [1057/1610]: discriminator_loss = 0.54110, target_loss = 0.41743, acc = 0.50000\n",
            "Epoch [5/5] Step [1058/1610]: discriminator_loss = 0.56279, target_loss = 0.40894, acc = 0.50000\n",
            "Epoch [5/5] Step [1059/1610]: discriminator_loss = 0.56617, target_loss = 0.39776, acc = 0.50000\n",
            "Epoch [5/5] Step [1060/1610]: discriminator_loss = 0.55402, target_loss = 0.39776, acc = 0.50000\n",
            "Epoch [5/5] Step [1061/1610]: discriminator_loss = 0.56758, target_loss = 0.41212, acc = 0.50000\n",
            "Epoch [5/5] Step [1062/1610]: discriminator_loss = 0.57096, target_loss = 0.41262, acc = 0.50000\n",
            "Epoch [5/5] Step [1063/1610]: discriminator_loss = 0.56081, target_loss = 0.39592, acc = 0.50000\n",
            "Epoch [5/5] Step [1064/1610]: discriminator_loss = 0.56385, target_loss = 0.40946, acc = 0.50000\n",
            "Epoch [5/5] Step [1065/1610]: discriminator_loss = 0.57603, target_loss = 0.40541, acc = 0.50000\n",
            "Epoch [5/5] Step [1066/1610]: discriminator_loss = 0.55356, target_loss = 0.40038, acc = 0.50000\n",
            "Epoch [5/5] Step [1067/1610]: discriminator_loss = 0.55433, target_loss = 0.40766, acc = 0.50000\n",
            "Epoch [5/5] Step [1068/1610]: discriminator_loss = 0.56258, target_loss = 0.39130, acc = 0.50000\n",
            "Epoch [5/5] Step [1069/1610]: discriminator_loss = 0.56707, target_loss = 0.38204, acc = 0.50000\n",
            "Epoch [5/5] Step [1070/1610]: discriminator_loss = 0.56806, target_loss = 0.38542, acc = 0.50000\n",
            "Epoch [5/5] Step [1071/1610]: discriminator_loss = 0.56416, target_loss = 0.39621, acc = 0.50000\n",
            "Epoch [5/5] Step [1072/1610]: discriminator_loss = 0.56490, target_loss = 0.39437, acc = 0.50000\n",
            "Epoch [5/5] Step [1073/1610]: discriminator_loss = 0.55733, target_loss = 0.39744, acc = 0.50000\n",
            "Epoch [5/5] Step [1074/1610]: discriminator_loss = 0.55280, target_loss = 0.39403, acc = 0.50000\n",
            "Epoch [5/5] Step [1075/1610]: discriminator_loss = 0.54636, target_loss = 0.40398, acc = 0.50000\n",
            "Epoch [5/5] Step [1076/1610]: discriminator_loss = 0.59193, target_loss = 0.41446, acc = 0.50000\n",
            "Epoch [5/5] Step [1077/1610]: discriminator_loss = 0.57168, target_loss = 0.41774, acc = 0.50000\n",
            "Epoch [5/5] Step [1078/1610]: discriminator_loss = 0.59314, target_loss = 0.39299, acc = 0.50000\n",
            "Epoch [5/5] Step [1079/1610]: discriminator_loss = 0.56262, target_loss = 0.39545, acc = 0.50000\n",
            "Epoch [5/5] Step [1080/1610]: discriminator_loss = 0.55007, target_loss = 0.40697, acc = 0.50000\n",
            "Epoch [5/5] Step [1081/1610]: discriminator_loss = 0.56376, target_loss = 0.40027, acc = 0.50000\n",
            "Epoch [5/5] Step [1082/1610]: discriminator_loss = 0.55523, target_loss = 0.39977, acc = 0.50000\n",
            "Epoch [5/5] Step [1083/1610]: discriminator_loss = 0.56703, target_loss = 0.39655, acc = 0.50000\n",
            "Epoch [5/5] Step [1084/1610]: discriminator_loss = 0.55309, target_loss = 0.40647, acc = 0.50000\n",
            "Epoch [5/5] Step [1085/1610]: discriminator_loss = 0.55756, target_loss = 0.39817, acc = 0.50000\n",
            "Epoch [5/5] Step [1086/1610]: discriminator_loss = 0.55509, target_loss = 0.39221, acc = 0.50000\n",
            "Epoch [5/5] Step [1087/1610]: discriminator_loss = 0.55965, target_loss = 0.40575, acc = 0.50000\n",
            "Epoch [5/5] Step [1088/1610]: discriminator_loss = 0.57726, target_loss = 0.40180, acc = 0.50000\n",
            "Epoch [5/5] Step [1089/1610]: discriminator_loss = 0.55605, target_loss = 0.38484, acc = 0.50000\n",
            "Epoch [5/5] Step [1090/1610]: discriminator_loss = 0.56175, target_loss = 0.40948, acc = 0.50000\n",
            "Epoch [5/5] Step [1091/1610]: discriminator_loss = 0.54719, target_loss = 0.40860, acc = 0.50000\n",
            "Epoch [5/5] Step [1092/1610]: discriminator_loss = 0.54669, target_loss = 0.41864, acc = 0.50000\n",
            "Epoch [5/5] Step [1093/1610]: discriminator_loss = 0.55227, target_loss = 0.41094, acc = 0.50000\n",
            "Epoch [5/5] Step [1094/1610]: discriminator_loss = 0.59057, target_loss = 0.40210, acc = 0.50000\n",
            "Epoch [5/5] Step [1095/1610]: discriminator_loss = 0.57053, target_loss = 0.39787, acc = 0.50000\n",
            "Epoch [5/5] Step [1096/1610]: discriminator_loss = 0.55275, target_loss = 0.41438, acc = 0.50000\n",
            "Epoch [5/5] Step [1097/1610]: discriminator_loss = 0.61909, target_loss = 0.36384, acc = 0.50000\n",
            "Epoch [5/5] Step [1098/1610]: discriminator_loss = 0.54106, target_loss = 0.41810, acc = 0.50000\n",
            "Epoch [5/5] Step [1099/1610]: discriminator_loss = 0.54351, target_loss = 0.42711, acc = 0.50000\n",
            "Epoch [5/5] Step [1100/1610]: discriminator_loss = 0.55755, target_loss = 0.40390, acc = 0.50000\n",
            "Epoch [5/5] Step [1101/1610]: discriminator_loss = 0.56319, target_loss = 0.40116, acc = 0.50000\n",
            "Epoch [5/5] Step [1102/1610]: discriminator_loss = 0.55735, target_loss = 0.40338, acc = 0.50000\n",
            "Epoch [5/5] Step [1103/1610]: discriminator_loss = 0.55912, target_loss = 0.40105, acc = 0.50000\n",
            "Epoch [5/5] Step [1104/1610]: discriminator_loss = 0.55666, target_loss = 0.39898, acc = 0.50000\n",
            "Epoch [5/5] Step [1105/1610]: discriminator_loss = 0.55846, target_loss = 0.39468, acc = 0.50000\n",
            "Epoch [5/5] Step [1106/1610]: discriminator_loss = 0.56839, target_loss = 0.39830, acc = 0.50000\n",
            "Epoch [5/5] Step [1107/1610]: discriminator_loss = 0.55637, target_loss = 0.40510, acc = 0.50000\n",
            "Epoch [5/5] Step [1108/1610]: discriminator_loss = 0.57204, target_loss = 0.40168, acc = 0.50000\n",
            "Epoch [5/5] Step [1109/1610]: discriminator_loss = 0.57542, target_loss = 0.40029, acc = 0.50000\n",
            "Epoch [5/5] Step [1110/1610]: discriminator_loss = 0.56201, target_loss = 0.38687, acc = 0.50000\n",
            "Epoch [5/5] Step [1111/1610]: discriminator_loss = 0.57987, target_loss = 0.38873, acc = 0.50000\n",
            "Epoch [5/5] Step [1112/1610]: discriminator_loss = 0.57483, target_loss = 0.38721, acc = 0.50000\n",
            "Epoch [5/5] Step [1113/1610]: discriminator_loss = 0.57606, target_loss = 0.37168, acc = 0.50000\n",
            "Epoch [5/5] Step [1114/1610]: discriminator_loss = 0.59946, target_loss = 0.38703, acc = 0.50000\n",
            "Epoch [5/5] Step [1115/1610]: discriminator_loss = 0.58792, target_loss = 0.39373, acc = 0.50000\n",
            "Epoch [5/5] Step [1116/1610]: discriminator_loss = 0.56442, target_loss = 0.39245, acc = 0.50000\n",
            "Epoch [5/5] Step [1117/1610]: discriminator_loss = 0.57353, target_loss = 0.39309, acc = 0.50000\n",
            "Epoch [5/5] Step [1118/1610]: discriminator_loss = 0.57865, target_loss = 0.38401, acc = 0.50000\n",
            "Epoch [5/5] Step [1119/1610]: discriminator_loss = 0.59008, target_loss = 0.38352, acc = 0.50000\n",
            "Epoch [5/5] Step [1120/1610]: discriminator_loss = 0.57172, target_loss = 0.39090, acc = 0.50000\n",
            "Epoch [5/5] Step [1121/1610]: discriminator_loss = 0.58962, target_loss = 0.38423, acc = 0.50000\n",
            "Epoch [5/5] Step [1122/1610]: discriminator_loss = 0.55296, target_loss = 0.37684, acc = 0.50000\n",
            "Epoch [5/5] Step [1123/1610]: discriminator_loss = 0.58555, target_loss = 0.39766, acc = 0.50000\n",
            "Epoch [5/5] Step [1124/1610]: discriminator_loss = 0.56874, target_loss = 0.39655, acc = 0.50000\n",
            "Epoch [5/5] Step [1125/1610]: discriminator_loss = 0.56648, target_loss = 0.41452, acc = 0.50000\n",
            "Epoch [5/5] Step [1126/1610]: discriminator_loss = 0.55101, target_loss = 0.41104, acc = 0.50000\n",
            "Epoch [5/5] Step [1127/1610]: discriminator_loss = 0.55357, target_loss = 0.38849, acc = 0.50000\n",
            "Epoch [5/5] Step [1128/1610]: discriminator_loss = 0.57053, target_loss = 0.39995, acc = 0.50000\n",
            "Epoch [5/5] Step [1129/1610]: discriminator_loss = 0.55337, target_loss = 0.39529, acc = 0.50000\n",
            "Epoch [5/5] Step [1130/1610]: discriminator_loss = 0.52996, target_loss = 0.38724, acc = 0.50000\n",
            "Epoch [5/5] Step [1131/1610]: discriminator_loss = 0.54472, target_loss = 0.40205, acc = 0.50000\n",
            "Epoch [5/5] Step [1132/1610]: discriminator_loss = 0.55887, target_loss = 0.41137, acc = 0.50000\n",
            "Epoch [5/5] Step [1133/1610]: discriminator_loss = 0.55392, target_loss = 0.41247, acc = 0.50000\n",
            "Epoch [5/5] Step [1134/1610]: discriminator_loss = 0.54515, target_loss = 0.40316, acc = 0.50000\n",
            "Epoch [5/5] Step [1135/1610]: discriminator_loss = 0.54985, target_loss = 0.40137, acc = 0.50000\n",
            "Epoch [5/5] Step [1136/1610]: discriminator_loss = 0.54517, target_loss = 0.41724, acc = 0.50000\n",
            "Epoch [5/5] Step [1137/1610]: discriminator_loss = 0.55665, target_loss = 0.40556, acc = 0.50000\n",
            "Epoch [5/5] Step [1138/1610]: discriminator_loss = 0.54951, target_loss = 0.39621, acc = 0.50000\n",
            "Epoch [5/5] Step [1139/1610]: discriminator_loss = 0.57198, target_loss = 0.40879, acc = 0.50000\n",
            "Epoch [5/5] Step [1140/1610]: discriminator_loss = 0.55693, target_loss = 0.39425, acc = 0.50000\n",
            "Epoch [5/5] Step [1141/1610]: discriminator_loss = 0.54566, target_loss = 0.37550, acc = 0.50000\n",
            "Epoch [5/5] Step [1142/1610]: discriminator_loss = 0.54579, target_loss = 0.40784, acc = 0.50000\n",
            "Epoch [5/5] Step [1143/1610]: discriminator_loss = 0.54169, target_loss = 0.39360, acc = 0.50000\n",
            "Epoch [5/5] Step [1144/1610]: discriminator_loss = 0.55719, target_loss = 0.40127, acc = 0.50000\n",
            "Epoch [5/5] Step [1145/1610]: discriminator_loss = 0.57112, target_loss = 0.39612, acc = 0.50000\n",
            "Epoch [5/5] Step [1146/1610]: discriminator_loss = 0.55468, target_loss = 0.40806, acc = 0.50000\n",
            "Epoch [5/5] Step [1147/1610]: discriminator_loss = 0.56466, target_loss = 0.41832, acc = 0.50000\n",
            "Epoch [5/5] Step [1148/1610]: discriminator_loss = 0.59925, target_loss = 0.40210, acc = 0.50000\n",
            "Epoch [5/5] Step [1149/1610]: discriminator_loss = 0.55601, target_loss = 0.40510, acc = 0.50000\n",
            "Epoch [5/5] Step [1150/1610]: discriminator_loss = 0.59060, target_loss = 0.41044, acc = 0.50000\n",
            "Epoch [5/5] Step [1151/1610]: discriminator_loss = 0.56469, target_loss = 0.40287, acc = 0.50000\n",
            "Epoch [5/5] Step [1152/1610]: discriminator_loss = 0.55264, target_loss = 0.40643, acc = 0.50000\n",
            "Epoch [5/5] Step [1153/1610]: discriminator_loss = 0.56079, target_loss = 0.40172, acc = 0.50000\n",
            "Epoch [5/5] Step [1154/1610]: discriminator_loss = 0.55169, target_loss = 0.39011, acc = 0.50000\n",
            "Epoch [5/5] Step [1155/1610]: discriminator_loss = 0.55279, target_loss = 0.39751, acc = 0.50000\n",
            "Epoch [5/5] Step [1156/1610]: discriminator_loss = 0.55891, target_loss = 0.40094, acc = 0.50000\n",
            "Epoch [5/5] Step [1157/1610]: discriminator_loss = 0.56426, target_loss = 0.41321, acc = 0.50000\n",
            "Epoch [5/5] Step [1158/1610]: discriminator_loss = 0.56872, target_loss = 0.39296, acc = 0.50000\n",
            "Epoch [5/5] Step [1159/1610]: discriminator_loss = 0.57643, target_loss = 0.39138, acc = 0.50000\n",
            "Epoch [5/5] Step [1160/1610]: discriminator_loss = 0.56545, target_loss = 0.38927, acc = 0.50000\n",
            "Epoch [5/5] Step [1161/1610]: discriminator_loss = 0.57800, target_loss = 0.38257, acc = 0.50000\n",
            "Epoch [5/5] Step [1162/1610]: discriminator_loss = 0.56659, target_loss = 0.40467, acc = 0.50000\n",
            "Epoch [5/5] Step [1163/1610]: discriminator_loss = 0.56103, target_loss = 0.40278, acc = 0.50000\n",
            "Epoch [5/5] Step [1164/1610]: discriminator_loss = 0.55837, target_loss = 0.40455, acc = 0.50000\n",
            "Epoch [5/5] Step [1165/1610]: discriminator_loss = 0.56247, target_loss = 0.39813, acc = 0.50000\n",
            "Epoch [5/5] Step [1166/1610]: discriminator_loss = 0.56316, target_loss = 0.39300, acc = 0.50000\n",
            "Epoch [5/5] Step [1167/1610]: discriminator_loss = 0.56147, target_loss = 0.40414, acc = 0.50000\n",
            "Epoch [5/5] Step [1168/1610]: discriminator_loss = 0.56548, target_loss = 0.41001, acc = 0.50000\n",
            "Epoch [5/5] Step [1169/1610]: discriminator_loss = 0.57453, target_loss = 0.39838, acc = 0.50000\n",
            "Epoch [5/5] Step [1170/1610]: discriminator_loss = 0.54637, target_loss = 0.41139, acc = 0.50000\n",
            "Epoch [5/5] Step [1171/1610]: discriminator_loss = 0.54430, target_loss = 0.42249, acc = 0.50000\n",
            "Epoch [5/5] Step [1172/1610]: discriminator_loss = 0.55720, target_loss = 0.41110, acc = 0.50000\n",
            "Epoch [5/5] Step [1173/1610]: discriminator_loss = 0.55626, target_loss = 0.40792, acc = 0.50000\n",
            "Epoch [5/5] Step [1174/1610]: discriminator_loss = 0.55780, target_loss = 0.40671, acc = 0.50000\n",
            "Epoch [5/5] Step [1175/1610]: discriminator_loss = 0.56812, target_loss = 0.41064, acc = 0.50000\n",
            "Epoch [5/5] Step [1176/1610]: discriminator_loss = 0.55474, target_loss = 0.40701, acc = 0.50000\n",
            "Epoch [5/5] Step [1177/1610]: discriminator_loss = 0.53518, target_loss = 0.41381, acc = 0.50000\n",
            "Epoch [5/5] Step [1178/1610]: discriminator_loss = 0.55917, target_loss = 0.41423, acc = 0.50000\n",
            "Epoch [5/5] Step [1179/1610]: discriminator_loss = 0.54308, target_loss = 0.41599, acc = 0.50000\n",
            "Epoch [5/5] Step [1180/1610]: discriminator_loss = 0.56026, target_loss = 0.42175, acc = 0.50000\n",
            "Epoch [5/5] Step [1181/1610]: discriminator_loss = 0.54362, target_loss = 0.41803, acc = 0.50000\n",
            "Epoch [5/5] Step [1182/1610]: discriminator_loss = 0.52968, target_loss = 0.42328, acc = 0.50000\n",
            "Epoch [5/5] Step [1183/1610]: discriminator_loss = 0.55629, target_loss = 0.40961, acc = 0.50000\n",
            "Epoch [5/5] Step [1184/1610]: discriminator_loss = 0.53780, target_loss = 0.42823, acc = 0.50000\n",
            "Epoch [5/5] Step [1185/1610]: discriminator_loss = 0.54916, target_loss = 0.41663, acc = 0.50000\n",
            "Epoch [5/5] Step [1186/1610]: discriminator_loss = 0.54583, target_loss = 0.41259, acc = 0.50000\n",
            "Epoch [5/5] Step [1187/1610]: discriminator_loss = 0.54667, target_loss = 0.40263, acc = 0.50000\n",
            "Epoch [5/5] Step [1188/1610]: discriminator_loss = 0.55040, target_loss = 0.40048, acc = 0.50000\n",
            "Epoch [5/5] Step [1189/1610]: discriminator_loss = 0.55038, target_loss = 0.41394, acc = 0.50000\n",
            "Epoch [5/5] Step [1190/1610]: discriminator_loss = 0.55168, target_loss = 0.40765, acc = 0.50000\n",
            "Epoch [5/5] Step [1191/1610]: discriminator_loss = 0.56015, target_loss = 0.39660, acc = 0.50000\n",
            "Epoch [5/5] Step [1192/1610]: discriminator_loss = 0.55371, target_loss = 0.39420, acc = 0.50000\n",
            "Epoch [5/5] Step [1193/1610]: discriminator_loss = 0.56199, target_loss = 0.39672, acc = 0.50000\n",
            "Epoch [5/5] Step [1194/1610]: discriminator_loss = 0.55284, target_loss = 0.41583, acc = 0.50000\n",
            "Epoch [5/5] Step [1195/1610]: discriminator_loss = 0.57668, target_loss = 0.39726, acc = 0.50000\n",
            "Epoch [5/5] Step [1196/1610]: discriminator_loss = 0.55634, target_loss = 0.41692, acc = 0.50000\n",
            "Epoch [5/5] Step [1197/1610]: discriminator_loss = 0.56426, target_loss = 0.39149, acc = 0.50000\n",
            "Epoch [5/5] Step [1198/1610]: discriminator_loss = 0.55379, target_loss = 0.41265, acc = 0.50000\n",
            "Epoch [5/5] Step [1199/1610]: discriminator_loss = 0.56352, target_loss = 0.39550, acc = 0.50000\n",
            "Epoch [5/5] Step [1200/1610]: discriminator_loss = 0.55416, target_loss = 0.40476, acc = 0.50000\n",
            "Epoch [5/5] Step [1201/1610]: discriminator_loss = 0.54531, target_loss = 0.41467, acc = 0.50000\n",
            "Epoch [5/5] Step [1202/1610]: discriminator_loss = 0.54512, target_loss = 0.40620, acc = 0.50000\n",
            "Epoch [5/5] Step [1203/1610]: discriminator_loss = 0.55786, target_loss = 0.39418, acc = 0.50000\n",
            "Epoch [5/5] Step [1204/1610]: discriminator_loss = 0.55395, target_loss = 0.40005, acc = 0.50000\n",
            "Epoch [5/5] Step [1205/1610]: discriminator_loss = 0.54987, target_loss = 0.41606, acc = 0.50000\n",
            "Epoch [5/5] Step [1206/1610]: discriminator_loss = 0.55215, target_loss = 0.41771, acc = 0.50000\n",
            "Epoch [5/5] Step [1207/1610]: discriminator_loss = 0.56244, target_loss = 0.41268, acc = 0.50000\n",
            "Epoch [5/5] Step [1208/1610]: discriminator_loss = 0.55617, target_loss = 0.40070, acc = 0.50000\n",
            "Epoch [5/5] Step [1209/1610]: discriminator_loss = 0.54622, target_loss = 0.41192, acc = 0.50000\n",
            "Epoch [5/5] Step [1210/1610]: discriminator_loss = 0.55474, target_loss = 0.41752, acc = 0.50000\n",
            "Epoch [5/5] Step [1211/1610]: discriminator_loss = 0.54869, target_loss = 0.40729, acc = 0.50000\n",
            "Epoch [5/5] Step [1212/1610]: discriminator_loss = 0.55495, target_loss = 0.40588, acc = 0.50000\n",
            "Epoch [5/5] Step [1213/1610]: discriminator_loss = 0.54235, target_loss = 0.41172, acc = 0.50000\n",
            "Epoch [5/5] Step [1214/1610]: discriminator_loss = 0.55164, target_loss = 0.39787, acc = 0.50000\n",
            "Epoch [5/5] Step [1215/1610]: discriminator_loss = 0.56819, target_loss = 0.40854, acc = 0.50000\n",
            "Epoch [5/5] Step [1216/1610]: discriminator_loss = 0.54971, target_loss = 0.40798, acc = 0.50000\n",
            "Epoch [5/5] Step [1217/1610]: discriminator_loss = 0.54300, target_loss = 0.40320, acc = 0.50000\n",
            "Epoch [5/5] Step [1218/1610]: discriminator_loss = 0.54061, target_loss = 0.39045, acc = 0.50000\n",
            "Epoch [5/5] Step [1219/1610]: discriminator_loss = 0.54450, target_loss = 0.41458, acc = 0.50000\n",
            "Epoch [5/5] Step [1220/1610]: discriminator_loss = 0.53784, target_loss = 0.40294, acc = 0.50000\n",
            "Epoch [5/5] Step [1221/1610]: discriminator_loss = 0.54622, target_loss = 0.41648, acc = 0.50000\n",
            "Epoch [5/5] Step [1222/1610]: discriminator_loss = 0.58016, target_loss = 0.41443, acc = 0.50000\n",
            "Epoch [5/5] Step [1223/1610]: discriminator_loss = 0.56436, target_loss = 0.41102, acc = 0.50000\n",
            "Epoch [5/5] Step [1224/1610]: discriminator_loss = 0.56505, target_loss = 0.38665, acc = 0.50000\n",
            "Epoch [5/5] Step [1225/1610]: discriminator_loss = 0.55102, target_loss = 0.41209, acc = 0.50000\n",
            "Epoch [5/5] Step [1226/1610]: discriminator_loss = 0.56605, target_loss = 0.39077, acc = 0.50000\n",
            "Epoch [5/5] Step [1227/1610]: discriminator_loss = 0.55103, target_loss = 0.39804, acc = 0.50000\n",
            "Epoch [5/5] Step [1228/1610]: discriminator_loss = 0.54527, target_loss = 0.39827, acc = 0.50000\n",
            "Epoch [5/5] Step [1229/1610]: discriminator_loss = 0.56356, target_loss = 0.42737, acc = 0.50000\n",
            "Epoch [5/5] Step [1230/1610]: discriminator_loss = 0.54780, target_loss = 0.39924, acc = 0.50000\n",
            "Epoch [5/5] Step [1231/1610]: discriminator_loss = 0.54901, target_loss = 0.41867, acc = 0.50000\n",
            "Epoch [5/5] Step [1232/1610]: discriminator_loss = 0.55896, target_loss = 0.40903, acc = 0.50000\n",
            "Epoch [5/5] Step [1233/1610]: discriminator_loss = 0.56174, target_loss = 0.40309, acc = 0.50000\n",
            "Epoch [5/5] Step [1234/1610]: discriminator_loss = 0.56398, target_loss = 0.41357, acc = 0.50000\n",
            "Epoch [5/5] Step [1235/1610]: discriminator_loss = 0.54825, target_loss = 0.40734, acc = 0.50000\n",
            "Epoch [5/5] Step [1236/1610]: discriminator_loss = 0.55372, target_loss = 0.39476, acc = 0.50000\n",
            "Epoch [5/5] Step [1237/1610]: discriminator_loss = 0.54783, target_loss = 0.40717, acc = 0.50000\n",
            "Epoch [5/5] Step [1238/1610]: discriminator_loss = 0.54990, target_loss = 0.39913, acc = 0.50000\n",
            "Epoch [5/5] Step [1239/1610]: discriminator_loss = 0.54564, target_loss = 0.40743, acc = 0.50000\n",
            "Epoch [5/5] Step [1240/1610]: discriminator_loss = 0.56303, target_loss = 0.40287, acc = 0.50000\n",
            "Epoch [5/5] Step [1241/1610]: discriminator_loss = 0.56611, target_loss = 0.38503, acc = 0.50000\n",
            "Epoch [5/5] Step [1242/1610]: discriminator_loss = 0.55151, target_loss = 0.40039, acc = 0.50000\n",
            "Epoch [5/5] Step [1243/1610]: discriminator_loss = 0.56367, target_loss = 0.40118, acc = 0.50000\n",
            "Epoch [5/5] Step [1244/1610]: discriminator_loss = 0.55285, target_loss = 0.38488, acc = 0.50000\n",
            "Epoch [5/5] Step [1245/1610]: discriminator_loss = 0.57935, target_loss = 0.38324, acc = 0.50000\n",
            "Epoch [5/5] Step [1246/1610]: discriminator_loss = 0.55698, target_loss = 0.38895, acc = 0.50000\n",
            "Epoch [5/5] Step [1247/1610]: discriminator_loss = 0.56015, target_loss = 0.37843, acc = 0.50000\n",
            "Epoch [5/5] Step [1248/1610]: discriminator_loss = 0.57357, target_loss = 0.39244, acc = 0.50000\n",
            "Epoch [5/5] Step [1249/1610]: discriminator_loss = 0.58350, target_loss = 0.36274, acc = 0.50000\n",
            "Epoch [5/5] Step [1250/1610]: discriminator_loss = 0.56091, target_loss = 0.37839, acc = 0.50000\n",
            "Epoch [5/5] Step [1251/1610]: discriminator_loss = 0.58945, target_loss = 0.36670, acc = 0.50000\n",
            "Epoch [5/5] Step [1252/1610]: discriminator_loss = 0.62362, target_loss = 0.39723, acc = 0.50000\n",
            "Epoch [5/5] Step [1253/1610]: discriminator_loss = 0.64980, target_loss = 0.37871, acc = 0.50000\n",
            "Epoch [5/5] Step [1254/1610]: discriminator_loss = 0.57648, target_loss = 0.38625, acc = 0.50000\n",
            "Epoch [5/5] Step [1255/1610]: discriminator_loss = 0.57500, target_loss = 0.40374, acc = 0.50000\n",
            "Epoch [5/5] Step [1256/1610]: discriminator_loss = 0.56980, target_loss = 0.38960, acc = 0.50000\n",
            "Epoch [5/5] Step [1257/1610]: discriminator_loss = 0.62881, target_loss = 0.39979, acc = 0.50000\n",
            "Epoch [5/5] Step [1258/1610]: discriminator_loss = 0.54263, target_loss = 0.41230, acc = 0.50000\n",
            "Epoch [5/5] Step [1259/1610]: discriminator_loss = 0.55689, target_loss = 0.39731, acc = 0.50000\n",
            "Epoch [5/5] Step [1260/1610]: discriminator_loss = 0.54818, target_loss = 0.40685, acc = 0.50000\n",
            "Epoch [5/5] Step [1261/1610]: discriminator_loss = 0.53568, target_loss = 0.42787, acc = 0.50000\n",
            "Epoch [5/5] Step [1262/1610]: discriminator_loss = 0.53550, target_loss = 0.41989, acc = 0.50000\n",
            "Epoch [5/5] Step [1263/1610]: discriminator_loss = 0.54975, target_loss = 0.41566, acc = 0.50000\n",
            "Epoch [5/5] Step [1264/1610]: discriminator_loss = 0.52637, target_loss = 0.41684, acc = 0.50000\n",
            "Epoch [5/5] Step [1265/1610]: discriminator_loss = 0.54066, target_loss = 0.43443, acc = 0.50000\n",
            "Epoch [5/5] Step [1266/1610]: discriminator_loss = 0.54016, target_loss = 0.41840, acc = 0.50000\n",
            "Epoch [5/5] Step [1267/1610]: discriminator_loss = 0.53440, target_loss = 0.41383, acc = 0.50000\n",
            "Epoch [5/5] Step [1268/1610]: discriminator_loss = 0.54501, target_loss = 0.40947, acc = 0.50000\n",
            "Epoch [5/5] Step [1269/1610]: discriminator_loss = 0.55209, target_loss = 0.42162, acc = 0.50000\n",
            "Epoch [5/5] Step [1270/1610]: discriminator_loss = 0.54638, target_loss = 0.42774, acc = 0.50000\n",
            "Epoch [5/5] Step [1271/1610]: discriminator_loss = 0.53564, target_loss = 0.40868, acc = 0.50000\n",
            "Epoch [5/5] Step [1272/1610]: discriminator_loss = 0.54923, target_loss = 0.39424, acc = 0.50000\n",
            "Epoch [5/5] Step [1273/1610]: discriminator_loss = 0.55435, target_loss = 0.40341, acc = 0.50000\n",
            "Epoch [5/5] Step [1274/1610]: discriminator_loss = 0.56616, target_loss = 0.39216, acc = 0.50000\n",
            "Epoch [5/5] Step [1275/1610]: discriminator_loss = 0.55844, target_loss = 0.39995, acc = 0.50000\n",
            "Epoch [5/5] Step [1276/1610]: discriminator_loss = 0.59059, target_loss = 0.39500, acc = 0.50000\n",
            "Epoch [5/5] Step [1277/1610]: discriminator_loss = 0.55702, target_loss = 0.38730, acc = 0.50000\n",
            "Epoch [5/5] Step [1278/1610]: discriminator_loss = 0.54266, target_loss = 0.40382, acc = 0.50000\n",
            "Epoch [5/5] Step [1279/1610]: discriminator_loss = 0.54943, target_loss = 0.39839, acc = 0.50000\n",
            "Epoch [5/5] Step [1280/1610]: discriminator_loss = 0.55831, target_loss = 0.38174, acc = 0.50000\n",
            "Epoch [5/5] Step [1281/1610]: discriminator_loss = 0.55840, target_loss = 0.41032, acc = 0.50000\n",
            "Epoch [5/5] Step [1282/1610]: discriminator_loss = 0.58112, target_loss = 0.40209, acc = 0.50000\n",
            "Epoch [5/5] Step [1283/1610]: discriminator_loss = 0.56196, target_loss = 0.38802, acc = 0.50000\n",
            "Epoch [5/5] Step [1284/1610]: discriminator_loss = 0.55630, target_loss = 0.40176, acc = 0.50000\n",
            "Epoch [5/5] Step [1285/1610]: discriminator_loss = 0.55701, target_loss = 0.39774, acc = 0.50000\n",
            "Epoch [5/5] Step [1286/1610]: discriminator_loss = 0.55939, target_loss = 0.40607, acc = 0.50000\n",
            "Epoch [5/5] Step [1287/1610]: discriminator_loss = 0.56341, target_loss = 0.40221, acc = 0.50000\n",
            "Epoch [5/5] Step [1288/1610]: discriminator_loss = 0.57398, target_loss = 0.39914, acc = 0.50000\n",
            "Epoch [5/5] Step [1289/1610]: discriminator_loss = 0.57275, target_loss = 0.38881, acc = 0.50000\n",
            "Epoch [5/5] Step [1290/1610]: discriminator_loss = 0.56895, target_loss = 0.38677, acc = 0.50000\n",
            "Epoch [5/5] Step [1291/1610]: discriminator_loss = 0.55655, target_loss = 0.40635, acc = 0.50000\n",
            "Epoch [5/5] Step [1292/1610]: discriminator_loss = 0.53181, target_loss = 0.40644, acc = 0.50000\n",
            "Epoch [5/5] Step [1293/1610]: discriminator_loss = 0.55276, target_loss = 0.41601, acc = 0.50000\n",
            "Epoch [5/5] Step [1294/1610]: discriminator_loss = 0.57058, target_loss = 0.40967, acc = 0.50000\n",
            "Epoch [5/5] Step [1295/1610]: discriminator_loss = 0.57567, target_loss = 0.40062, acc = 0.50000\n",
            "Epoch [5/5] Step [1296/1610]: discriminator_loss = 0.54605, target_loss = 0.41713, acc = 0.50000\n",
            "Epoch [5/5] Step [1297/1610]: discriminator_loss = 0.55795, target_loss = 0.41930, acc = 0.50000\n",
            "Epoch [5/5] Step [1298/1610]: discriminator_loss = 0.54451, target_loss = 0.40928, acc = 0.50000\n",
            "Epoch [5/5] Step [1299/1610]: discriminator_loss = 0.55082, target_loss = 0.41921, acc = 0.50000\n",
            "Epoch [5/5] Step [1300/1610]: discriminator_loss = 0.56435, target_loss = 0.40189, acc = 0.50000\n",
            "Epoch [5/5] Step [1301/1610]: discriminator_loss = 0.55080, target_loss = 0.40727, acc = 0.50000\n",
            "Epoch [5/5] Step [1302/1610]: discriminator_loss = 0.54614, target_loss = 0.42193, acc = 0.50000\n",
            "Epoch [5/5] Step [1303/1610]: discriminator_loss = 0.55009, target_loss = 0.40624, acc = 0.50000\n",
            "Epoch [5/5] Step [1304/1610]: discriminator_loss = 0.55856, target_loss = 0.40183, acc = 0.50000\n",
            "Epoch [5/5] Step [1305/1610]: discriminator_loss = 0.56563, target_loss = 0.40500, acc = 0.50000\n",
            "Epoch [5/5] Step [1306/1610]: discriminator_loss = 0.54435, target_loss = 0.40697, acc = 0.50000\n",
            "Epoch [5/5] Step [1307/1610]: discriminator_loss = 0.55925, target_loss = 0.39711, acc = 0.50000\n",
            "Epoch [5/5] Step [1308/1610]: discriminator_loss = 0.54001, target_loss = 0.41690, acc = 0.50000\n",
            "Epoch [5/5] Step [1309/1610]: discriminator_loss = 0.55429, target_loss = 0.40601, acc = 0.50000\n",
            "Epoch [5/5] Step [1310/1610]: discriminator_loss = 0.54691, target_loss = 0.41845, acc = 0.50000\n",
            "Epoch [5/5] Step [1311/1610]: discriminator_loss = 0.54041, target_loss = 0.40881, acc = 0.50000\n",
            "Epoch [5/5] Step [1312/1610]: discriminator_loss = 0.54559, target_loss = 0.39632, acc = 0.50000\n",
            "Epoch [5/5] Step [1313/1610]: discriminator_loss = 0.57055, target_loss = 0.38144, acc = 0.50000\n",
            "Epoch [5/5] Step [1314/1610]: discriminator_loss = 0.55145, target_loss = 0.38777, acc = 0.50000\n",
            "Epoch [5/5] Step [1315/1610]: discriminator_loss = 0.54427, target_loss = 0.40233, acc = 0.50000\n",
            "Epoch [5/5] Step [1316/1610]: discriminator_loss = 0.56263, target_loss = 0.40881, acc = 0.50000\n",
            "Epoch [5/5] Step [1317/1610]: discriminator_loss = 0.55123, target_loss = 0.38072, acc = 0.50000\n",
            "Epoch [5/5] Step [1318/1610]: discriminator_loss = 0.55445, target_loss = 0.40004, acc = 0.50000\n",
            "Epoch [5/5] Step [1319/1610]: discriminator_loss = 0.55792, target_loss = 0.39582, acc = 0.50000\n",
            "Epoch [5/5] Step [1320/1610]: discriminator_loss = 0.56796, target_loss = 0.38171, acc = 0.50000\n",
            "Epoch [5/5] Step [1321/1610]: discriminator_loss = 0.57401, target_loss = 0.40675, acc = 0.50000\n",
            "Epoch [5/5] Step [1322/1610]: discriminator_loss = 0.54748, target_loss = 0.39055, acc = 0.50000\n",
            "Epoch [5/5] Step [1323/1610]: discriminator_loss = 0.56097, target_loss = 0.37196, acc = 0.50000\n",
            "Epoch [5/5] Step [1324/1610]: discriminator_loss = 0.55997, target_loss = 0.38871, acc = 0.50000\n",
            "Epoch [5/5] Step [1325/1610]: discriminator_loss = 0.54547, target_loss = 0.39694, acc = 0.50000\n",
            "Epoch [5/5] Step [1326/1610]: discriminator_loss = 0.56432, target_loss = 0.37751, acc = 0.50000\n",
            "Epoch [5/5] Step [1327/1610]: discriminator_loss = 0.56794, target_loss = 0.39308, acc = 0.50000\n",
            "Epoch [5/5] Step [1328/1610]: discriminator_loss = 0.57229, target_loss = 0.40468, acc = 0.50000\n",
            "Epoch [5/5] Step [1329/1610]: discriminator_loss = 0.68280, target_loss = 0.38977, acc = 0.50000\n",
            "Epoch [5/5] Step [1330/1610]: discriminator_loss = 0.56449, target_loss = 0.40357, acc = 0.50000\n",
            "Epoch [5/5] Step [1331/1610]: discriminator_loss = 0.55611, target_loss = 0.39165, acc = 0.50000\n",
            "Epoch [5/5] Step [1332/1610]: discriminator_loss = 0.56410, target_loss = 0.40826, acc = 0.50000\n",
            "Epoch [5/5] Step [1333/1610]: discriminator_loss = 0.59218, target_loss = 0.42744, acc = 0.50000\n",
            "Epoch [5/5] Step [1334/1610]: discriminator_loss = 0.54728, target_loss = 0.40912, acc = 0.50000\n",
            "Epoch [5/5] Step [1335/1610]: discriminator_loss = 0.57620, target_loss = 0.40676, acc = 0.50000\n",
            "Epoch [5/5] Step [1336/1610]: discriminator_loss = 0.54659, target_loss = 0.40801, acc = 0.50000\n",
            "Epoch [5/5] Step [1337/1610]: discriminator_loss = 0.55060, target_loss = 0.41398, acc = 0.50000\n",
            "Epoch [5/5] Step [1338/1610]: discriminator_loss = 0.54552, target_loss = 0.41525, acc = 0.50000\n",
            "Epoch [5/5] Step [1339/1610]: discriminator_loss = 0.54409, target_loss = 0.40856, acc = 0.50000\n",
            "Epoch [5/5] Step [1340/1610]: discriminator_loss = 0.54326, target_loss = 0.40336, acc = 0.50000\n",
            "Epoch [5/5] Step [1341/1610]: discriminator_loss = 0.54328, target_loss = 0.40852, acc = 0.50000\n",
            "Epoch [5/5] Step [1342/1610]: discriminator_loss = 0.55460, target_loss = 0.40123, acc = 0.50000\n",
            "Epoch [5/5] Step [1343/1610]: discriminator_loss = 0.55137, target_loss = 0.40462, acc = 0.50000\n",
            "Epoch [5/5] Step [1344/1610]: discriminator_loss = 0.54147, target_loss = 0.42140, acc = 0.50000\n",
            "Epoch [5/5] Step [1345/1610]: discriminator_loss = 0.55282, target_loss = 0.40854, acc = 0.50000\n",
            "Epoch [5/5] Step [1346/1610]: discriminator_loss = 0.60854, target_loss = 0.40376, acc = 0.50000\n",
            "Epoch [5/5] Step [1347/1610]: discriminator_loss = 0.52714, target_loss = 0.42522, acc = 0.50000\n",
            "Epoch [5/5] Step [1348/1610]: discriminator_loss = 0.54782, target_loss = 0.41517, acc = 0.50000\n",
            "Epoch [5/5] Step [1349/1610]: discriminator_loss = 0.54623, target_loss = 0.41535, acc = 0.50000\n",
            "Epoch [5/5] Step [1350/1610]: discriminator_loss = 0.54317, target_loss = 0.40403, acc = 0.50000\n",
            "Epoch [5/5] Step [1351/1610]: discriminator_loss = 0.54546, target_loss = 0.41475, acc = 0.50000\n",
            "Epoch [5/5] Step [1352/1610]: discriminator_loss = 0.55249, target_loss = 0.41137, acc = 0.50000\n",
            "Epoch [5/5] Step [1353/1610]: discriminator_loss = 0.54908, target_loss = 0.39735, acc = 0.50000\n",
            "Epoch [5/5] Step [1354/1610]: discriminator_loss = 0.55847, target_loss = 0.40766, acc = 0.50000\n",
            "Epoch [5/5] Step [1355/1610]: discriminator_loss = 0.55684, target_loss = 0.40088, acc = 0.50000\n",
            "Epoch [5/5] Step [1356/1610]: discriminator_loss = 0.55758, target_loss = 0.40299, acc = 0.50000\n",
            "Epoch [5/5] Step [1357/1610]: discriminator_loss = 0.54719, target_loss = 0.40912, acc = 0.50000\n",
            "Epoch [5/5] Step [1358/1610]: discriminator_loss = 0.56098, target_loss = 0.39519, acc = 0.50000\n",
            "Epoch [5/5] Step [1359/1610]: discriminator_loss = 0.60726, target_loss = 0.41002, acc = 0.50000\n",
            "Epoch [5/5] Step [1360/1610]: discriminator_loss = 0.56455, target_loss = 0.39748, acc = 0.50000\n",
            "Epoch [5/5] Step [1361/1610]: discriminator_loss = 0.55118, target_loss = 0.40250, acc = 0.50000\n",
            "Epoch [5/5] Step [1362/1610]: discriminator_loss = 0.57886, target_loss = 0.40831, acc = 0.50000\n",
            "Epoch [5/5] Step [1363/1610]: discriminator_loss = 0.56335, target_loss = 0.40448, acc = 0.50000\n",
            "Epoch [5/5] Step [1364/1610]: discriminator_loss = 0.55026, target_loss = 0.40217, acc = 0.50000\n",
            "Epoch [5/5] Step [1365/1610]: discriminator_loss = 0.57296, target_loss = 0.40332, acc = 0.50000\n",
            "Epoch [5/5] Step [1366/1610]: discriminator_loss = 0.54748, target_loss = 0.40707, acc = 0.50000\n",
            "Epoch [5/5] Step [1367/1610]: discriminator_loss = 0.56365, target_loss = 0.39918, acc = 0.50000\n",
            "Epoch [5/5] Step [1368/1610]: discriminator_loss = 0.54724, target_loss = 0.39595, acc = 0.50000\n",
            "Epoch [5/5] Step [1369/1610]: discriminator_loss = 0.62352, target_loss = 0.41070, acc = 0.50000\n",
            "Epoch [5/5] Step [1370/1610]: discriminator_loss = 0.56267, target_loss = 0.40746, acc = 0.50000\n",
            "Epoch [5/5] Step [1371/1610]: discriminator_loss = 0.56719, target_loss = 0.40511, acc = 0.50000\n",
            "Epoch [5/5] Step [1372/1610]: discriminator_loss = 0.55942, target_loss = 0.40387, acc = 0.50000\n",
            "Epoch [5/5] Step [1373/1610]: discriminator_loss = 0.55457, target_loss = 0.40902, acc = 0.50000\n",
            "Epoch [5/5] Step [1374/1610]: discriminator_loss = 0.55743, target_loss = 0.40351, acc = 0.50000\n",
            "Epoch [5/5] Step [1375/1610]: discriminator_loss = 0.56153, target_loss = 0.39414, acc = 0.50000\n",
            "Epoch [5/5] Step [1376/1610]: discriminator_loss = 0.56694, target_loss = 0.40422, acc = 0.50000\n",
            "Epoch [5/5] Step [1377/1610]: discriminator_loss = 0.56024, target_loss = 0.39875, acc = 0.50000\n",
            "Epoch [5/5] Step [1378/1610]: discriminator_loss = 0.55734, target_loss = 0.40760, acc = 0.50000\n",
            "Epoch [5/5] Step [1379/1610]: discriminator_loss = 0.56544, target_loss = 0.40380, acc = 0.50000\n",
            "Epoch [5/5] Step [1380/1610]: discriminator_loss = 0.57240, target_loss = 0.39583, acc = 0.50000\n",
            "Epoch [5/5] Step [1381/1610]: discriminator_loss = 0.55601, target_loss = 0.39418, acc = 0.50000\n",
            "Epoch [5/5] Step [1382/1610]: discriminator_loss = 0.55670, target_loss = 0.40534, acc = 0.50000\n",
            "Epoch [5/5] Step [1383/1610]: discriminator_loss = 0.55589, target_loss = 0.39833, acc = 0.50000\n",
            "Epoch [5/5] Step [1384/1610]: discriminator_loss = 0.56094, target_loss = 0.40544, acc = 0.50000\n",
            "Epoch [5/5] Step [1385/1610]: discriminator_loss = 0.55610, target_loss = 0.39397, acc = 0.50000\n",
            "Epoch [5/5] Step [1386/1610]: discriminator_loss = 0.55631, target_loss = 0.40013, acc = 0.50000\n",
            "Epoch [5/5] Step [1387/1610]: discriminator_loss = 0.55445, target_loss = 0.40362, acc = 0.50000\n",
            "Epoch [5/5] Step [1388/1610]: discriminator_loss = 0.55421, target_loss = 0.41020, acc = 0.50000\n",
            "Epoch [5/5] Step [1389/1610]: discriminator_loss = 0.55704, target_loss = 0.40935, acc = 0.50000\n",
            "Epoch [5/5] Step [1390/1610]: discriminator_loss = 0.54293, target_loss = 0.41807, acc = 0.50000\n",
            "Epoch [5/5] Step [1391/1610]: discriminator_loss = 0.54865, target_loss = 0.41190, acc = 0.50000\n",
            "Epoch [5/5] Step [1392/1610]: discriminator_loss = 0.55483, target_loss = 0.40503, acc = 0.50000\n",
            "Epoch [5/5] Step [1393/1610]: discriminator_loss = 0.55781, target_loss = 0.39588, acc = 0.50000\n",
            "Epoch [5/5] Step [1394/1610]: discriminator_loss = 0.54362, target_loss = 0.40080, acc = 0.50000\n",
            "Epoch [5/5] Step [1395/1610]: discriminator_loss = 0.54606, target_loss = 0.40143, acc = 0.50000\n",
            "Epoch [5/5] Step [1396/1610]: discriminator_loss = 0.55604, target_loss = 0.38816, acc = 0.50000\n",
            "Epoch [5/5] Step [1397/1610]: discriminator_loss = 0.56068, target_loss = 0.39159, acc = 0.50000\n",
            "Epoch [5/5] Step [1398/1610]: discriminator_loss = 0.58192, target_loss = 0.41078, acc = 0.50000\n",
            "Epoch [5/5] Step [1399/1610]: discriminator_loss = 0.55017, target_loss = 0.38627, acc = 0.50000\n",
            "Epoch [5/5] Step [1400/1610]: discriminator_loss = 0.55169, target_loss = 0.38692, acc = 0.50000\n",
            "Epoch [5/5] Step [1401/1610]: discriminator_loss = 0.56795, target_loss = 0.41370, acc = 0.50000\n",
            "Epoch [5/5] Step [1402/1610]: discriminator_loss = 0.56972, target_loss = 0.38911, acc = 0.50000\n",
            "Epoch [5/5] Step [1403/1610]: discriminator_loss = 0.54978, target_loss = 0.39956, acc = 0.50000\n",
            "Epoch [5/5] Step [1404/1610]: discriminator_loss = 0.55489, target_loss = 0.40769, acc = 0.50000\n",
            "Epoch [5/5] Step [1405/1610]: discriminator_loss = 0.57432, target_loss = 0.41447, acc = 0.50000\n",
            "Epoch [5/5] Step [1406/1610]: discriminator_loss = 0.56246, target_loss = 0.39936, acc = 0.50000\n",
            "Epoch [5/5] Step [1407/1610]: discriminator_loss = 0.55191, target_loss = 0.40718, acc = 0.50000\n",
            "Epoch [5/5] Step [1408/1610]: discriminator_loss = 0.63017, target_loss = 0.40109, acc = 0.50000\n",
            "Epoch [5/5] Step [1409/1610]: discriminator_loss = 0.54728, target_loss = 0.40498, acc = 0.50000\n",
            "Epoch [5/5] Step [1410/1610]: discriminator_loss = 0.54960, target_loss = 0.41722, acc = 0.50000\n",
            "Epoch [5/5] Step [1411/1610]: discriminator_loss = 0.55034, target_loss = 0.40779, acc = 0.50000\n",
            "Epoch [5/5] Step [1412/1610]: discriminator_loss = 0.56081, target_loss = 0.40814, acc = 0.50000\n",
            "Epoch [5/5] Step [1413/1610]: discriminator_loss = 0.55075, target_loss = 0.41036, acc = 0.50000\n",
            "Epoch [5/5] Step [1414/1610]: discriminator_loss = 0.55639, target_loss = 0.42016, acc = 0.50000\n",
            "Epoch [5/5] Step [1415/1610]: discriminator_loss = 0.54632, target_loss = 0.41501, acc = 0.50000\n",
            "Epoch [5/5] Step [1416/1610]: discriminator_loss = 0.55221, target_loss = 0.41704, acc = 0.50000\n",
            "Epoch [5/5] Step [1417/1610]: discriminator_loss = 0.55404, target_loss = 0.41478, acc = 0.50000\n",
            "Epoch [5/5] Step [1418/1610]: discriminator_loss = 0.53872, target_loss = 0.40568, acc = 0.50000\n",
            "Epoch [5/5] Step [1419/1610]: discriminator_loss = 0.56145, target_loss = 0.41820, acc = 0.50000\n",
            "Epoch [5/5] Step [1420/1610]: discriminator_loss = 0.54725, target_loss = 0.41443, acc = 0.50000\n",
            "Epoch [5/5] Step [1421/1610]: discriminator_loss = 0.55541, target_loss = 0.40667, acc = 0.50000\n",
            "Epoch [5/5] Step [1422/1610]: discriminator_loss = 0.55416, target_loss = 0.39897, acc = 0.50000\n",
            "Epoch [5/5] Step [1423/1610]: discriminator_loss = 0.55802, target_loss = 0.40687, acc = 0.50000\n",
            "Epoch [5/5] Step [1424/1610]: discriminator_loss = 0.56419, target_loss = 0.39605, acc = 0.50000\n",
            "Epoch [5/5] Step [1425/1610]: discriminator_loss = 0.55519, target_loss = 0.39189, acc = 0.50000\n",
            "Epoch [5/5] Step [1426/1610]: discriminator_loss = 0.55842, target_loss = 0.39882, acc = 0.50000\n",
            "Epoch [5/5] Step [1427/1610]: discriminator_loss = 0.56441, target_loss = 0.40284, acc = 0.50000\n",
            "Epoch [5/5] Step [1428/1610]: discriminator_loss = 0.55682, target_loss = 0.40123, acc = 0.50000\n",
            "Epoch [5/5] Step [1429/1610]: discriminator_loss = 0.57433, target_loss = 0.39438, acc = 0.50000\n",
            "Epoch [5/5] Step [1430/1610]: discriminator_loss = 0.55559, target_loss = 0.40368, acc = 0.50000\n",
            "Epoch [5/5] Step [1431/1610]: discriminator_loss = 0.56563, target_loss = 0.38059, acc = 0.50000\n",
            "Epoch [5/5] Step [1432/1610]: discriminator_loss = 0.55131, target_loss = 0.38942, acc = 0.50000\n",
            "Epoch [5/5] Step [1433/1610]: discriminator_loss = 0.55067, target_loss = 0.40673, acc = 0.50000\n",
            "Epoch [5/5] Step [1434/1610]: discriminator_loss = 0.54955, target_loss = 0.40328, acc = 0.50000\n",
            "Epoch [5/5] Step [1435/1610]: discriminator_loss = 0.55191, target_loss = 0.40994, acc = 0.50000\n",
            "Epoch [5/5] Step [1436/1610]: discriminator_loss = 0.56923, target_loss = 0.39664, acc = 0.50000\n",
            "Epoch [5/5] Step [1437/1610]: discriminator_loss = 0.56032, target_loss = 0.39456, acc = 0.50000\n",
            "Epoch [5/5] Step [1438/1610]: discriminator_loss = 0.55183, target_loss = 0.41281, acc = 0.50000\n",
            "Epoch [5/5] Step [1439/1610]: discriminator_loss = 0.55670, target_loss = 0.38962, acc = 0.50000\n",
            "Epoch [5/5] Step [1440/1610]: discriminator_loss = 0.55786, target_loss = 0.39387, acc = 0.50000\n",
            "Epoch [5/5] Step [1441/1610]: discriminator_loss = 0.55463, target_loss = 0.40271, acc = 0.50000\n",
            "Epoch [5/5] Step [1442/1610]: discriminator_loss = 0.55504, target_loss = 0.40438, acc = 0.50000\n",
            "Epoch [5/5] Step [1443/1610]: discriminator_loss = 0.56011, target_loss = 0.39905, acc = 0.50000\n",
            "Epoch [5/5] Step [1444/1610]: discriminator_loss = 0.55464, target_loss = 0.40095, acc = 0.50000\n",
            "Epoch [5/5] Step [1445/1610]: discriminator_loss = 0.65415, target_loss = 0.38687, acc = 0.50000\n",
            "Epoch [5/5] Step [1446/1610]: discriminator_loss = 0.55807, target_loss = 0.40621, acc = 0.50000\n",
            "Epoch [5/5] Step [1447/1610]: discriminator_loss = 0.56841, target_loss = 0.39634, acc = 0.50000\n",
            "Epoch [5/5] Step [1448/1610]: discriminator_loss = 0.57247, target_loss = 0.40481, acc = 0.50000\n",
            "Epoch [5/5] Step [1449/1610]: discriminator_loss = 0.55075, target_loss = 0.39019, acc = 0.50000\n",
            "Epoch [5/5] Step [1450/1610]: discriminator_loss = 0.55300, target_loss = 0.40503, acc = 0.50000\n",
            "Epoch [5/5] Step [1451/1610]: discriminator_loss = 0.56331, target_loss = 0.40421, acc = 0.50000\n",
            "Epoch [5/5] Step [1452/1610]: discriminator_loss = 0.56410, target_loss = 0.40235, acc = 0.50000\n",
            "Epoch [5/5] Step [1453/1610]: discriminator_loss = 0.57168, target_loss = 0.39736, acc = 0.50000\n",
            "Epoch [5/5] Step [1454/1610]: discriminator_loss = 0.54638, target_loss = 0.38765, acc = 0.50000\n",
            "Epoch [5/5] Step [1455/1610]: discriminator_loss = 0.55509, target_loss = 0.41012, acc = 0.50000\n",
            "Epoch [5/5] Step [1456/1610]: discriminator_loss = 0.56753, target_loss = 0.40101, acc = 0.50000\n",
            "Epoch [5/5] Step [1457/1610]: discriminator_loss = 0.55271, target_loss = 0.40127, acc = 0.50000\n",
            "Epoch [5/5] Step [1458/1610]: discriminator_loss = 0.56671, target_loss = 0.40483, acc = 0.50000\n",
            "Epoch [5/5] Step [1459/1610]: discriminator_loss = 0.55751, target_loss = 0.40341, acc = 0.50000\n",
            "Epoch [5/5] Step [1460/1610]: discriminator_loss = 0.56942, target_loss = 0.41135, acc = 0.50000\n",
            "Epoch [5/5] Step [1461/1610]: discriminator_loss = 0.54643, target_loss = 0.39964, acc = 0.50000\n",
            "Epoch [5/5] Step [1462/1610]: discriminator_loss = 0.54981, target_loss = 0.40925, acc = 0.50000\n",
            "Epoch [5/5] Step [1463/1610]: discriminator_loss = 0.54339, target_loss = 0.40302, acc = 0.50000\n",
            "Epoch [5/5] Step [1464/1610]: discriminator_loss = 0.54896, target_loss = 0.39999, acc = 0.50000\n",
            "Epoch [5/5] Step [1465/1610]: discriminator_loss = 0.55992, target_loss = 0.39296, acc = 0.50000\n",
            "Epoch [5/5] Step [1466/1610]: discriminator_loss = 0.58416, target_loss = 0.40531, acc = 0.50000\n",
            "Epoch [5/5] Step [1467/1610]: discriminator_loss = 0.54731, target_loss = 0.39840, acc = 0.50000\n",
            "Epoch [5/5] Step [1468/1610]: discriminator_loss = 0.54685, target_loss = 0.38861, acc = 0.50000\n",
            "Epoch [5/5] Step [1469/1610]: discriminator_loss = 0.54850, target_loss = 0.40384, acc = 0.50000\n",
            "Epoch [5/5] Step [1470/1610]: discriminator_loss = 0.54887, target_loss = 0.39360, acc = 0.50000\n",
            "Epoch [5/5] Step [1471/1610]: discriminator_loss = 0.55804, target_loss = 0.39993, acc = 0.50000\n",
            "Epoch [5/5] Step [1472/1610]: discriminator_loss = 0.57152, target_loss = 0.41209, acc = 0.50000\n",
            "Epoch [5/5] Step [1473/1610]: discriminator_loss = 0.55508, target_loss = 0.40994, acc = 0.50000\n",
            "Epoch [5/5] Step [1474/1610]: discriminator_loss = 0.57109, target_loss = 0.39652, acc = 0.50000\n",
            "Epoch [5/5] Step [1475/1610]: discriminator_loss = 0.54704, target_loss = 0.40340, acc = 0.50000\n",
            "Epoch [5/5] Step [1476/1610]: discriminator_loss = 0.55606, target_loss = 0.40280, acc = 0.50000\n",
            "Epoch [5/5] Step [1477/1610]: discriminator_loss = 0.55024, target_loss = 0.37908, acc = 0.50000\n",
            "Epoch [5/5] Step [1478/1610]: discriminator_loss = 0.55482, target_loss = 0.39784, acc = 0.50000\n",
            "Epoch [5/5] Step [1479/1610]: discriminator_loss = 0.61640, target_loss = 0.40817, acc = 0.50000\n",
            "Epoch [5/5] Step [1480/1610]: discriminator_loss = 0.56891, target_loss = 0.40846, acc = 0.50000\n",
            "Epoch [5/5] Step [1481/1610]: discriminator_loss = 0.53987, target_loss = 0.43252, acc = 0.50000\n",
            "Epoch [5/5] Step [1482/1610]: discriminator_loss = 0.57659, target_loss = 0.40805, acc = 0.50000\n",
            "Epoch [5/5] Step [1483/1610]: discriminator_loss = 0.57678, target_loss = 0.41574, acc = 0.50000\n",
            "Epoch [5/5] Step [1484/1610]: discriminator_loss = 0.54055, target_loss = 0.41280, acc = 0.50000\n",
            "Epoch [5/5] Step [1485/1610]: discriminator_loss = 0.53626, target_loss = 0.40748, acc = 0.50000\n",
            "Epoch [5/5] Step [1486/1610]: discriminator_loss = 0.54949, target_loss = 0.41546, acc = 0.50000\n",
            "Epoch [5/5] Step [1487/1610]: discriminator_loss = 0.53085, target_loss = 0.41586, acc = 0.50000\n",
            "Epoch [5/5] Step [1488/1610]: discriminator_loss = 0.54258, target_loss = 0.42533, acc = 0.50000\n",
            "Epoch [5/5] Step [1489/1610]: discriminator_loss = 0.55924, target_loss = 0.41718, acc = 0.50000\n",
            "Epoch [5/5] Step [1490/1610]: discriminator_loss = 0.53806, target_loss = 0.41850, acc = 0.50000\n",
            "Epoch [5/5] Step [1491/1610]: discriminator_loss = 0.54684, target_loss = 0.41088, acc = 0.50000\n",
            "Epoch [5/5] Step [1492/1610]: discriminator_loss = 0.55137, target_loss = 0.41056, acc = 0.50000\n",
            "Epoch [5/5] Step [1493/1610]: discriminator_loss = 0.54736, target_loss = 0.40511, acc = 0.50000\n",
            "Epoch [5/5] Step [1494/1610]: discriminator_loss = 0.54431, target_loss = 0.40468, acc = 0.50000\n",
            "Epoch [5/5] Step [1495/1610]: discriminator_loss = 0.53806, target_loss = 0.39775, acc = 0.50000\n",
            "Epoch [5/5] Step [1496/1610]: discriminator_loss = 0.54086, target_loss = 0.41056, acc = 0.50000\n",
            "Epoch [5/5] Step [1497/1610]: discriminator_loss = 0.55507, target_loss = 0.40518, acc = 0.50000\n",
            "Epoch [5/5] Step [1498/1610]: discriminator_loss = 0.56095, target_loss = 0.40188, acc = 0.50000\n",
            "Epoch [5/5] Step [1499/1610]: discriminator_loss = 0.54699, target_loss = 0.39735, acc = 0.50000\n",
            "Epoch [5/5] Step [1500/1610]: discriminator_loss = 0.58937, target_loss = 0.40435, acc = 0.50000\n",
            "Epoch [5/5] Step [1501/1610]: discriminator_loss = 0.55308, target_loss = 0.37763, acc = 0.50000\n",
            "Epoch [5/5] Step [1502/1610]: discriminator_loss = 0.55214, target_loss = 0.39021, acc = 0.50000\n",
            "Epoch [5/5] Step [1503/1610]: discriminator_loss = 0.56633, target_loss = 0.40087, acc = 0.50000\n",
            "Epoch [5/5] Step [1504/1610]: discriminator_loss = 0.55385, target_loss = 0.40301, acc = 0.50000\n",
            "Epoch [5/5] Step [1505/1610]: discriminator_loss = 0.55317, target_loss = 0.39780, acc = 0.50000\n",
            "Epoch [5/5] Step [1506/1610]: discriminator_loss = 0.56692, target_loss = 0.39484, acc = 0.50000\n",
            "Epoch [5/5] Step [1507/1610]: discriminator_loss = 0.56548, target_loss = 0.40118, acc = 0.50000\n",
            "Epoch [5/5] Step [1508/1610]: discriminator_loss = 0.55605, target_loss = 0.41187, acc = 0.50000\n",
            "Epoch [5/5] Step [1509/1610]: discriminator_loss = 0.56361, target_loss = 0.40382, acc = 0.50000\n",
            "Epoch [5/5] Step [1510/1610]: discriminator_loss = 0.55042, target_loss = 0.41390, acc = 0.50000\n",
            "Epoch [5/5] Step [1511/1610]: discriminator_loss = 0.54509, target_loss = 0.40762, acc = 0.50000\n",
            "Epoch [5/5] Step [1512/1610]: discriminator_loss = 0.54306, target_loss = 0.40353, acc = 0.50000\n",
            "Epoch [5/5] Step [1513/1610]: discriminator_loss = 0.54905, target_loss = 0.40759, acc = 0.50000\n",
            "Epoch [5/5] Step [1514/1610]: discriminator_loss = 0.54864, target_loss = 0.41605, acc = 0.50000\n",
            "Epoch [5/5] Step [1515/1610]: discriminator_loss = 0.56308, target_loss = 0.40865, acc = 0.50000\n",
            "Epoch [5/5] Step [1516/1610]: discriminator_loss = 0.55258, target_loss = 0.40219, acc = 0.50000\n",
            "Epoch [5/5] Step [1517/1610]: discriminator_loss = 0.55868, target_loss = 0.40134, acc = 0.50000\n",
            "Epoch [5/5] Step [1518/1610]: discriminator_loss = 0.54589, target_loss = 0.40433, acc = 0.50000\n",
            "Epoch [5/5] Step [1519/1610]: discriminator_loss = 0.55213, target_loss = 0.40814, acc = 0.50000\n",
            "Epoch [5/5] Step [1520/1610]: discriminator_loss = 0.56174, target_loss = 0.38961, acc = 0.50000\n",
            "Epoch [5/5] Step [1521/1610]: discriminator_loss = 0.55198, target_loss = 0.41234, acc = 0.50000\n",
            "Epoch [5/5] Step [1522/1610]: discriminator_loss = 0.54991, target_loss = 0.41287, acc = 0.50000\n",
            "Epoch [5/5] Step [1523/1610]: discriminator_loss = 0.55138, target_loss = 0.40225, acc = 0.50000\n",
            "Epoch [5/5] Step [1524/1610]: discriminator_loss = 0.54416, target_loss = 0.41791, acc = 0.50000\n",
            "Epoch [5/5] Step [1525/1610]: discriminator_loss = 0.55544, target_loss = 0.40909, acc = 0.50000\n",
            "Epoch [5/5] Step [1526/1610]: discriminator_loss = 0.54967, target_loss = 0.41259, acc = 0.50000\n",
            "Epoch [5/5] Step [1527/1610]: discriminator_loss = 0.55316, target_loss = 0.40751, acc = 0.50000\n",
            "Epoch [5/5] Step [1528/1610]: discriminator_loss = 0.56538, target_loss = 0.40485, acc = 0.50000\n",
            "Epoch [5/5] Step [1529/1610]: discriminator_loss = 0.56234, target_loss = 0.40084, acc = 0.50000\n",
            "Epoch [5/5] Step [1530/1610]: discriminator_loss = 0.58431, target_loss = 0.40506, acc = 0.50000\n",
            "Epoch [5/5] Step [1531/1610]: discriminator_loss = 0.55139, target_loss = 0.39430, acc = 0.50000\n",
            "Epoch [5/5] Step [1532/1610]: discriminator_loss = 0.58577, target_loss = 0.39628, acc = 0.50000\n",
            "Epoch [5/5] Step [1533/1610]: discriminator_loss = 0.56094, target_loss = 0.39720, acc = 0.50000\n",
            "Epoch [5/5] Step [1534/1610]: discriminator_loss = 0.59905, target_loss = 0.39343, acc = 0.50000\n",
            "Epoch [5/5] Step [1535/1610]: discriminator_loss = 0.56012, target_loss = 0.39648, acc = 0.50000\n",
            "Epoch [5/5] Step [1536/1610]: discriminator_loss = 0.56602, target_loss = 0.40046, acc = 0.50000\n",
            "Epoch [5/5] Step [1537/1610]: discriminator_loss = 0.56372, target_loss = 0.39826, acc = 0.50000\n",
            "Epoch [5/5] Step [1538/1610]: discriminator_loss = 0.56235, target_loss = 0.39658, acc = 0.50000\n",
            "Epoch [5/5] Step [1539/1610]: discriminator_loss = 0.56028, target_loss = 0.39742, acc = 0.50000\n",
            "Epoch [5/5] Step [1540/1610]: discriminator_loss = 0.55922, target_loss = 0.40024, acc = 0.50000\n",
            "Epoch [5/5] Step [1541/1610]: discriminator_loss = 0.55808, target_loss = 0.40168, acc = 0.50000\n",
            "Epoch [5/5] Step [1542/1610]: discriminator_loss = 0.55684, target_loss = 0.40158, acc = 0.50000\n",
            "Epoch [5/5] Step [1543/1610]: discriminator_loss = 0.55441, target_loss = 0.40132, acc = 0.50000\n",
            "Epoch [5/5] Step [1544/1610]: discriminator_loss = 0.58013, target_loss = 0.38605, acc = 0.50000\n",
            "Epoch [5/5] Step [1545/1610]: discriminator_loss = 0.56152, target_loss = 0.39416, acc = 0.50000\n",
            "Epoch [5/5] Step [1546/1610]: discriminator_loss = 0.56276, target_loss = 0.40381, acc = 0.50000\n",
            "Epoch [5/5] Step [1547/1610]: discriminator_loss = 0.55843, target_loss = 0.40873, acc = 0.50000\n",
            "Epoch [5/5] Step [1548/1610]: discriminator_loss = 0.57553, target_loss = 0.40739, acc = 0.50000\n",
            "Epoch [5/5] Step [1549/1610]: discriminator_loss = 0.56087, target_loss = 0.39274, acc = 0.50000\n",
            "Epoch [5/5] Step [1550/1610]: discriminator_loss = 0.55919, target_loss = 0.39913, acc = 0.50000\n",
            "Epoch [5/5] Step [1551/1610]: discriminator_loss = 0.55377, target_loss = 0.39984, acc = 0.50000\n",
            "Epoch [5/5] Step [1552/1610]: discriminator_loss = 0.54610, target_loss = 0.40763, acc = 0.50000\n",
            "Epoch [5/5] Step [1553/1610]: discriminator_loss = 0.54731, target_loss = 0.40419, acc = 0.50000\n",
            "Epoch [5/5] Step [1554/1610]: discriminator_loss = 0.55026, target_loss = 0.40671, acc = 0.50000\n",
            "Epoch [5/5] Step [1555/1610]: discriminator_loss = 0.55910, target_loss = 0.40830, acc = 0.50000\n",
            "Epoch [5/5] Step [1556/1610]: discriminator_loss = 0.54641, target_loss = 0.40891, acc = 0.50000\n",
            "Epoch [5/5] Step [1557/1610]: discriminator_loss = 0.55257, target_loss = 0.40235, acc = 0.50000\n",
            "Epoch [5/5] Step [1558/1610]: discriminator_loss = 0.56129, target_loss = 0.41567, acc = 0.50000\n",
            "Epoch [5/5] Step [1559/1610]: discriminator_loss = 0.56012, target_loss = 0.41303, acc = 0.50000\n",
            "Epoch [5/5] Step [1560/1610]: discriminator_loss = 0.54343, target_loss = 0.40758, acc = 0.50000\n",
            "Epoch [5/5] Step [1561/1610]: discriminator_loss = 0.55016, target_loss = 0.41491, acc = 0.50000\n",
            "Epoch [5/5] Step [1562/1610]: discriminator_loss = 0.56034, target_loss = 0.39819, acc = 0.50000\n",
            "Epoch [5/5] Step [1563/1610]: discriminator_loss = 0.56298, target_loss = 0.40952, acc = 0.50000\n",
            "Epoch [5/5] Step [1564/1610]: discriminator_loss = 0.54807, target_loss = 0.40705, acc = 0.50000\n",
            "Epoch [5/5] Step [1565/1610]: discriminator_loss = 0.55351, target_loss = 0.40938, acc = 0.50000\n",
            "Epoch [5/5] Step [1566/1610]: discriminator_loss = 0.56180, target_loss = 0.40016, acc = 0.50000\n",
            "Epoch [5/5] Step [1567/1610]: discriminator_loss = 0.55701, target_loss = 0.40856, acc = 0.50000\n",
            "Epoch [5/5] Step [1568/1610]: discriminator_loss = 0.56068, target_loss = 0.41296, acc = 0.50000\n",
            "Epoch [5/5] Step [1569/1610]: discriminator_loss = 0.55536, target_loss = 0.40063, acc = 0.50000\n",
            "Epoch [5/5] Step [1570/1610]: discriminator_loss = 0.55231, target_loss = 0.39846, acc = 0.50000\n",
            "Epoch [5/5] Step [1571/1610]: discriminator_loss = 0.55828, target_loss = 0.38056, acc = 0.50000\n",
            "Epoch [5/5] Step [1572/1610]: discriminator_loss = 0.55270, target_loss = 0.40804, acc = 0.50000\n",
            "Epoch [5/5] Step [1573/1610]: discriminator_loss = 0.54937, target_loss = 0.41201, acc = 0.50000\n",
            "Epoch [5/5] Step [1574/1610]: discriminator_loss = 0.54635, target_loss = 0.41082, acc = 0.50000\n",
            "Epoch [5/5] Step [1575/1610]: discriminator_loss = 0.55623, target_loss = 0.41098, acc = 0.50000\n",
            "Epoch [5/5] Step [1576/1610]: discriminator_loss = 0.54735, target_loss = 0.40984, acc = 0.50000\n",
            "Epoch [5/5] Step [1577/1610]: discriminator_loss = 0.54597, target_loss = 0.39976, acc = 0.50000\n",
            "Epoch [5/5] Step [1578/1610]: discriminator_loss = 0.54687, target_loss = 0.40422, acc = 0.50000\n",
            "Epoch [5/5] Step [1579/1610]: discriminator_loss = 0.55310, target_loss = 0.41542, acc = 0.50000\n",
            "Epoch [5/5] Step [1580/1610]: discriminator_loss = 0.54730, target_loss = 0.40315, acc = 0.50000\n",
            "Epoch [5/5] Step [1581/1610]: discriminator_loss = 0.55273, target_loss = 0.40072, acc = 0.50000\n",
            "Epoch [5/5] Step [1582/1610]: discriminator_loss = 0.55728, target_loss = 0.39662, acc = 0.50000\n",
            "Epoch [5/5] Step [1583/1610]: discriminator_loss = 0.57033, target_loss = 0.41861, acc = 0.50000\n",
            "Epoch [5/5] Step [1584/1610]: discriminator_loss = 0.55222, target_loss = 0.40900, acc = 0.50000\n",
            "Epoch [5/5] Step [1585/1610]: discriminator_loss = 0.56596, target_loss = 0.40028, acc = 0.50000\n",
            "Epoch [5/5] Step [1586/1610]: discriminator_loss = 0.54638, target_loss = 0.40040, acc = 0.50000\n",
            "Epoch [5/5] Step [1587/1610]: discriminator_loss = 0.54578, target_loss = 0.40816, acc = 0.50000\n",
            "Epoch [5/5] Step [1588/1610]: discriminator_loss = 0.53710, target_loss = 0.39597, acc = 0.50000\n",
            "Epoch [5/5] Step [1589/1610]: discriminator_loss = 0.57232, target_loss = 0.40228, acc = 0.50000\n",
            "Epoch [5/5] Step [1590/1610]: discriminator_loss = 0.54738, target_loss = 0.41549, acc = 0.50000\n",
            "Epoch [5/5] Step [1591/1610]: discriminator_loss = 0.53693, target_loss = 0.41311, acc = 0.50000\n",
            "Epoch [5/5] Step [1592/1610]: discriminator_loss = 0.55577, target_loss = 0.39687, acc = 0.50000\n",
            "Epoch [5/5] Step [1593/1610]: discriminator_loss = 0.54733, target_loss = 0.38940, acc = 0.50000\n",
            "Epoch [5/5] Step [1594/1610]: discriminator_loss = 0.54877, target_loss = 0.40549, acc = 0.50000\n",
            "Epoch [5/5] Step [1595/1610]: discriminator_loss = 0.58538, target_loss = 0.41205, acc = 0.50000\n",
            "Epoch [5/5] Step [1596/1610]: discriminator_loss = 0.54144, target_loss = 0.38895, acc = 0.50000\n",
            "Epoch [5/5] Step [1597/1610]: discriminator_loss = 0.53915, target_loss = 0.40393, acc = 0.50000\n",
            "Epoch [5/5] Step [1598/1610]: discriminator_loss = 0.54089, target_loss = 0.39269, acc = 0.50000\n",
            "Epoch [5/5] Step [1599/1610]: discriminator_loss = 0.56146, target_loss = 0.39742, acc = 0.50000\n",
            "Epoch [5/5] Step [1600/1610]: discriminator_loss = 0.54385, target_loss = 0.39285, acc = 0.50000\n",
            "Epoch [5/5] Step [1601/1610]: discriminator_loss = 0.55147, target_loss = 0.41571, acc = 0.50000\n",
            "Epoch [5/5] Step [1602/1610]: discriminator_loss = 0.54251, target_loss = 0.39911, acc = 0.50000\n",
            "Epoch [5/5] Step [1603/1610]: discriminator_loss = 0.55646, target_loss = 0.39004, acc = 0.50000\n",
            "Epoch [5/5] Step [1604/1610]: discriminator_loss = 0.54774, target_loss = 0.40566, acc = 0.50000\n",
            "Epoch [5/5] Step [1605/1610]: discriminator_loss = 0.53989, target_loss = 0.39962, acc = 0.50000\n",
            "Epoch [5/5] Step [1606/1610]: discriminator_loss = 0.57918, target_loss = 0.39695, acc = 0.50000\n",
            "Epoch [5/5] Step [1607/1610]: discriminator_loss = 0.58276, target_loss = 0.38810, acc = 0.50000\n",
            "Epoch [5/5] Step [1608/1610]: discriminator_loss = 0.55594, target_loss = 0.37117, acc = 0.50000\n",
            "Epoch [5/5] Step [1609/1610]: discriminator_loss = 0.60594, target_loss = 0.35953, acc = 0.50000\n",
            "Epoch [5/5] Step [1610/1610]: discriminator_loss = 0.54276, target_loss = 0.39537, acc = 0.50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCE7ojDEWIjihM8ZYiOCEzhhjIeL/AQWXNLkuo8xTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcuKpM7U2kXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlvrgh3dIBL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#torch.save(tgt_encoder.state_dict(),os.path.join('/content/drive/My Drive/grad_cam','tgt_encoder.pt'))\n",
        "#model = tgt_model\n",
        "#model.save_state_dict('tgt_encoder.pt')\n",
        "\n",
        "#/content/drive/My Drive/prova_real/train\n",
        "#torch.save({'epoch':NUM_EPOCHS,\n",
        "#           'state_dict': tgt_encoder.state_dict(),\n",
        "#            'optimizer_state_dict': target_optimizer.state_dict(),\n",
        "#           'loss': loss_tgt,\n",
        "#           'best_acc1': accuracy,   \n",
        "#           },'/content/drive/My Drive/grad_cam/tgt_encoder.pt')\n",
        "#'/content/drive/My Drive/prova_real'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU3yINM92dNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koU90tLLuSWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT0UwXJrvN-E",
        "colab_type": "code",
        "outputId": "3c642e51-c40a-4ba4-b0d7-46caa645415e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Train evidence CNN \n",
        "evidence_encoder = vgg16(pretrained = True)\n",
        "evidence_classifier = Vgg_classifier()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "ev_parameters_to_optimize = evidence_encoder.parameters() \n",
        "ev_optimizer = optim.SGD(ev_parameters_to_optimize, lr=LR_tgt, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "ev_scheduler = optim.lr_scheduler.StepLR(ev_optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "evidence_loss_vector = []\n",
        "\n",
        "evidence_encoder = tgt_model  \n",
        "evidence_classifier = src_classifier\n",
        "\n",
        "cudnn.benchmark\n",
        "\n",
        "current_step = 0\n",
        "\n",
        "evidence_encoder = evidence_encoder.to(DEVICE)\n",
        "evidence_classifier = evidence_classifier.to(DEVICE)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for images,labels in saliency_dataloader:\n",
        "\n",
        "      evidence_encoder.train()\n",
        "      evidence_classifier.train()\n",
        "\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      ev_optimizer.zero_grad()\n",
        "\n",
        "      out = evidence_classifier(evidence_encoder(images))\n",
        "\n",
        "      loss = criterion(out,labels)\n",
        "      evidence_loss_vector.append(loss)\n",
        "\n",
        "\n",
        "      # Log loss\n",
        "      if current_step % LOG_FREQUENCY == 0:\n",
        "        print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "      loss.backward()  \n",
        "      ev_optimizer.step() \n",
        "\n",
        "      current_step += 1\n",
        "\n",
        "    ev_scheduler.step()\n",
        "\n",
        "plt.plot(evidence_loss_vector)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0, Loss 1.0790467262268066\n",
            "Step 10, Loss 0.32089340686798096\n",
            "Step 20, Loss 1.9708759784698486\n",
            "Step 30, Loss 0.8344789743423462\n",
            "Step 40, Loss 1.181681513786316\n",
            "Step 50, Loss 0.8145287036895752\n",
            "Step 60, Loss 2.3103256225585938\n",
            "Step 70, Loss 1.8742915391921997\n",
            "Step 80, Loss 1.2976469993591309\n",
            "Step 90, Loss 1.940340280532837\n",
            "Step 100, Loss 0.7737430334091187\n",
            "Step 110, Loss 1.9906104803085327\n",
            "Step 120, Loss 0.401292085647583\n",
            "Step 130, Loss 0.8914834260940552\n",
            "Step 140, Loss 1.274228572845459\n",
            "Step 150, Loss 1.5428593158721924\n",
            "Step 160, Loss 1.0940757989883423\n",
            "Step 170, Loss 1.6948909759521484\n",
            "Step 180, Loss 1.3569023609161377\n",
            "Step 190, Loss 1.1844967603683472\n",
            "Step 200, Loss 1.6239750385284424\n",
            "Step 210, Loss 1.4776177406311035\n",
            "Step 220, Loss 0.9369149804115295\n",
            "Step 230, Loss 1.394446611404419\n",
            "Step 240, Loss 0.6904773712158203\n",
            "Step 250, Loss 1.4156678915023804\n",
            "Step 260, Loss 1.097252607345581\n",
            "Step 270, Loss 0.5573746562004089\n",
            "Step 280, Loss 1.2640106678009033\n",
            "Step 290, Loss 1.4210480451583862\n",
            "Step 300, Loss 1.1076653003692627\n",
            "Step 310, Loss 2.1657657623291016\n",
            "Step 320, Loss 3.069897413253784\n",
            "Step 330, Loss 2.4280879497528076\n",
            "Step 340, Loss 1.7111389636993408\n",
            "Step 350, Loss 1.6043424606323242\n",
            "Step 360, Loss 1.4666069746017456\n",
            "Step 370, Loss 1.3475244045257568\n",
            "Step 380, Loss 2.620388984680176\n",
            "Step 390, Loss 1.6572221517562866\n",
            "Step 400, Loss 1.4632823467254639\n",
            "Step 410, Loss 1.5396655797958374\n",
            "Step 420, Loss 0.36755189299583435\n",
            "Step 430, Loss 1.7621557712554932\n",
            "Step 440, Loss 2.1973400115966797\n",
            "Step 450, Loss 1.3413548469543457\n",
            "Step 460, Loss 0.7740483283996582\n",
            "Step 470, Loss 1.64394211769104\n",
            "Step 480, Loss 1.2537890672683716\n",
            "Step 490, Loss 1.5829713344573975\n",
            "Step 500, Loss 1.5864043235778809\n",
            "Step 510, Loss 1.1602091789245605\n",
            "Step 520, Loss 1.0983093976974487\n",
            "Step 530, Loss 1.4655537605285645\n",
            "Step 540, Loss 1.2331855297088623\n",
            "Step 550, Loss 2.0603954792022705\n",
            "Step 560, Loss 1.7778728008270264\n",
            "Step 570, Loss 1.8987022638320923\n",
            "Step 580, Loss 0.8765271902084351\n",
            "Step 590, Loss 1.7629518508911133\n",
            "Step 600, Loss 1.8099266290664673\n",
            "Step 610, Loss 1.6336383819580078\n",
            "Step 620, Loss 1.4955940246582031\n",
            "Step 630, Loss 1.7947756052017212\n",
            "Step 640, Loss 1.6318609714508057\n",
            "Step 650, Loss 1.2120845317840576\n",
            "Step 660, Loss 0.6709891557693481\n",
            "Step 670, Loss 1.1117122173309326\n",
            "Step 680, Loss 1.288679599761963\n",
            "Step 690, Loss 1.2223109006881714\n",
            "Step 700, Loss 1.3636283874511719\n",
            "Step 710, Loss 1.7004536390304565\n",
            "Step 720, Loss 2.473362684249878\n",
            "Step 730, Loss 1.1465673446655273\n",
            "Step 740, Loss 1.5865814685821533\n",
            "Step 750, Loss 1.162302851676941\n",
            "Step 760, Loss 1.293434500694275\n",
            "Step 770, Loss 1.350008487701416\n",
            "Step 780, Loss 1.2211096286773682\n",
            "Step 790, Loss 1.1338242292404175\n",
            "Step 800, Loss 1.5969109535217285\n",
            "Step 810, Loss 2.3656678199768066\n",
            "Step 820, Loss 2.233457565307617\n",
            "Step 830, Loss 2.857241630554199\n",
            "Step 840, Loss 1.610937476158142\n",
            "Step 850, Loss 1.8191689252853394\n",
            "Step 860, Loss 1.6959519386291504\n",
            "Step 870, Loss 2.6850671768188477\n",
            "Step 880, Loss 1.637951374053955\n",
            "Step 890, Loss 1.6608144044876099\n",
            "Step 900, Loss 1.5982623100280762\n",
            "Step 910, Loss 1.520805835723877\n",
            "Step 920, Loss 1.7843317985534668\n",
            "Step 930, Loss 1.0920515060424805\n",
            "Step 940, Loss 1.6742072105407715\n",
            "Step 950, Loss 1.6632118225097656\n",
            "Step 960, Loss 1.4564288854599\n",
            "Step 970, Loss 0.7777186632156372\n",
            "Step 980, Loss 1.0437904596328735\n",
            "Step 990, Loss 1.388214111328125\n",
            "Step 1000, Loss 0.9990291595458984\n",
            "Step 1010, Loss 1.991513967514038\n",
            "Step 1020, Loss 2.3179755210876465\n",
            "Step 1030, Loss 1.4946348667144775\n",
            "Step 1040, Loss 1.349540114402771\n",
            "Step 1050, Loss 1.6839563846588135\n",
            "Step 1060, Loss 1.5619045495986938\n",
            "Step 1070, Loss 1.4327739477157593\n",
            "Step 1080, Loss 1.5068325996398926\n",
            "Step 1090, Loss 2.1593101024627686\n",
            "Step 1100, Loss 2.578725576400757\n",
            "Step 1110, Loss 1.7511554956436157\n",
            "Step 1120, Loss 2.141787528991699\n",
            "Step 1130, Loss 1.1729800701141357\n",
            "Step 1140, Loss 1.874861717224121\n",
            "Step 1150, Loss 1.4969574213027954\n",
            "Step 1160, Loss 1.2687418460845947\n",
            "Step 1170, Loss 0.4632871747016907\n",
            "Step 1180, Loss 0.9160705804824829\n",
            "Step 1190, Loss 1.2472584247589111\n",
            "Step 1200, Loss 1.3479384183883667\n",
            "Step 1210, Loss 0.7461141347885132\n",
            "Step 1220, Loss 1.6245783567428589\n",
            "Step 1230, Loss 1.0165072679519653\n",
            "Step 1240, Loss 1.885077953338623\n",
            "Step 1250, Loss 1.352142572402954\n",
            "Step 1260, Loss 0.8839738368988037\n",
            "Step 1270, Loss 0.8883062601089478\n",
            "Step 1280, Loss 1.0472114086151123\n",
            "Step 1290, Loss 1.2705103158950806\n",
            "Step 1300, Loss 2.513862133026123\n",
            "Step 1310, Loss 1.2935702800750732\n",
            "Step 1320, Loss 1.5739811658859253\n",
            "Step 1330, Loss 0.9261997938156128\n",
            "Step 1340, Loss 1.1205224990844727\n",
            "Step 1350, Loss 1.6083667278289795\n",
            "Step 1360, Loss 1.4114408493041992\n",
            "Step 1370, Loss 1.8655247688293457\n",
            "Step 1380, Loss 0.624343752861023\n",
            "Step 1390, Loss 2.024494171142578\n",
            "Step 1400, Loss 1.182546615600586\n",
            "Step 1410, Loss 1.6577174663543701\n",
            "Step 1420, Loss 1.4708878993988037\n",
            "Step 1430, Loss 0.7893937826156616\n",
            "Step 1440, Loss 0.9014604091644287\n",
            "Step 1450, Loss 1.8353480100631714\n",
            "Step 1460, Loss 1.1327755451202393\n",
            "Step 1470, Loss 1.1061370372772217\n",
            "Step 1480, Loss 1.7570452690124512\n",
            "Step 1490, Loss 1.6711663007736206\n",
            "Step 1500, Loss 1.4065203666687012\n",
            "Step 1510, Loss 1.295915961265564\n",
            "Step 1520, Loss 1.953779697418213\n",
            "Step 1530, Loss 1.5819370746612549\n",
            "Step 1540, Loss 2.425720453262329\n",
            "Step 1550, Loss 1.005206823348999\n",
            "Step 1560, Loss 1.999159574508667\n",
            "Step 1570, Loss 2.1023473739624023\n",
            "Step 1580, Loss 1.1003878116607666\n",
            "Step 1590, Loss 2.17470383644104\n",
            "Step 1600, Loss 0.41971519589424133\n",
            "Step 1610, Loss 1.3297450542449951\n",
            "Step 1620, Loss 1.417984127998352\n",
            "Step 1630, Loss 1.3100109100341797\n",
            "Step 1640, Loss 1.944926142692566\n",
            "Step 1650, Loss 1.91690194606781\n",
            "Step 1660, Loss 1.9045417308807373\n",
            "Step 1670, Loss 0.926079511642456\n",
            "Step 1680, Loss 1.9063174724578857\n",
            "Step 1690, Loss 1.5559961795806885\n",
            "Step 1700, Loss 2.263092041015625\n",
            "Step 1710, Loss 1.7039674520492554\n",
            "Step 1720, Loss 1.0503512620925903\n",
            "Step 1730, Loss 1.250343680381775\n",
            "Step 1740, Loss 1.4105404615402222\n",
            "Step 1750, Loss 1.6277697086334229\n",
            "Step 1760, Loss 1.3539021015167236\n",
            "Step 1770, Loss 1.7222263813018799\n",
            "Step 1780, Loss 1.1578640937805176\n",
            "Step 1790, Loss 1.566709041595459\n",
            "Step 1800, Loss 2.0659291744232178\n",
            "Step 1810, Loss 1.7023860216140747\n",
            "Step 1820, Loss 1.9965184926986694\n",
            "Step 1830, Loss 1.934129238128662\n",
            "Step 1840, Loss 0.9098966121673584\n",
            "Step 1850, Loss 0.6897726655006409\n",
            "Step 1860, Loss 1.439535140991211\n",
            "Step 1870, Loss 1.1549885272979736\n",
            "Step 1880, Loss 1.1977046728134155\n",
            "Step 1890, Loss 0.9523293972015381\n",
            "Step 1900, Loss 2.7411744594573975\n",
            "Step 1910, Loss 1.6300902366638184\n",
            "Step 1920, Loss 1.9192633628845215\n",
            "Step 1930, Loss 2.175204038619995\n",
            "Step 1940, Loss 0.7968906760215759\n",
            "Step 1950, Loss 1.7344025373458862\n",
            "Step 1960, Loss 1.6698590517044067\n",
            "Step 1970, Loss 1.762041687965393\n",
            "Step 1980, Loss 1.473841667175293\n",
            "Step 1990, Loss 1.656735897064209\n",
            "Step 2000, Loss 1.814536452293396\n",
            "Step 2010, Loss 2.288090467453003\n",
            "Step 2020, Loss 2.552171230316162\n",
            "Step 2030, Loss 1.3824505805969238\n",
            "Step 2040, Loss 0.8275582790374756\n",
            "Step 2050, Loss 0.7843089699745178\n",
            "Step 2060, Loss 0.9634321331977844\n",
            "Step 2070, Loss 1.372568130493164\n",
            "Step 2080, Loss 0.9489595293998718\n",
            "Step 2090, Loss 1.362006425857544\n",
            "Step 2100, Loss 1.0540425777435303\n",
            "Step 2110, Loss 1.2096641063690186\n",
            "Step 2120, Loss 1.7801581621170044\n",
            "Step 2130, Loss 1.1344070434570312\n",
            "Step 2140, Loss 1.4454251527786255\n",
            "Step 2150, Loss 1.5985740423202515\n",
            "Step 2160, Loss 1.0635703802108765\n",
            "Step 2170, Loss 1.1556932926177979\n",
            "Step 2180, Loss 1.4698535203933716\n",
            "Step 2190, Loss 0.701088011264801\n",
            "Step 2200, Loss 1.6426136493682861\n",
            "Step 2210, Loss 1.293930172920227\n",
            "Step 2220, Loss 0.8394466638565063\n",
            "Step 2230, Loss 1.3242645263671875\n",
            "Step 2240, Loss 1.5301144123077393\n",
            "Step 2250, Loss 1.1545519828796387\n",
            "Step 2260, Loss 1.1301424503326416\n",
            "Step 2270, Loss 0.5882962942123413\n",
            "Step 2280, Loss 1.00304114818573\n",
            "Step 2290, Loss 1.7185713052749634\n",
            "Step 2300, Loss 1.8345715999603271\n",
            "Step 2310, Loss 1.0541577339172363\n",
            "Step 2320, Loss 0.5021929740905762\n",
            "Step 2330, Loss 2.4062492847442627\n",
            "Step 2340, Loss 1.5451420545578003\n",
            "Step 2350, Loss 1.090728521347046\n",
            "Step 2360, Loss 2.8847692012786865\n",
            "Step 2370, Loss 1.8543421030044556\n",
            "Step 2380, Loss 1.0569685697555542\n",
            "Step 2390, Loss 2.5248260498046875\n",
            "Step 2400, Loss 1.0807111263275146\n",
            "Step 2410, Loss 1.8993737697601318\n",
            "Step 2420, Loss 0.8244290351867676\n",
            "Step 2430, Loss 2.771702289581299\n",
            "Step 2440, Loss 1.7927061319351196\n",
            "Step 2450, Loss 1.4208482503890991\n",
            "Step 2460, Loss 2.5417165756225586\n",
            "Step 2470, Loss 0.7330063581466675\n",
            "Step 2480, Loss 1.4013285636901855\n",
            "Step 2490, Loss 3.3187968730926514\n",
            "Step 2500, Loss 2.1675145626068115\n",
            "Step 2510, Loss 1.9650044441223145\n",
            "Step 2520, Loss 1.4037493467330933\n",
            "Step 2530, Loss 1.2050877809524536\n",
            "Step 2540, Loss 1.993551254272461\n",
            "Step 2550, Loss 1.7587052583694458\n",
            "Step 2560, Loss 1.8629639148712158\n",
            "Step 2570, Loss 1.8956329822540283\n",
            "Step 2580, Loss 0.6226296424865723\n",
            "Step 2590, Loss 2.0616111755371094\n",
            "Step 2600, Loss 0.7544710636138916\n",
            "Step 2610, Loss 1.0581791400909424\n",
            "Step 2620, Loss 1.9328503608703613\n",
            "Step 2630, Loss 1.6981115341186523\n",
            "Step 2640, Loss 1.4768794775009155\n",
            "Step 2650, Loss 2.0324456691741943\n",
            "Step 2660, Loss 0.8250637054443359\n",
            "Step 2670, Loss 1.40317702293396\n",
            "Step 2680, Loss 1.0731281042099\n",
            "Step 2690, Loss 1.4650752544403076\n",
            "Step 2700, Loss 1.4385812282562256\n",
            "Step 2710, Loss 2.8664135932922363\n",
            "Step 2720, Loss 1.0601532459259033\n",
            "Step 2730, Loss 1.7669212818145752\n",
            "Step 2740, Loss 0.866263210773468\n",
            "Step 2750, Loss 2.1520376205444336\n",
            "Step 2760, Loss 2.0236849784851074\n",
            "Step 2770, Loss 1.9656747579574585\n",
            "Step 2780, Loss 1.5789822340011597\n",
            "Step 2790, Loss 1.0437490940093994\n",
            "Step 2800, Loss 1.5498133897781372\n",
            "Step 2810, Loss 1.156125545501709\n",
            "Step 2820, Loss 1.0351636409759521\n",
            "Step 2830, Loss 1.1890881061553955\n",
            "Step 2840, Loss 1.253482460975647\n",
            "Step 2850, Loss 1.4011576175689697\n",
            "Step 2860, Loss 1.6231238842010498\n",
            "Step 2870, Loss 1.8116096258163452\n",
            "Step 2880, Loss 1.2713162899017334\n",
            "Step 2890, Loss 1.6902735233306885\n",
            "Step 2900, Loss 1.8760457038879395\n",
            "Step 2910, Loss 0.9465007781982422\n",
            "Step 2920, Loss 1.6347332000732422\n",
            "Step 2930, Loss 1.6519683599472046\n",
            "Step 2940, Loss 2.0617995262145996\n",
            "Step 2950, Loss 1.6525516510009766\n",
            "Step 2960, Loss 1.5263409614562988\n",
            "Step 2970, Loss 1.5671418905258179\n",
            "Step 2980, Loss 2.2270162105560303\n",
            "Step 2990, Loss 1.5555516481399536\n",
            "Step 3000, Loss 1.3509958982467651\n",
            "Step 3010, Loss 0.6099823713302612\n",
            "Step 3020, Loss 1.999267578125\n",
            "Step 3030, Loss 1.2133122682571411\n",
            "Step 3040, Loss 1.3085746765136719\n",
            "Step 3050, Loss 1.3688032627105713\n",
            "Step 3060, Loss 2.48301100730896\n",
            "Step 3070, Loss 1.0263999700546265\n",
            "Step 3080, Loss 2.173004627227783\n",
            "Step 3090, Loss 1.3985589742660522\n",
            "Step 3100, Loss 2.0376744270324707\n",
            "Step 3110, Loss 0.7325044274330139\n",
            "Step 3120, Loss 1.9129893779754639\n",
            "Step 3130, Loss 1.726349949836731\n",
            "Step 3140, Loss 0.49907219409942627\n",
            "Step 3150, Loss 1.7381078004837036\n",
            "Step 3160, Loss 1.701473593711853\n",
            "Step 3170, Loss 1.1430199146270752\n",
            "Step 3180, Loss 1.7825658321380615\n",
            "Step 3190, Loss 1.2036556005477905\n",
            "Step 3200, Loss 1.8054273128509521\n",
            "Step 3210, Loss 1.110130786895752\n",
            "Step 3220, Loss 1.7371069192886353\n",
            "Step 3230, Loss 1.7502522468566895\n",
            "Step 3240, Loss 1.186830759048462\n",
            "Step 3250, Loss 1.7185176610946655\n",
            "Step 3260, Loss 1.5717823505401611\n",
            "Step 3270, Loss 1.8857598304748535\n",
            "Step 3280, Loss 1.2158793210983276\n",
            "Step 3290, Loss 1.0227986574172974\n",
            "Step 3300, Loss 1.466097354888916\n",
            "Step 3310, Loss 1.8285853862762451\n",
            "Step 3320, Loss 0.8041338324546814\n",
            "Step 3330, Loss 1.5360280275344849\n",
            "Step 3340, Loss 2.0965423583984375\n",
            "Step 3350, Loss 1.9451810121536255\n",
            "Step 3360, Loss 1.5880444049835205\n",
            "Step 3370, Loss 1.4610801935195923\n",
            "Step 3380, Loss 1.8510487079620361\n",
            "Step 3390, Loss 1.7315362691879272\n",
            "Step 3400, Loss 2.241901397705078\n",
            "Step 3410, Loss 1.2559947967529297\n",
            "Step 3420, Loss 1.3904399871826172\n",
            "Step 3430, Loss 1.4121557474136353\n",
            "Step 3440, Loss 1.2978509664535522\n",
            "Step 3450, Loss 1.4768366813659668\n",
            "Step 3460, Loss 1.1928129196166992\n",
            "Step 3470, Loss 1.3665015697479248\n",
            "Step 3480, Loss 1.1830044984817505\n",
            "Step 3490, Loss 1.178128719329834\n",
            "Step 3500, Loss 1.4576969146728516\n",
            "Step 3510, Loss 1.082573652267456\n",
            "Step 3520, Loss 0.7884643077850342\n",
            "Step 3530, Loss 1.2972019910812378\n",
            "Step 3540, Loss 0.9402027726173401\n",
            "Step 3550, Loss 1.481188416481018\n",
            "Step 3560, Loss 1.761056661605835\n",
            "Step 3570, Loss 1.0786306858062744\n",
            "Step 3580, Loss 1.304229736328125\n",
            "Step 3590, Loss 0.16241306066513062\n",
            "Step 3600, Loss 0.8722682595252991\n",
            "Step 3610, Loss 1.1474896669387817\n",
            "Step 3620, Loss 1.3278049230575562\n",
            "Step 3630, Loss 1.7116087675094604\n",
            "Step 3640, Loss 2.0364129543304443\n",
            "Step 3650, Loss 1.3755064010620117\n",
            "Step 3660, Loss 2.0107338428497314\n",
            "Step 3670, Loss 0.5180176496505737\n",
            "Step 3680, Loss 1.6088597774505615\n",
            "Step 3690, Loss 1.2977969646453857\n",
            "Step 3700, Loss 0.7955487370491028\n",
            "Step 3710, Loss 1.8643033504486084\n",
            "Step 3720, Loss 1.2085274457931519\n",
            "Step 3730, Loss 0.8687137365341187\n",
            "Step 3740, Loss 1.274861454963684\n",
            "Step 3750, Loss 1.808112621307373\n",
            "Step 3760, Loss 1.187426209449768\n",
            "Step 3770, Loss 1.4699199199676514\n",
            "Step 3780, Loss 2.3853485584259033\n",
            "Step 3790, Loss 0.9126606583595276\n",
            "Step 3800, Loss 1.7985522747039795\n",
            "Step 3810, Loss 1.5187087059020996\n",
            "Step 3820, Loss 1.799095630645752\n",
            "Step 3830, Loss 0.6513572931289673\n",
            "Step 3840, Loss 1.2831621170043945\n",
            "Step 3850, Loss 1.409569263458252\n",
            "Step 3860, Loss 1.2227658033370972\n",
            "Step 3870, Loss 1.5839042663574219\n",
            "Step 3880, Loss 1.8747273683547974\n",
            "Step 3890, Loss 1.3522781133651733\n",
            "Step 3900, Loss 1.0541443824768066\n",
            "Step 3910, Loss 1.3807872533798218\n",
            "Step 3920, Loss 1.1787104606628418\n",
            "Step 3930, Loss 1.8654406070709229\n",
            "Step 3940, Loss 1.1867554187774658\n",
            "Step 3950, Loss 1.2501702308654785\n",
            "Step 3960, Loss 1.8153810501098633\n",
            "Step 3970, Loss 1.7472772598266602\n",
            "Step 3980, Loss 1.1901639699935913\n",
            "Step 3990, Loss 1.2102854251861572\n",
            "Step 4000, Loss 1.4883315563201904\n",
            "Step 4010, Loss 0.9771825075149536\n",
            "Step 4020, Loss 0.950687050819397\n",
            "Step 4030, Loss 0.806734561920166\n",
            "Step 4040, Loss 1.1752654314041138\n",
            "Step 4050, Loss 1.765853762626648\n",
            "Step 4060, Loss 2.448883056640625\n",
            "Step 4070, Loss 0.6746941208839417\n",
            "Step 4080, Loss 2.5344834327697754\n",
            "Step 4090, Loss 1.0356134176254272\n",
            "Step 4100, Loss 1.7332079410552979\n",
            "Step 4110, Loss 1.580936312675476\n",
            "Step 4120, Loss 1.0521845817565918\n",
            "Step 4130, Loss 2.5053839683532715\n",
            "Step 4140, Loss 0.8016340732574463\n",
            "Step 4150, Loss 1.479978322982788\n",
            "Step 4160, Loss 1.5614051818847656\n",
            "Step 4170, Loss 1.6631666421890259\n",
            "Step 4180, Loss 1.180938720703125\n",
            "Step 4190, Loss 1.1995675563812256\n",
            "Step 4200, Loss 0.7756124138832092\n",
            "Step 4210, Loss 1.3293492794036865\n",
            "Step 4220, Loss 1.854924201965332\n",
            "Step 4230, Loss 2.044515371322632\n",
            "Step 4240, Loss 1.1932780742645264\n",
            "Step 4250, Loss 1.754999041557312\n",
            "Step 4260, Loss 1.710533857345581\n",
            "Step 4270, Loss 1.247983694076538\n",
            "Step 4280, Loss 1.1397042274475098\n",
            "Step 4290, Loss 1.78383207321167\n",
            "Step 4300, Loss 1.4960907697677612\n",
            "Step 4310, Loss 1.3092092275619507\n",
            "Step 4320, Loss 1.4415065050125122\n",
            "Step 4330, Loss 1.2924724817276\n",
            "Step 4340, Loss 1.3796056509017944\n",
            "Step 4350, Loss 1.1622676849365234\n",
            "Step 4360, Loss 1.7926735877990723\n",
            "Step 4370, Loss 1.1620237827301025\n",
            "Step 4380, Loss 1.777888298034668\n",
            "Step 4390, Loss 1.2977052927017212\n",
            "Step 4400, Loss 1.0394175052642822\n",
            "Step 4410, Loss 2.0651326179504395\n",
            "Step 4420, Loss 1.8895657062530518\n",
            "Step 4430, Loss 1.2011784315109253\n",
            "Step 4440, Loss 0.8789300918579102\n",
            "Step 4450, Loss 1.6232635974884033\n",
            "Step 4460, Loss 1.2670619487762451\n",
            "Step 4470, Loss 1.2065191268920898\n",
            "Step 4480, Loss 1.1548140048980713\n",
            "Step 4490, Loss 1.387498140335083\n",
            "Step 4500, Loss 1.8977298736572266\n",
            "Step 4510, Loss 1.099672794342041\n",
            "Step 4520, Loss 1.5942895412445068\n",
            "Step 4530, Loss 2.585236072540283\n",
            "Step 4540, Loss 2.3708229064941406\n",
            "Step 4550, Loss 1.767249584197998\n",
            "Step 4560, Loss 1.1471481323242188\n",
            "Step 4570, Loss 1.3734846115112305\n",
            "Step 4580, Loss 1.194542407989502\n",
            "Step 4590, Loss 1.901718258857727\n",
            "Step 4600, Loss 1.685817837715149\n",
            "Step 4610, Loss 1.5946533679962158\n",
            "Step 4620, Loss 1.2712217569351196\n",
            "Step 4630, Loss 2.2495603561401367\n",
            "Step 4640, Loss 1.5481997728347778\n",
            "Step 4650, Loss 2.2358899116516113\n",
            "Step 4660, Loss 1.7278220653533936\n",
            "Step 4670, Loss 1.453232765197754\n",
            "Step 4680, Loss 1.796207070350647\n",
            "Step 4690, Loss 1.9939699172973633\n",
            "Step 4700, Loss 1.793868064880371\n",
            "Step 4710, Loss 1.252705693244934\n",
            "Step 4720, Loss 1.355916142463684\n",
            "Step 4730, Loss 1.3969857692718506\n",
            "Step 4740, Loss 1.7243750095367432\n",
            "Step 4750, Loss 1.4188518524169922\n",
            "Step 4760, Loss 1.0661956071853638\n",
            "Step 4770, Loss 2.3826537132263184\n",
            "Step 4780, Loss 1.2830332517623901\n",
            "Step 4790, Loss 1.780322551727295\n",
            "Step 4800, Loss 1.2756285667419434\n",
            "Step 4810, Loss 1.0779556035995483\n",
            "Step 4820, Loss 1.438297986984253\n",
            "Step 4830, Loss 1.8381013870239258\n",
            "Step 4840, Loss 3.059892177581787\n",
            "Step 4850, Loss 1.9481315612792969\n",
            "Step 4860, Loss 0.8182908296585083\n",
            "Step 4870, Loss 1.7298121452331543\n",
            "Step 4880, Loss 1.6384845972061157\n",
            "Step 4890, Loss 1.4808251857757568\n",
            "Step 4900, Loss 2.0933520793914795\n",
            "Step 4910, Loss 1.6047394275665283\n",
            "Step 4920, Loss 1.2520843744277954\n",
            "Step 4930, Loss 1.4196521043777466\n",
            "Step 4940, Loss 1.7995504140853882\n",
            "Step 4950, Loss 0.9568953514099121\n",
            "Step 4960, Loss 1.9385664463043213\n",
            "Step 4970, Loss 1.5206834077835083\n",
            "Step 4980, Loss 1.9931608438491821\n",
            "Step 4990, Loss 1.697138786315918\n",
            "Step 5000, Loss 1.8596910238265991\n",
            "Step 5010, Loss 1.6092722415924072\n",
            "Step 5020, Loss 0.933738112449646\n",
            "Step 5030, Loss 1.5230339765548706\n",
            "Step 5040, Loss 0.8637837767601013\n",
            "Step 5050, Loss 1.6312882900238037\n",
            "Step 5060, Loss 0.5126875638961792\n",
            "Step 5070, Loss 1.509850025177002\n",
            "Step 5080, Loss 0.6270864605903625\n",
            "Step 5090, Loss 0.8850216865539551\n",
            "Step 5100, Loss 0.7127434015274048\n",
            "Step 5110, Loss 1.1492388248443604\n",
            "Step 5120, Loss 1.48406183719635\n",
            "Step 5130, Loss 1.2002424001693726\n",
            "Step 5140, Loss 1.645338535308838\n",
            "Step 5150, Loss 1.6656227111816406\n",
            "Step 5160, Loss 1.6150647401809692\n",
            "Step 5170, Loss 0.7303406596183777\n",
            "Step 5180, Loss 0.9219642877578735\n",
            "Step 5190, Loss 0.7623417973518372\n",
            "Step 5200, Loss 1.4109727144241333\n",
            "Step 5210, Loss 1.3984558582305908\n",
            "Step 5220, Loss 1.0741256475448608\n",
            "Step 5230, Loss 0.9985281229019165\n",
            "Step 5240, Loss 0.9392842650413513\n",
            "Step 5250, Loss 1.1372356414794922\n",
            "Step 5260, Loss 1.0357365608215332\n",
            "Step 5270, Loss 1.3174201250076294\n",
            "Step 5280, Loss 1.0503289699554443\n",
            "Step 5290, Loss 0.7096380591392517\n",
            "Step 5300, Loss 1.5399231910705566\n",
            "Step 5310, Loss 2.7915616035461426\n",
            "Step 5320, Loss 0.966003954410553\n",
            "Step 5330, Loss 1.4234471321105957\n",
            "Step 5340, Loss 0.7711517810821533\n",
            "Step 5350, Loss 1.9954417943954468\n",
            "Step 5360, Loss 0.6699647903442383\n",
            "Step 5370, Loss 1.089676856994629\n",
            "Step 5380, Loss 1.7661337852478027\n",
            "Step 5390, Loss 1.6573526859283447\n",
            "Step 5400, Loss 1.5790523290634155\n",
            "Step 5410, Loss 2.002358913421631\n",
            "Step 5420, Loss 1.977487325668335\n",
            "Step 5430, Loss 1.7751624584197998\n",
            "Step 5440, Loss 1.6286407709121704\n",
            "Step 5450, Loss 0.7023868560791016\n",
            "Step 5460, Loss 2.3968098163604736\n",
            "Step 5470, Loss 1.7511677742004395\n",
            "Step 5480, Loss 1.1588447093963623\n",
            "Step 5490, Loss 0.9051498174667358\n",
            "Step 5500, Loss 0.8021709322929382\n",
            "Step 5510, Loss 1.2257847785949707\n",
            "Step 5520, Loss 1.5317800045013428\n",
            "Step 5530, Loss 1.1383064985275269\n",
            "Step 5540, Loss 1.304415225982666\n",
            "Step 5550, Loss 0.9966511726379395\n",
            "Step 5560, Loss 1.0152192115783691\n",
            "Step 5570, Loss 1.7714967727661133\n",
            "Step 5580, Loss 1.2615699768066406\n",
            "Step 5590, Loss 1.6927307844161987\n",
            "Step 5600, Loss 1.1314152479171753\n",
            "Step 5610, Loss 1.233181118965149\n",
            "Step 5620, Loss 0.8809881210327148\n",
            "Step 5630, Loss 1.2592246532440186\n",
            "Step 5640, Loss 0.8840140104293823\n",
            "Step 5650, Loss 0.9406912326812744\n",
            "Step 5660, Loss 1.5161458253860474\n",
            "Step 5670, Loss 1.9545843601226807\n",
            "Step 5680, Loss 0.992475688457489\n",
            "Step 5690, Loss 1.4672166109085083\n",
            "Step 5700, Loss 1.6224722862243652\n",
            "Step 5710, Loss 2.5164666175842285\n",
            "Step 5720, Loss 2.1601245403289795\n",
            "Step 5730, Loss 0.980573296546936\n",
            "Step 5740, Loss 1.429396390914917\n",
            "Step 5750, Loss 1.6569840908050537\n",
            "Step 5760, Loss 1.6059362888336182\n",
            "Step 5770, Loss 1.680103063583374\n",
            "Step 5780, Loss 0.9443190693855286\n",
            "Step 5790, Loss 1.6362719535827637\n",
            "Step 5800, Loss 1.320595383644104\n",
            "Step 5810, Loss 0.7389848828315735\n",
            "Step 5820, Loss 2.3473289012908936\n",
            "Step 5830, Loss 0.9789984822273254\n",
            "Step 5840, Loss 2.2728700637817383\n",
            "Step 5850, Loss 2.051910400390625\n",
            "Step 5860, Loss 1.2326886653900146\n",
            "Step 5870, Loss 1.0178191661834717\n",
            "Step 5880, Loss 2.860441207885742\n",
            "Step 5890, Loss 1.6012492179870605\n",
            "Step 5900, Loss 2.0806283950805664\n",
            "Step 5910, Loss 1.8290255069732666\n",
            "Step 5920, Loss 2.461451292037964\n",
            "Step 5930, Loss 1.4969514608383179\n",
            "Step 5940, Loss 1.011676549911499\n",
            "Step 5950, Loss 1.1041357517242432\n",
            "Step 5960, Loss 1.4321515560150146\n",
            "Step 5970, Loss 1.4633969068527222\n",
            "Step 5980, Loss 1.674401044845581\n",
            "Step 5990, Loss 1.9147228002548218\n",
            "Step 6000, Loss 1.952134132385254\n",
            "Step 6010, Loss 1.1710381507873535\n",
            "Step 6020, Loss 1.560238242149353\n",
            "Step 6030, Loss 2.0663857460021973\n",
            "Step 6040, Loss 1.3067647218704224\n",
            "Step 6050, Loss 0.8673118948936462\n",
            "Step 6060, Loss 1.2424540519714355\n",
            "Step 6070, Loss 1.2105125188827515\n",
            "Step 6080, Loss 0.44278597831726074\n",
            "Step 6090, Loss 0.8854230642318726\n",
            "Step 6100, Loss 1.5334417819976807\n",
            "Step 6110, Loss 1.7386844158172607\n",
            "Step 6120, Loss 1.3273069858551025\n",
            "Step 6130, Loss 0.8566217422485352\n",
            "Step 6140, Loss 1.3897814750671387\n",
            "Step 6150, Loss 0.3395773768424988\n",
            "Step 6160, Loss 1.86359441280365\n",
            "Step 6170, Loss 1.3987776041030884\n",
            "Step 6180, Loss 1.267918586730957\n",
            "Step 6190, Loss 1.3061046600341797\n",
            "Step 6200, Loss 1.6593151092529297\n",
            "Step 6210, Loss 1.3377057313919067\n",
            "Step 6220, Loss 1.1447734832763672\n",
            "Step 6230, Loss 1.5772736072540283\n",
            "Step 6240, Loss 1.3816158771514893\n",
            "Step 6250, Loss 1.402248501777649\n",
            "Step 6260, Loss 1.350320816040039\n",
            "Step 6270, Loss 0.8995180726051331\n",
            "Step 6280, Loss 1.221224308013916\n",
            "Step 6290, Loss 1.282569408416748\n",
            "Step 6300, Loss 2.3540422916412354\n",
            "Step 6310, Loss 1.3032197952270508\n",
            "Step 6320, Loss 1.6542171239852905\n",
            "Step 6330, Loss 1.6359949111938477\n",
            "Step 6340, Loss 1.0500484704971313\n",
            "Step 6350, Loss 0.5405288934707642\n",
            "Step 6360, Loss 1.2586534023284912\n",
            "Step 6370, Loss 0.5942652225494385\n",
            "Step 6380, Loss 1.8147766590118408\n",
            "Step 6390, Loss 0.896084189414978\n",
            "Step 6400, Loss 1.8944848775863647\n",
            "Step 6410, Loss 1.4758741855621338\n",
            "Step 6420, Loss 1.9636822938919067\n",
            "Step 6430, Loss 1.4615626335144043\n",
            "Step 6440, Loss 1.266434669494629\n",
            "Step 6450, Loss 1.5470809936523438\n",
            "Step 6460, Loss 0.8602674603462219\n",
            "Step 6470, Loss 1.141749382019043\n",
            "Step 6480, Loss 1.6762434244155884\n",
            "Step 6490, Loss 2.4714207649230957\n",
            "Step 6500, Loss 1.3635740280151367\n",
            "Step 6510, Loss 0.8696240782737732\n",
            "Step 6520, Loss 1.2982951402664185\n",
            "Step 6530, Loss 1.5808509588241577\n",
            "Step 6540, Loss 2.3114213943481445\n",
            "Step 6550, Loss 1.1575411558151245\n",
            "Step 6560, Loss 1.7673273086547852\n",
            "Step 6570, Loss 1.1989686489105225\n",
            "Step 6580, Loss 1.8820180892944336\n",
            "Step 6590, Loss 1.9411630630493164\n",
            "Step 6600, Loss 2.4619510173797607\n",
            "Step 6610, Loss 1.176264762878418\n",
            "Step 6620, Loss 1.762157678604126\n",
            "Step 6630, Loss 1.4710298776626587\n",
            "Step 6640, Loss 2.66340970993042\n",
            "Step 6650, Loss 0.9307714700698853\n",
            "Step 6660, Loss 1.8794742822647095\n",
            "Step 6670, Loss 1.614227056503296\n",
            "Step 6680, Loss 1.8196814060211182\n",
            "Step 6690, Loss 1.409452199935913\n",
            "Step 6700, Loss 1.3500607013702393\n",
            "Step 6710, Loss 2.7881107330322266\n",
            "Step 6720, Loss 1.3160877227783203\n",
            "Step 6730, Loss 0.8650798797607422\n",
            "Step 6740, Loss 0.8254203200340271\n",
            "Step 6750, Loss 1.1140074729919434\n",
            "Step 6760, Loss 1.1626248359680176\n",
            "Step 6770, Loss 1.719629168510437\n",
            "Step 6780, Loss 1.9119631052017212\n",
            "Step 6790, Loss 1.7495222091674805\n",
            "Step 6800, Loss 2.291253089904785\n",
            "Step 6810, Loss 2.072061538696289\n",
            "Step 6820, Loss 2.0258548259735107\n",
            "Step 6830, Loss 0.4326278865337372\n",
            "Step 6840, Loss 1.368373155593872\n",
            "Step 6850, Loss 1.4160573482513428\n",
            "Step 6860, Loss 0.5468615293502808\n",
            "Step 6870, Loss 2.1454880237579346\n",
            "Step 6880, Loss 2.3128409385681152\n",
            "Step 6890, Loss 1.7575263977050781\n",
            "Step 6900, Loss 0.848579466342926\n",
            "Step 6910, Loss 1.2474504709243774\n",
            "Step 6920, Loss 1.7587065696716309\n",
            "Step 6930, Loss 1.9746010303497314\n",
            "Step 6940, Loss 1.9071857929229736\n",
            "Step 6950, Loss 0.8435313105583191\n",
            "Step 6960, Loss 1.5963788032531738\n",
            "Step 6970, Loss 1.3970998525619507\n",
            "Step 6980, Loss 1.6056573390960693\n",
            "Step 6990, Loss 1.38411545753479\n",
            "Step 7000, Loss 1.582446813583374\n",
            "Step 7010, Loss 1.046877145767212\n",
            "Step 7020, Loss 1.4147303104400635\n",
            "Step 7030, Loss 1.155137062072754\n",
            "Step 7040, Loss 1.1950207948684692\n",
            "Step 7050, Loss 1.5685707330703735\n",
            "Step 7060, Loss 1.360368013381958\n",
            "Step 7070, Loss 1.7195539474487305\n",
            "Step 7080, Loss 1.8735493421554565\n",
            "Step 7090, Loss 0.6593165993690491\n",
            "Step 7100, Loss 0.8379582166671753\n",
            "Step 7110, Loss 1.6409187316894531\n",
            "Step 7120, Loss 1.6092760562896729\n",
            "Step 7130, Loss 1.3789010047912598\n",
            "Step 7140, Loss 1.4484487771987915\n",
            "Step 7150, Loss 1.9250555038452148\n",
            "Step 7160, Loss 1.5792535543441772\n",
            "Step 7170, Loss 1.166701316833496\n",
            "Step 7180, Loss 1.1412302255630493\n",
            "Step 7190, Loss 0.7092592716217041\n",
            "Step 7200, Loss 1.5445590019226074\n",
            "Step 7210, Loss 1.318657398223877\n",
            "Step 7220, Loss 0.8722797632217407\n",
            "Step 7230, Loss 1.4492731094360352\n",
            "Step 7240, Loss 1.8206264972686768\n",
            "Step 7250, Loss 1.2588233947753906\n",
            "Step 7260, Loss 1.1209520101547241\n",
            "Step 7270, Loss 1.0903899669647217\n",
            "Step 7280, Loss 1.7952733039855957\n",
            "Step 7290, Loss 2.1588218212127686\n",
            "Step 7300, Loss 1.62701416015625\n",
            "Step 7310, Loss 1.0647627115249634\n",
            "Step 7320, Loss 1.7630301713943481\n",
            "Step 7330, Loss 1.8353509902954102\n",
            "Step 7340, Loss 1.7354612350463867\n",
            "Step 7350, Loss 1.0274560451507568\n",
            "Step 7360, Loss 0.716902494430542\n",
            "Step 7370, Loss 1.981278419494629\n",
            "Step 7380, Loss 2.0678324699401855\n",
            "Step 7390, Loss 1.6218611001968384\n",
            "Step 7400, Loss 1.60837984085083\n",
            "Step 7410, Loss 1.3479112386703491\n",
            "Step 7420, Loss 1.3245869874954224\n",
            "Step 7430, Loss 1.8253300189971924\n",
            "Step 7440, Loss 1.4144246578216553\n",
            "Step 7450, Loss 1.357590675354004\n",
            "Step 7460, Loss 0.7539817690849304\n",
            "Step 7470, Loss 1.303625226020813\n",
            "Step 7480, Loss 0.6243234872817993\n",
            "Step 7490, Loss 1.8336375951766968\n",
            "Step 7500, Loss 1.4708298444747925\n",
            "Step 7510, Loss 2.162184715270996\n",
            "Step 7520, Loss 2.082650661468506\n",
            "Step 7530, Loss 2.1070003509521484\n",
            "Step 7540, Loss 1.7237334251403809\n",
            "Step 7550, Loss 1.2388145923614502\n",
            "Step 7560, Loss 1.6322803497314453\n",
            "Step 7570, Loss 2.507396697998047\n",
            "Step 7580, Loss 0.5712026357650757\n",
            "Step 7590, Loss 1.0795135498046875\n",
            "Step 7600, Loss 1.208954095840454\n",
            "Step 7610, Loss 1.8320379257202148\n",
            "Step 7620, Loss 1.416258454322815\n",
            "Step 7630, Loss 1.8848176002502441\n",
            "Step 7640, Loss 1.3328473567962646\n",
            "Step 7650, Loss 1.4295282363891602\n",
            "Step 7660, Loss 0.5852189064025879\n",
            "Step 7670, Loss 1.0803158283233643\n",
            "Step 7680, Loss 1.728633999824524\n",
            "Step 7690, Loss 1.2782727479934692\n",
            "Step 7700, Loss 0.9987872838973999\n",
            "Step 7710, Loss 1.1847671270370483\n",
            "Step 7720, Loss 1.1321570873260498\n",
            "Step 7730, Loss 1.0729913711547852\n",
            "Step 7740, Loss 1.3380260467529297\n",
            "Step 7750, Loss 0.7518855333328247\n",
            "Step 7760, Loss 1.7957797050476074\n",
            "Step 7770, Loss 0.7035781741142273\n",
            "Step 7780, Loss 1.5165982246398926\n",
            "Step 7790, Loss 2.64192795753479\n",
            "Step 7800, Loss 1.357877254486084\n",
            "Step 7810, Loss 0.5682264566421509\n",
            "Step 7820, Loss 1.265307903289795\n",
            "Step 7830, Loss 1.2044174671173096\n",
            "Step 7840, Loss 1.0026429891586304\n",
            "Step 7850, Loss 1.5637621879577637\n",
            "Step 7860, Loss 2.3296306133270264\n",
            "Step 7870, Loss 0.4325716197490692\n",
            "Step 7880, Loss 1.0013766288757324\n",
            "Step 7890, Loss 0.9040555357933044\n",
            "Step 7900, Loss 2.542553186416626\n",
            "Step 7910, Loss 2.1454954147338867\n",
            "Step 7920, Loss 1.512688159942627\n",
            "Step 7930, Loss 1.4949159622192383\n",
            "Step 7940, Loss 1.3768422603607178\n",
            "Step 7950, Loss 2.1719484329223633\n",
            "Step 7960, Loss 1.2605817317962646\n",
            "Step 7970, Loss 1.5056456327438354\n",
            "Step 7980, Loss 1.2662612199783325\n",
            "Step 7990, Loss 0.9819457530975342\n",
            "Step 8000, Loss 1.4190982580184937\n",
            "Step 8010, Loss 0.9473586082458496\n",
            "Step 8020, Loss 2.2510881423950195\n",
            "Step 8030, Loss 1.937658667564392\n",
            "Step 8040, Loss 0.6741195917129517\n",
            "Step 8050, Loss 0.933938205242157\n",
            "Step 8060, Loss 1.8152562379837036\n",
            "Step 8070, Loss 1.670052409172058\n",
            "Step 8080, Loss 0.41789475083351135\n",
            "Step 8090, Loss 2.8705546855926514\n",
            "Step 8100, Loss 1.1594027280807495\n",
            "Step 8110, Loss 0.8201916217803955\n",
            "Step 8120, Loss 2.5902700424194336\n",
            "Step 8130, Loss 1.7674418687820435\n",
            "Step 8140, Loss 1.0229992866516113\n",
            "Step 8150, Loss 1.4490643739700317\n",
            "Step 8160, Loss 2.0651276111602783\n",
            "Step 8170, Loss 1.3192840814590454\n",
            "Step 8180, Loss 1.456324577331543\n",
            "Step 8190, Loss 2.0686216354370117\n",
            "Step 8200, Loss 1.9823609590530396\n",
            "Step 8210, Loss 1.3659420013427734\n",
            "Step 8220, Loss 1.260610580444336\n",
            "Step 8230, Loss 1.310828685760498\n",
            "Step 8240, Loss 1.1729892492294312\n",
            "Step 8250, Loss 0.722730278968811\n",
            "Step 8260, Loss 1.492067813873291\n",
            "Step 8270, Loss 1.8366326093673706\n",
            "Step 8280, Loss 2.0643014907836914\n",
            "Step 8290, Loss 1.5827507972717285\n",
            "Step 8300, Loss 1.434855580329895\n",
            "Step 8310, Loss 1.6097443103790283\n",
            "Step 8320, Loss 2.215762138366699\n",
            "Step 8330, Loss 0.7790365219116211\n",
            "Step 8340, Loss 2.077274799346924\n",
            "Step 8350, Loss 1.7958875894546509\n",
            "Step 8360, Loss 1.149519443511963\n",
            "Step 8370, Loss 1.42428457736969\n",
            "Step 8380, Loss 1.4536333084106445\n",
            "Step 8390, Loss 1.4420124292373657\n",
            "Step 8400, Loss 2.204270362854004\n",
            "Step 8410, Loss 1.4454506635665894\n",
            "Step 8420, Loss 1.6011085510253906\n",
            "Step 8430, Loss 0.38802310824394226\n",
            "Step 8440, Loss 2.6565158367156982\n",
            "Step 8450, Loss 1.655550241470337\n",
            "Step 8460, Loss 0.6318275928497314\n",
            "Step 8470, Loss 1.014326810836792\n",
            "Step 8480, Loss 0.8073647022247314\n",
            "Step 8490, Loss 1.3699913024902344\n",
            "Step 8500, Loss 0.8887449502944946\n",
            "Step 8510, Loss 1.9945061206817627\n",
            "Step 8520, Loss 1.717313289642334\n",
            "Step 8530, Loss 1.7560724020004272\n",
            "Step 8540, Loss 1.1402827501296997\n",
            "Step 8550, Loss 1.4140762090682983\n",
            "Step 8560, Loss 2.189804792404175\n",
            "Step 8570, Loss 1.7548868656158447\n",
            "Step 8580, Loss 0.9092424511909485\n",
            "Step 8590, Loss 1.4658645391464233\n",
            "Step 8600, Loss 0.7484402060508728\n",
            "Step 8610, Loss 1.6598401069641113\n",
            "Step 8620, Loss 1.1773790121078491\n",
            "Step 8630, Loss 1.0780576467514038\n",
            "Step 8640, Loss 1.3749910593032837\n",
            "Step 8650, Loss 1.5273579359054565\n",
            "Step 8660, Loss 2.4646716117858887\n",
            "Step 8670, Loss 1.7368087768554688\n",
            "Step 8680, Loss 1.2803900241851807\n",
            "Step 8690, Loss 2.2267332077026367\n",
            "Step 8700, Loss 1.515998125076294\n",
            "Step 8710, Loss 1.1849128007888794\n",
            "Step 8720, Loss 1.1976383924484253\n",
            "Step 8730, Loss 1.4667342901229858\n",
            "Step 8740, Loss 1.7399672269821167\n",
            "Step 8750, Loss 0.7886145710945129\n",
            "Step 8760, Loss 2.064819574356079\n",
            "Step 8770, Loss 0.9228777885437012\n",
            "Step 8780, Loss 1.1546794176101685\n",
            "Step 8790, Loss 0.6750818490982056\n",
            "Step 8800, Loss 2.186142921447754\n",
            "Step 8810, Loss 1.5147769451141357\n",
            "Step 8820, Loss 1.8961338996887207\n",
            "Step 8830, Loss 1.2120566368103027\n",
            "Step 8840, Loss 0.9676909446716309\n",
            "Step 8850, Loss 1.810329556465149\n",
            "Step 8860, Loss 1.6048407554626465\n",
            "Step 8870, Loss 1.398015022277832\n",
            "Step 8880, Loss 1.558760166168213\n",
            "Step 8890, Loss 1.7875361442565918\n",
            "Step 8900, Loss 2.4800052642822266\n",
            "Step 8910, Loss 2.2369794845581055\n",
            "Step 8920, Loss 1.1940093040466309\n",
            "Step 8930, Loss 1.8267467021942139\n",
            "Step 8940, Loss 1.385557770729065\n",
            "Step 8950, Loss 2.17307710647583\n",
            "Step 8960, Loss 0.7615878582000732\n",
            "Step 8970, Loss 0.4774436950683594\n",
            "Step 8980, Loss 0.7755613327026367\n",
            "Step 8990, Loss 0.9954882860183716\n",
            "Step 9000, Loss 0.9630854725837708\n",
            "Step 9010, Loss 1.6051199436187744\n",
            "Step 9020, Loss 1.4208239316940308\n",
            "Step 9030, Loss 1.9334871768951416\n",
            "Step 9040, Loss 1.729819416999817\n",
            "Step 9050, Loss 1.762341022491455\n",
            "Step 9060, Loss 1.8567125797271729\n",
            "Step 9070, Loss 2.1039412021636963\n",
            "Step 9080, Loss 2.3674228191375732\n",
            "Step 9090, Loss 0.7001962661743164\n",
            "Step 9100, Loss 1.3904094696044922\n",
            "Step 9110, Loss 0.8814528584480286\n",
            "Step 9120, Loss 1.1195992231369019\n",
            "Step 9130, Loss 1.0837364196777344\n",
            "Step 9140, Loss 1.1743433475494385\n",
            "Step 9150, Loss 1.5354418754577637\n",
            "Step 9160, Loss 1.056777834892273\n",
            "Step 9170, Loss 2.383303642272949\n",
            "Step 9180, Loss 2.007755994796753\n",
            "Step 9190, Loss 1.3858857154846191\n",
            "Step 9200, Loss 2.6785621643066406\n",
            "Step 9210, Loss 0.8731961846351624\n",
            "Step 9220, Loss 2.621748924255371\n",
            "Step 9230, Loss 1.2014777660369873\n",
            "Step 9240, Loss 1.5844268798828125\n",
            "Step 9250, Loss 0.8800898194313049\n",
            "Step 9260, Loss 1.3155264854431152\n",
            "Step 9270, Loss 1.8997113704681396\n",
            "Step 9280, Loss 2.6527836322784424\n",
            "Step 9290, Loss 1.7075800895690918\n",
            "Step 9300, Loss 1.405292272567749\n",
            "Step 9310, Loss 1.6284891366958618\n",
            "Step 9320, Loss 1.0792973041534424\n",
            "Step 9330, Loss 1.2488046884536743\n",
            "Step 9340, Loss 1.0774009227752686\n",
            "Step 9350, Loss 1.3989958763122559\n",
            "Step 9360, Loss 1.043794870376587\n",
            "Step 9370, Loss 1.1948381662368774\n",
            "Step 9380, Loss 1.0475802421569824\n",
            "Step 9390, Loss 1.5731239318847656\n",
            "Step 9400, Loss 1.5806883573532104\n",
            "Step 9410, Loss 0.917022705078125\n",
            "Step 9420, Loss 1.3745436668395996\n",
            "Step 9430, Loss 1.0536179542541504\n",
            "Step 9440, Loss 1.8405742645263672\n",
            "Step 9450, Loss 1.0856566429138184\n",
            "Step 9460, Loss 2.109598398208618\n",
            "Step 9470, Loss 0.5671232342720032\n",
            "Step 9480, Loss 2.050887107849121\n",
            "Step 9490, Loss 1.3834919929504395\n",
            "Step 9500, Loss 0.5429275631904602\n",
            "Step 9510, Loss 1.4358205795288086\n",
            "Step 9520, Loss 1.271065354347229\n",
            "Step 9530, Loss 1.6028980016708374\n",
            "Step 9540, Loss 1.8235540390014648\n",
            "Step 9550, Loss 1.0289664268493652\n",
            "Step 9560, Loss 1.2804478406906128\n",
            "Step 9570, Loss 1.1393661499023438\n",
            "Step 9580, Loss 1.5036333799362183\n",
            "Step 9590, Loss 0.998964250087738\n",
            "Step 9600, Loss 1.3176534175872803\n",
            "Step 9610, Loss 1.2648922204971313\n",
            "Step 9620, Loss 2.0060534477233887\n",
            "Step 9630, Loss 1.5745575428009033\n",
            "Step 9640, Loss 1.8020175695419312\n",
            "Step 9650, Loss 1.2296665906906128\n",
            "Step 9660, Loss 1.3952105045318604\n",
            "Step 9670, Loss 0.7494255304336548\n",
            "Step 9680, Loss 1.4960358142852783\n",
            "Step 9690, Loss 0.988217294216156\n",
            "Step 9700, Loss 1.7760579586029053\n",
            "Step 9710, Loss 2.4610307216644287\n",
            "Step 9720, Loss 1.0408803224563599\n",
            "Step 9730, Loss 1.6791706085205078\n",
            "Step 9740, Loss 1.3295059204101562\n",
            "Step 9750, Loss 1.025625228881836\n",
            "Step 9760, Loss 0.868853747844696\n",
            "Step 9770, Loss 0.5831100344657898\n",
            "Step 9780, Loss 1.363006591796875\n",
            "Step 9790, Loss 1.540550947189331\n",
            "Step 9800, Loss 1.6519348621368408\n",
            "Step 9810, Loss 1.1065969467163086\n",
            "Step 9820, Loss 2.755725860595703\n",
            "Step 9830, Loss 1.9942288398742676\n",
            "Step 9840, Loss 1.082523226737976\n",
            "Step 9850, Loss 1.7664036750793457\n",
            "Step 9860, Loss 1.3020890951156616\n",
            "Step 9870, Loss 1.570563554763794\n",
            "Step 9880, Loss 2.2296483516693115\n",
            "Step 9890, Loss 1.3697190284729004\n",
            "Step 9900, Loss 1.9537618160247803\n",
            "Step 9910, Loss 1.6978237628936768\n",
            "Step 9920, Loss 1.6111226081848145\n",
            "Step 9930, Loss 1.5296175479888916\n",
            "Step 9940, Loss 1.4219624996185303\n",
            "Step 9950, Loss 1.1298633813858032\n",
            "Step 9960, Loss 1.2617079019546509\n",
            "Step 9970, Loss 1.4296984672546387\n",
            "Step 9980, Loss 1.0510984659194946\n",
            "Step 9990, Loss 1.7142527103424072\n",
            "Step 10000, Loss 0.6929690837860107\n",
            "Step 10010, Loss 1.4398553371429443\n",
            "Step 10020, Loss 1.0431220531463623\n",
            "Step 10030, Loss 0.8939810991287231\n",
            "Step 10040, Loss 1.2259999513626099\n",
            "Step 10050, Loss 1.2837116718292236\n",
            "Step 10060, Loss 1.4936633110046387\n",
            "Step 10070, Loss 1.4226665496826172\n",
            "Step 10080, Loss 0.8397841453552246\n",
            "Step 10090, Loss 1.4781267642974854\n",
            "Step 10100, Loss 0.41005098819732666\n",
            "Step 10110, Loss 1.119350552558899\n",
            "Step 10120, Loss 1.2848001718521118\n",
            "Step 10130, Loss 0.6311606764793396\n",
            "Step 10140, Loss 0.8919061422348022\n",
            "Step 10150, Loss 1.4634881019592285\n",
            "Step 10160, Loss 1.7062164545059204\n",
            "Step 10170, Loss 1.399770975112915\n",
            "Step 10180, Loss 2.113894462585449\n",
            "Step 10190, Loss 0.7773963809013367\n",
            "Step 10200, Loss 0.8930233120918274\n",
            "Step 10210, Loss 1.6358855962753296\n",
            "Step 10220, Loss 0.6323825716972351\n",
            "Step 10230, Loss 1.105021595954895\n",
            "Step 10240, Loss 0.6877170205116272\n",
            "Step 10250, Loss 2.1225950717926025\n",
            "Step 10260, Loss 1.3185378313064575\n",
            "Step 10270, Loss 1.9153859615325928\n",
            "Step 10280, Loss 1.048854112625122\n",
            "Step 10290, Loss 1.0198746919631958\n",
            "Step 10300, Loss 1.787105679512024\n",
            "Step 10310, Loss 0.9544291496276855\n",
            "Step 10320, Loss 2.205493688583374\n",
            "Step 10330, Loss 1.2657968997955322\n",
            "Step 10340, Loss 1.7346060276031494\n",
            "Step 10350, Loss 1.6882495880126953\n",
            "Step 10360, Loss 1.742929458618164\n",
            "Step 10370, Loss 1.180328130722046\n",
            "Step 10380, Loss 1.4307256937026978\n",
            "Step 10390, Loss 1.5769927501678467\n",
            "Step 10400, Loss 0.6367231607437134\n",
            "Step 10410, Loss 0.9234482645988464\n",
            "Step 10420, Loss 1.692448377609253\n",
            "Step 10430, Loss 0.7062604427337646\n",
            "Step 10440, Loss 1.533470630645752\n",
            "Step 10450, Loss 1.8083081245422363\n",
            "Step 10460, Loss 1.571690559387207\n",
            "Step 10470, Loss 1.2243062257766724\n",
            "Step 10480, Loss 1.1497520208358765\n",
            "Step 10490, Loss 0.91168612241745\n",
            "Step 10500, Loss 1.8278453350067139\n",
            "Step 10510, Loss 2.4261887073516846\n",
            "Step 10520, Loss 1.3164854049682617\n",
            "Step 10530, Loss 1.6651068925857544\n",
            "Step 10540, Loss 2.1363461017608643\n",
            "Step 10550, Loss 1.180701732635498\n",
            "Step 10560, Loss 2.16092586517334\n",
            "Step 10570, Loss 1.5610651969909668\n",
            "Step 10580, Loss 1.6618783473968506\n",
            "Step 10590, Loss 1.0031991004943848\n",
            "Step 10600, Loss 1.2070221900939941\n",
            "Step 10610, Loss 1.510798692703247\n",
            "Step 10620, Loss 1.5844295024871826\n",
            "Step 10630, Loss 1.524733304977417\n",
            "Step 10640, Loss 1.5483194589614868\n",
            "Step 10650, Loss 1.478285551071167\n",
            "Step 10660, Loss 2.1542789936065674\n",
            "Step 10670, Loss 1.1955939531326294\n",
            "Step 10680, Loss 1.9535677433013916\n",
            "Step 10690, Loss 1.4449623823165894\n",
            "Step 10700, Loss 1.1999759674072266\n",
            "Step 10710, Loss 0.9374293684959412\n",
            "Step 10720, Loss 1.6332813501358032\n",
            "Step 10730, Loss 1.1734867095947266\n",
            "Step 10740, Loss 2.1590805053710938\n",
            "Step 10750, Loss 2.2104673385620117\n",
            "Step 10760, Loss 0.9914083480834961\n",
            "Step 10770, Loss 1.0390756130218506\n",
            "Step 10780, Loss 1.676070213317871\n",
            "Step 10790, Loss 1.7959774732589722\n",
            "Step 10800, Loss 2.574449062347412\n",
            "Step 10810, Loss 1.2311559915542603\n",
            "Step 10820, Loss 1.8400987386703491\n",
            "Step 10830, Loss 1.2771170139312744\n",
            "Step 10840, Loss 2.0346202850341797\n",
            "Step 10850, Loss 1.8545958995819092\n",
            "Step 10860, Loss 1.0932077169418335\n",
            "Step 10870, Loss 1.5745341777801514\n",
            "Step 10880, Loss 1.2059261798858643\n",
            "Step 10890, Loss 1.1786950826644897\n",
            "Step 10900, Loss 1.890030026435852\n",
            "Step 10910, Loss 0.684661865234375\n",
            "Step 10920, Loss 1.7869036197662354\n",
            "Step 10930, Loss 1.9407553672790527\n",
            "Step 10940, Loss 1.4975324869155884\n",
            "Step 10950, Loss 1.7565279006958008\n",
            "Step 10960, Loss 1.8361420631408691\n",
            "Step 10970, Loss 2.039862632751465\n",
            "Step 10980, Loss 1.6753277778625488\n",
            "Step 10990, Loss 1.3832206726074219\n",
            "Step 11000, Loss 1.0821099281311035\n",
            "Step 11010, Loss 1.433359146118164\n",
            "Step 11020, Loss 0.7796338200569153\n",
            "Step 11030, Loss 0.959640622138977\n",
            "Step 11040, Loss 1.6902835369110107\n",
            "Step 11050, Loss 1.6227998733520508\n",
            "Step 11060, Loss 1.6376383304595947\n",
            "Step 11070, Loss 2.0301125049591064\n",
            "Step 11080, Loss 1.021745204925537\n",
            "Step 11090, Loss 1.3122246265411377\n",
            "Step 11100, Loss 1.0648077726364136\n",
            "Step 11110, Loss 1.40951406955719\n",
            "Step 11120, Loss 1.3652763366699219\n",
            "Step 11130, Loss 1.5315171480178833\n",
            "Step 11140, Loss 1.5235514640808105\n",
            "Step 11150, Loss 0.6298796534538269\n",
            "Step 11160, Loss 1.4653282165527344\n",
            "Step 11170, Loss 1.4138948917388916\n",
            "Step 11180, Loss 1.206047534942627\n",
            "Step 11190, Loss 2.59674334526062\n",
            "Step 11200, Loss 2.337923526763916\n",
            "Step 11210, Loss 1.6039751768112183\n",
            "Step 11220, Loss 0.9973752498626709\n",
            "Step 11230, Loss 1.6423875093460083\n",
            "Step 11240, Loss 1.2788991928100586\n",
            "Step 11250, Loss 1.9169647693634033\n",
            "Step 11260, Loss 0.9220159649848938\n",
            "Step 11270, Loss 1.6280404329299927\n",
            "Step 11280, Loss 1.5313966274261475\n",
            "Step 11290, Loss 0.3324781358242035\n",
            "Step 11300, Loss 0.9120758175849915\n",
            "Step 11310, Loss 1.4923791885375977\n",
            "Step 11320, Loss 1.1742823123931885\n",
            "Step 11330, Loss 2.431570529937744\n",
            "Step 11340, Loss 1.2494313716888428\n",
            "Step 11350, Loss 2.699976921081543\n",
            "Step 11360, Loss 1.2193458080291748\n",
            "Step 11370, Loss 1.9551743268966675\n",
            "Step 11380, Loss 2.0892081260681152\n",
            "Step 11390, Loss 2.1897637844085693\n",
            "Step 11400, Loss 1.0694797039031982\n",
            "Step 11410, Loss 2.185401439666748\n",
            "Step 11420, Loss 1.2160605192184448\n",
            "Step 11430, Loss 1.7904245853424072\n",
            "Step 11440, Loss 1.614546775817871\n",
            "Step 11450, Loss 1.8090927600860596\n",
            "Step 11460, Loss 1.5313259363174438\n",
            "Step 11470, Loss 3.4089527130126953\n",
            "Step 11480, Loss 0.6479401588439941\n",
            "Step 11490, Loss 1.0908095836639404\n",
            "Step 11500, Loss 1.000406265258789\n",
            "Step 11510, Loss 2.4264020919799805\n",
            "Step 11520, Loss 1.3160604238510132\n",
            "Step 11530, Loss 1.1339409351348877\n",
            "Step 11540, Loss 1.665158987045288\n",
            "Step 11550, Loss 1.6179795265197754\n",
            "Step 11560, Loss 2.1195335388183594\n",
            "Step 11570, Loss 0.8915037512779236\n",
            "Step 11580, Loss 0.9921257495880127\n",
            "Step 11590, Loss 0.5750430226325989\n",
            "Step 11600, Loss 2.0764431953430176\n",
            "Step 11610, Loss 1.728337049484253\n",
            "Step 11620, Loss 1.6232471466064453\n",
            "Step 11630, Loss 1.3535362482070923\n",
            "Step 11640, Loss 1.5987690687179565\n",
            "Step 11650, Loss 1.4796142578125\n",
            "Step 11660, Loss 1.6803134679794312\n",
            "Step 11670, Loss 1.5186576843261719\n",
            "Step 11680, Loss 1.8808796405792236\n",
            "Step 11690, Loss 0.6482696533203125\n",
            "Step 11700, Loss 1.228517770767212\n",
            "Step 11710, Loss 0.869637131690979\n",
            "Step 11720, Loss 1.185215950012207\n",
            "Step 11730, Loss 1.7399400472640991\n",
            "Step 11740, Loss 1.7800934314727783\n",
            "Step 11750, Loss 0.8403574824333191\n",
            "Step 11760, Loss 1.082880973815918\n",
            "Step 11770, Loss 2.1436867713928223\n",
            "Step 11780, Loss 1.3523154258728027\n",
            "Step 11790, Loss 1.0744065046310425\n",
            "Step 11800, Loss 0.7639564871788025\n",
            "Step 11810, Loss 1.0858300924301147\n",
            "Step 11820, Loss 0.9907321929931641\n",
            "Step 11830, Loss 1.4668047428131104\n",
            "Step 11840, Loss 0.7328997850418091\n",
            "Step 11850, Loss 1.1541800498962402\n",
            "Step 11860, Loss 1.9383188486099243\n",
            "Step 11870, Loss 1.6648238897323608\n",
            "Step 11880, Loss 1.2974644899368286\n",
            "Step 11890, Loss 1.172736644744873\n",
            "Step 11900, Loss 2.8755455017089844\n",
            "Step 11910, Loss 0.9107439517974854\n",
            "Step 11920, Loss 1.6874210834503174\n",
            "Step 11930, Loss 1.7228888273239136\n",
            "Step 11940, Loss 1.7848354578018188\n",
            "Step 11950, Loss 1.946966528892517\n",
            "Step 11960, Loss 1.5583198070526123\n",
            "Step 11970, Loss 1.1249260902404785\n",
            "Step 11980, Loss 1.511559247970581\n",
            "Step 11990, Loss 1.574405550956726\n",
            "Step 12000, Loss 1.4395256042480469\n",
            "Step 12010, Loss 2.0875539779663086\n",
            "Step 12020, Loss 0.6323906183242798\n",
            "Step 12030, Loss 0.8102335333824158\n",
            "Step 12040, Loss 2.2619950771331787\n",
            "Step 12050, Loss 1.7704952955245972\n",
            "Step 12060, Loss 1.4182157516479492\n",
            "Step 12070, Loss 0.9376491904258728\n",
            "Step 12080, Loss 1.9088891744613647\n",
            "Step 12090, Loss 1.8801836967468262\n",
            "Step 12100, Loss 1.866352915763855\n",
            "Step 12110, Loss 1.4239163398742676\n",
            "Step 12120, Loss 1.5596033334732056\n",
            "Step 12130, Loss 1.839638590812683\n",
            "Step 12140, Loss 1.5305142402648926\n",
            "Step 12150, Loss 1.1517889499664307\n",
            "Step 12160, Loss 1.020313024520874\n",
            "Step 12170, Loss 1.8223778009414673\n",
            "Step 12180, Loss 2.12833833694458\n",
            "Step 12190, Loss 1.4145441055297852\n",
            "Step 12200, Loss 1.0760337114334106\n",
            "Step 12210, Loss 1.0580880641937256\n",
            "Step 12220, Loss 1.374206781387329\n",
            "Step 12230, Loss 1.5505019426345825\n",
            "Step 12240, Loss 0.8954709768295288\n",
            "Step 12250, Loss 1.4179761409759521\n",
            "Step 12260, Loss 2.016361713409424\n",
            "Step 12270, Loss 1.2453187704086304\n",
            "Step 12280, Loss 2.002854347229004\n",
            "Step 12290, Loss 1.7038354873657227\n",
            "Step 12300, Loss 1.8730876445770264\n",
            "Step 12310, Loss 0.7859107851982117\n",
            "Step 12320, Loss 1.4283530712127686\n",
            "Step 12330, Loss 1.9177676439285278\n",
            "Step 12340, Loss 1.1068114042282104\n",
            "Step 12350, Loss 0.8797481656074524\n",
            "Step 12360, Loss 1.4799836874008179\n",
            "Step 12370, Loss 0.8201847076416016\n",
            "Step 12380, Loss 2.179447650909424\n",
            "Step 12390, Loss 1.5806965827941895\n",
            "Step 12400, Loss 1.6268491744995117\n",
            "Step 12410, Loss 1.137529969215393\n",
            "Step 12420, Loss 0.8718605637550354\n",
            "Step 12430, Loss 1.8760910034179688\n",
            "Step 12440, Loss 1.8148562908172607\n",
            "Step 12450, Loss 0.9918581247329712\n",
            "Step 12460, Loss 1.2184641361236572\n",
            "Step 12470, Loss 1.109838604927063\n",
            "Step 12480, Loss 2.6873981952667236\n",
            "Step 12490, Loss 1.6568111181259155\n",
            "Step 12500, Loss 1.6346895694732666\n",
            "Step 12510, Loss 0.5024043321609497\n",
            "Step 12520, Loss 0.6594066619873047\n",
            "Step 12530, Loss 1.5384567975997925\n",
            "Step 12540, Loss 2.041538715362549\n",
            "Step 12550, Loss 1.410826563835144\n",
            "Step 12560, Loss 2.2123095989227295\n",
            "Step 12570, Loss 1.4300787448883057\n",
            "Step 12580, Loss 1.67795991897583\n",
            "Step 12590, Loss 0.9716824889183044\n",
            "Step 12600, Loss 1.1972150802612305\n",
            "Step 12610, Loss 1.3403916358947754\n",
            "Step 12620, Loss 0.848857045173645\n",
            "Step 12630, Loss 1.574320912361145\n",
            "Step 12640, Loss 2.3967831134796143\n",
            "Step 12650, Loss 2.0585763454437256\n",
            "Step 12660, Loss 2.0549163818359375\n",
            "Step 12670, Loss 1.5310800075531006\n",
            "Step 12680, Loss 2.1420114040374756\n",
            "Step 12690, Loss 0.9229934215545654\n",
            "Step 12700, Loss 1.7754077911376953\n",
            "Step 12710, Loss 1.9655606746673584\n",
            "Step 12720, Loss 0.7717496752738953\n",
            "Step 12730, Loss 2.288626194000244\n",
            "Step 12740, Loss 1.7203024625778198\n",
            "Step 12750, Loss 1.660991907119751\n",
            "Step 12760, Loss 1.6544572114944458\n",
            "Step 12770, Loss 1.7683701515197754\n",
            "Step 12780, Loss 0.8342369794845581\n",
            "Step 12790, Loss 1.8924336433410645\n",
            "Step 12800, Loss 2.827023506164551\n",
            "Step 12810, Loss 1.2479660511016846\n",
            "Step 12820, Loss 2.436037302017212\n",
            "Step 12830, Loss 2.090541362762451\n",
            "Step 12840, Loss 1.7428781986236572\n",
            "Step 12850, Loss 1.8412506580352783\n",
            "Step 12860, Loss 2.417611598968506\n",
            "Step 12870, Loss 0.6891372799873352\n",
            "Step 12880, Loss 1.6694653034210205\n",
            "Step 12890, Loss 1.1054368019104004\n",
            "Step 12900, Loss 0.8525141477584839\n",
            "Step 12910, Loss 2.063899278640747\n",
            "Step 12920, Loss 0.8746243119239807\n",
            "Step 12930, Loss 1.2496328353881836\n",
            "Step 12940, Loss 1.5450631380081177\n",
            "Step 12950, Loss 1.1668317317962646\n",
            "Step 12960, Loss 1.2915034294128418\n",
            "Step 12970, Loss 1.1429097652435303\n",
            "Step 12980, Loss 1.857123851776123\n",
            "Step 12990, Loss 0.6968988180160522\n",
            "Step 13000, Loss 1.685130000114441\n",
            "Step 13010, Loss 1.8822684288024902\n",
            "Step 13020, Loss 1.6167001724243164\n",
            "Step 13030, Loss 0.7904171347618103\n",
            "Step 13040, Loss 0.514576256275177\n",
            "Step 13050, Loss 0.8512351512908936\n",
            "Step 13060, Loss 1.1349310874938965\n",
            "Step 13070, Loss 1.5865716934204102\n",
            "Step 13080, Loss 1.9503798484802246\n",
            "Step 13090, Loss 2.1026811599731445\n",
            "Step 13100, Loss 1.317995309829712\n",
            "Step 13110, Loss 0.7328577637672424\n",
            "Step 13120, Loss 0.8112614750862122\n",
            "Step 13130, Loss 2.276162624359131\n",
            "Step 13140, Loss 1.4213992357254028\n",
            "Step 13150, Loss 1.7105791568756104\n",
            "Step 13160, Loss 1.3456212282180786\n",
            "Step 13170, Loss 1.5535507202148438\n",
            "Step 13180, Loss 1.548459529876709\n",
            "Step 13190, Loss 1.2933659553527832\n",
            "Step 13200, Loss 0.9842934608459473\n",
            "Step 13210, Loss 1.2171947956085205\n",
            "Step 13220, Loss 0.4566611051559448\n",
            "Step 13230, Loss 2.315253734588623\n",
            "Step 13240, Loss 1.7951152324676514\n",
            "Step 13250, Loss 2.6506128311157227\n",
            "Step 13260, Loss 1.1951546669006348\n",
            "Step 13270, Loss 2.7952394485473633\n",
            "Step 13280, Loss 1.555955171585083\n",
            "Step 13290, Loss 2.338737726211548\n",
            "Step 13300, Loss 2.3702335357666016\n",
            "Step 13310, Loss 1.7601544857025146\n",
            "Step 13320, Loss 1.1751060485839844\n",
            "Step 13330, Loss 0.9312398433685303\n",
            "Step 13340, Loss 1.4998481273651123\n",
            "Step 13350, Loss 1.6852333545684814\n",
            "Step 13360, Loss 1.2702136039733887\n",
            "Step 13370, Loss 1.3863823413848877\n",
            "Step 13380, Loss 0.8945392370223999\n",
            "Step 13390, Loss 1.1463375091552734\n",
            "Step 13400, Loss 2.0080347061157227\n",
            "Step 13410, Loss 1.4130053520202637\n",
            "Step 13420, Loss 2.0459163188934326\n",
            "Step 13430, Loss 1.7241684198379517\n",
            "Step 13440, Loss 1.1573238372802734\n",
            "Step 13450, Loss 1.8225399255752563\n",
            "Step 13460, Loss 1.586336612701416\n",
            "Step 13470, Loss 1.3659617900848389\n",
            "Step 13480, Loss 0.934726357460022\n",
            "Step 13490, Loss 1.5553823709487915\n",
            "Step 13500, Loss 1.6389296054840088\n",
            "Step 13510, Loss 1.4554871320724487\n",
            "Step 13520, Loss 1.4552853107452393\n",
            "Step 13530, Loss 2.0994842052459717\n",
            "Step 13540, Loss 1.040280818939209\n",
            "Step 13550, Loss 1.4242823123931885\n",
            "Step 13560, Loss 1.721012830734253\n",
            "Step 13570, Loss 1.8387579917907715\n",
            "Step 13580, Loss 1.327606201171875\n",
            "Step 13590, Loss 1.236330509185791\n",
            "Step 13600, Loss 2.524923324584961\n",
            "Step 13610, Loss 1.456330418586731\n",
            "Step 13620, Loss 1.4719173908233643\n",
            "Step 13630, Loss 1.2675325870513916\n",
            "Step 13640, Loss 1.088900089263916\n",
            "Step 13650, Loss 1.7711501121520996\n",
            "Step 13660, Loss 1.1865262985229492\n",
            "Step 13670, Loss 0.8539242148399353\n",
            "Step 13680, Loss 1.4644006490707397\n",
            "Step 13690, Loss 2.4376649856567383\n",
            "Step 13700, Loss 0.8617825508117676\n",
            "Step 13710, Loss 2.0329697132110596\n",
            "Step 13720, Loss 0.6922064423561096\n",
            "Step 13730, Loss 2.0142900943756104\n",
            "Step 13740, Loss 0.8629031777381897\n",
            "Step 13750, Loss 1.1312217712402344\n",
            "Step 13760, Loss 3.0166778564453125\n",
            "Step 13770, Loss 2.1018311977386475\n",
            "Step 13780, Loss 1.198917031288147\n",
            "Step 13790, Loss 1.1680405139923096\n",
            "Step 13800, Loss 0.79502934217453\n",
            "Step 13810, Loss 2.6132144927978516\n",
            "Step 13820, Loss 1.5257915258407593\n",
            "Step 13830, Loss 0.7564542889595032\n",
            "Step 13840, Loss 0.9613015055656433\n",
            "Step 13850, Loss 1.5241436958312988\n",
            "Step 13860, Loss 1.3826663494110107\n",
            "Step 13870, Loss 1.6256705522537231\n",
            "Step 13880, Loss 1.1343557834625244\n",
            "Step 13890, Loss 1.3168604373931885\n",
            "Step 13900, Loss 1.0113805532455444\n",
            "Step 13910, Loss 0.6372872591018677\n",
            "Step 13920, Loss 2.251591682434082\n",
            "Step 13930, Loss 0.9462401270866394\n",
            "Step 13940, Loss 1.3819739818572998\n",
            "Step 13950, Loss 1.0026543140411377\n",
            "Step 13960, Loss 1.7236607074737549\n",
            "Step 13970, Loss 1.33466637134552\n",
            "Step 13980, Loss 1.9826512336730957\n",
            "Step 13990, Loss 1.4567859172821045\n",
            "Step 14000, Loss 1.4793907403945923\n",
            "Step 14010, Loss 0.8957804441452026\n",
            "Step 14020, Loss 2.0016214847564697\n",
            "Step 14030, Loss 0.8049218058586121\n",
            "Step 14040, Loss 1.4330742359161377\n",
            "Step 14050, Loss 1.441980242729187\n",
            "Step 14060, Loss 1.2956483364105225\n",
            "Step 14070, Loss 1.4873625040054321\n",
            "Step 14080, Loss 0.9669504165649414\n",
            "Step 14090, Loss 1.229309320449829\n",
            "Step 14100, Loss 1.8441650867462158\n",
            "Step 14110, Loss 1.3333971500396729\n",
            "Step 14120, Loss 1.2570841312408447\n",
            "Step 14130, Loss 0.7884432673454285\n",
            "Step 14140, Loss 3.4226882457733154\n",
            "Step 14150, Loss 1.4833638668060303\n",
            "Step 14160, Loss 1.287463903427124\n",
            "Step 14170, Loss 1.3188616037368774\n",
            "Step 14180, Loss 1.0471875667572021\n",
            "Step 14190, Loss 1.0374643802642822\n",
            "Step 14200, Loss 0.8804847598075867\n",
            "Step 14210, Loss 0.9740826487541199\n",
            "Step 14220, Loss 2.888239622116089\n",
            "Step 14230, Loss 1.2071000337600708\n",
            "Step 14240, Loss 0.8702919483184814\n",
            "Step 14250, Loss 1.7191166877746582\n",
            "Step 14260, Loss 0.917691171169281\n",
            "Step 14270, Loss 1.2949399948120117\n",
            "Step 14280, Loss 1.923657774925232\n",
            "Step 14290, Loss 1.8018375635147095\n",
            "Step 14300, Loss 0.8438624739646912\n",
            "Step 14310, Loss 1.4594438076019287\n",
            "Step 14320, Loss 1.703906536102295\n",
            "Step 14330, Loss 1.671216607093811\n",
            "Step 14340, Loss 1.2337205410003662\n",
            "Step 14350, Loss 1.5042526721954346\n",
            "Step 14360, Loss 1.8939242362976074\n",
            "Step 14370, Loss 0.8827987909317017\n",
            "Step 14380, Loss 0.44914424419403076\n",
            "Step 14390, Loss 1.4553464651107788\n",
            "Step 14400, Loss 1.6448016166687012\n",
            "Step 14410, Loss 0.692365825176239\n",
            "Step 14420, Loss 1.8197129964828491\n",
            "Step 14430, Loss 0.8942155838012695\n",
            "Step 14440, Loss 1.3012686967849731\n",
            "Step 14450, Loss 1.2442247867584229\n",
            "Step 14460, Loss 0.784821629524231\n",
            "Step 14470, Loss 0.9093406200408936\n",
            "Step 14480, Loss 1.5506508350372314\n",
            "Step 14490, Loss 1.4993133544921875\n",
            "Step 14500, Loss 1.2441514730453491\n",
            "Step 14510, Loss 0.6491683721542358\n",
            "Step 14520, Loss 0.789965033531189\n",
            "Step 14530, Loss 2.107935905456543\n",
            "Step 14540, Loss 1.7570022344589233\n",
            "Step 14550, Loss 0.9944507479667664\n",
            "Step 14560, Loss 1.761055588722229\n",
            "Step 14570, Loss 1.3522688150405884\n",
            "Step 14580, Loss 1.477007269859314\n",
            "Step 14590, Loss 1.525356411933899\n",
            "Step 14600, Loss 2.7073328495025635\n",
            "Step 14610, Loss 3.212580919265747\n",
            "Step 14620, Loss 2.0011870861053467\n",
            "Step 14630, Loss 0.6361731290817261\n",
            "Step 14640, Loss 1.2516826391220093\n",
            "Step 14650, Loss 1.2080864906311035\n",
            "Step 14660, Loss 1.247466802597046\n",
            "Step 14670, Loss 0.7742061614990234\n",
            "Step 14680, Loss 1.6024413108825684\n",
            "Step 14690, Loss 0.9515009522438049\n",
            "Step 14700, Loss 1.3235502243041992\n",
            "Step 14710, Loss 1.7228223085403442\n",
            "Step 14720, Loss 1.4324233531951904\n",
            "Step 14730, Loss 1.5769774913787842\n",
            "Step 14740, Loss 0.767445981502533\n",
            "Step 14750, Loss 1.309964656829834\n",
            "Step 14760, Loss 1.471205711364746\n",
            "Step 14770, Loss 0.725620687007904\n",
            "Step 14780, Loss 1.2608895301818848\n",
            "Step 14790, Loss 1.8307746648788452\n",
            "Step 14800, Loss 1.8026373386383057\n",
            "Step 14810, Loss 1.4653820991516113\n",
            "Step 14820, Loss 2.486632823944092\n",
            "Step 14830, Loss 1.6900718212127686\n",
            "Step 14840, Loss 0.6480758786201477\n",
            "Step 14850, Loss 1.208711862564087\n",
            "Step 14860, Loss 1.2072296142578125\n",
            "Step 14870, Loss 0.5167316198348999\n",
            "Step 14880, Loss 1.0479251146316528\n",
            "Step 14890, Loss 1.4751592874526978\n",
            "Step 14900, Loss 0.8664289712905884\n",
            "Step 14910, Loss 0.525976300239563\n",
            "Step 14920, Loss 1.400660514831543\n",
            "Step 14930, Loss 1.4134310483932495\n",
            "Step 14940, Loss 1.4866907596588135\n",
            "Step 14950, Loss 1.2327187061309814\n",
            "Step 14960, Loss 1.412941575050354\n",
            "Step 14970, Loss 2.5473814010620117\n",
            "Step 14980, Loss 1.2406232357025146\n",
            "Step 14990, Loss 1.694770097732544\n",
            "Step 15000, Loss 0.48128119111061096\n",
            "Step 15010, Loss 0.574489951133728\n",
            "Step 15020, Loss 0.48708707094192505\n",
            "Step 15030, Loss 1.4635655879974365\n",
            "Step 15040, Loss 1.6864423751831055\n",
            "Step 15050, Loss 1.0485628843307495\n",
            "Step 15060, Loss 0.8543752431869507\n",
            "Step 15070, Loss 2.0429155826568604\n",
            "Step 15080, Loss 1.7545074224472046\n",
            "Step 15090, Loss 2.267174005508423\n",
            "Step 15100, Loss 1.9000979661941528\n",
            "Step 15110, Loss 1.3649868965148926\n",
            "Step 15120, Loss 0.8680273294448853\n",
            "Step 15130, Loss 1.6913822889328003\n",
            "Step 15140, Loss 1.6390687227249146\n",
            "Step 15150, Loss 1.949493408203125\n",
            "Step 15160, Loss 1.7317731380462646\n",
            "Step 15170, Loss 1.0800398588180542\n",
            "Step 15180, Loss 1.4494307041168213\n",
            "Step 15190, Loss 2.085367441177368\n",
            "Step 15200, Loss 1.696308970451355\n",
            "Step 15210, Loss 1.1468782424926758\n",
            "Step 15220, Loss 1.4182695150375366\n",
            "Step 15230, Loss 0.752807080745697\n",
            "Step 15240, Loss 0.9644416570663452\n",
            "Step 15250, Loss 1.7767271995544434\n",
            "Step 15260, Loss 1.0712499618530273\n",
            "Step 15270, Loss 0.779118537902832\n",
            "Step 15280, Loss 1.1531922817230225\n",
            "Step 15290, Loss 2.161433458328247\n",
            "Step 15300, Loss 0.4859301447868347\n",
            "Step 15310, Loss 1.5158344507217407\n",
            "Step 15320, Loss 1.1811585426330566\n",
            "Step 15330, Loss 1.2591629028320312\n",
            "Step 15340, Loss 1.1956321001052856\n",
            "Step 15350, Loss 1.9186007976531982\n",
            "Step 15360, Loss 2.116396188735962\n",
            "Step 15370, Loss 0.7389148473739624\n",
            "Step 15380, Loss 2.5495500564575195\n",
            "Step 15390, Loss 1.1823900938034058\n",
            "Step 15400, Loss 1.4455029964447021\n",
            "Step 15410, Loss 1.893875002861023\n",
            "Step 15420, Loss 1.3798856735229492\n",
            "Step 15430, Loss 1.4422688484191895\n",
            "Step 15440, Loss 1.3275940418243408\n",
            "Step 15450, Loss 1.2532234191894531\n",
            "Step 15460, Loss 1.9251134395599365\n",
            "Step 15470, Loss 1.6473939418792725\n",
            "Step 15480, Loss 1.4811420440673828\n",
            "Step 15490, Loss 1.889840841293335\n",
            "Step 15500, Loss 1.320556640625\n",
            "Step 15510, Loss 1.5141977071762085\n",
            "Step 15520, Loss 1.8499388694763184\n",
            "Step 15530, Loss 1.5267151594161987\n",
            "Step 15540, Loss 1.1671327352523804\n",
            "Step 15550, Loss 0.3615381121635437\n",
            "Step 15560, Loss 2.0545437335968018\n",
            "Step 15570, Loss 0.7673556804656982\n",
            "Step 15580, Loss 1.27126944065094\n",
            "Step 15590, Loss 1.4166532754898071\n",
            "Step 15600, Loss 1.5981026887893677\n",
            "Step 15610, Loss 1.013960361480713\n",
            "Step 15620, Loss 1.3702481985092163\n",
            "Step 15630, Loss 1.6529823541641235\n",
            "Step 15640, Loss 1.662604808807373\n",
            "Step 15650, Loss 1.2947745323181152\n",
            "Step 15660, Loss 1.6985807418823242\n",
            "Step 15670, Loss 1.272984504699707\n",
            "Step 15680, Loss 1.3285961151123047\n",
            "Step 15690, Loss 1.1603707075119019\n",
            "Step 15700, Loss 0.9941601753234863\n",
            "Step 15710, Loss 2.1883223056793213\n",
            "Step 15720, Loss 1.1432260274887085\n",
            "Step 15730, Loss 2.0483155250549316\n",
            "Step 15740, Loss 1.086240291595459\n",
            "Step 15750, Loss 1.8007858991622925\n",
            "Step 15760, Loss 1.5302855968475342\n",
            "Step 15770, Loss 1.1853759288787842\n",
            "Step 15780, Loss 0.7380019426345825\n",
            "Step 15790, Loss 2.0015499591827393\n",
            "Step 15800, Loss 1.4527552127838135\n",
            "Step 15810, Loss 0.8042771816253662\n",
            "Step 15820, Loss 1.482940673828125\n",
            "Step 15830, Loss 0.9521361589431763\n",
            "Step 15840, Loss 1.0526764392852783\n",
            "Step 15850, Loss 2.3736443519592285\n",
            "Step 15860, Loss 0.546882152557373\n",
            "Step 15870, Loss 1.4639947414398193\n",
            "Step 15880, Loss 1.4865978956222534\n",
            "Step 15890, Loss 1.7891770601272583\n",
            "Step 15900, Loss 1.5068061351776123\n",
            "Step 15910, Loss 1.7363258600234985\n",
            "Step 15920, Loss 1.5804288387298584\n",
            "Step 15930, Loss 2.4483814239501953\n",
            "Step 15940, Loss 1.977647304534912\n",
            "Step 15950, Loss 1.5399346351623535\n",
            "Step 15960, Loss 1.2875816822052002\n",
            "Step 15970, Loss 1.4382975101470947\n",
            "Step 15980, Loss 1.7020384073257446\n",
            "Step 15990, Loss 2.4059228897094727\n",
            "Step 16000, Loss 1.3076503276824951\n",
            "Step 16010, Loss 1.18808913230896\n",
            "Step 16020, Loss 2.2984724044799805\n",
            "Step 16030, Loss 1.4403507709503174\n",
            "Step 16040, Loss 1.8626484870910645\n",
            "Step 16050, Loss 1.707507610321045\n",
            "Step 16060, Loss 0.6603651642799377\n",
            "Step 16070, Loss 1.2507133483886719\n",
            "Step 16080, Loss 1.426560878753662\n",
            "Step 16090, Loss 1.3503308296203613\n",
            "Step 16100, Loss 1.6503522396087646\n",
            "Step 16110, Loss 1.3689144849777222\n",
            "Step 16120, Loss 1.458359718322754\n",
            "Step 16130, Loss 0.7981683611869812\n",
            "Step 16140, Loss 1.5577824115753174\n",
            "Step 16150, Loss 1.4740996360778809\n",
            "Step 16160, Loss 1.1007153987884521\n",
            "Step 16170, Loss 1.8996782302856445\n",
            "Step 16180, Loss 1.2469104528427124\n",
            "Step 16190, Loss 2.1664845943450928\n",
            "Step 16200, Loss 1.6294631958007812\n",
            "Step 16210, Loss 1.2387874126434326\n",
            "Step 16220, Loss 1.2101645469665527\n",
            "Step 16230, Loss 1.7440705299377441\n",
            "Step 16240, Loss 1.4026265144348145\n",
            "Step 16250, Loss 1.7969021797180176\n",
            "Step 16260, Loss 1.4732320308685303\n",
            "Step 16270, Loss 1.0737025737762451\n",
            "Step 16280, Loss 1.3860559463500977\n",
            "Step 16290, Loss 2.491866111755371\n",
            "Step 16300, Loss 0.9264227747917175\n",
            "Step 16310, Loss 1.625615119934082\n",
            "Step 16320, Loss 1.136584758758545\n",
            "Step 16330, Loss 1.1269391775131226\n",
            "Step 16340, Loss 3.004804849624634\n",
            "Step 16350, Loss 0.9457491040229797\n",
            "Step 16360, Loss 0.7011619806289673\n",
            "Step 16370, Loss 1.1836800575256348\n",
            "Step 16380, Loss 0.8807641267776489\n",
            "Step 16390, Loss 0.8939409255981445\n",
            "Step 16400, Loss 1.8752273321151733\n",
            "Step 16410, Loss 1.2058695554733276\n",
            "Step 16420, Loss 1.4266822338104248\n",
            "Step 16430, Loss 0.7008482217788696\n",
            "Step 16440, Loss 0.5959384441375732\n",
            "Step 16450, Loss 1.7610199451446533\n",
            "Step 16460, Loss 1.7735942602157593\n",
            "Step 16470, Loss 1.4647834300994873\n",
            "Step 16480, Loss 0.6655586361885071\n",
            "Step 16490, Loss 1.7790682315826416\n",
            "Step 16500, Loss 1.2681934833526611\n",
            "Step 16510, Loss 1.203980803489685\n",
            "Step 16520, Loss 1.3282172679901123\n",
            "Step 16530, Loss 1.2327488660812378\n",
            "Step 16540, Loss 1.6080081462860107\n",
            "Step 16550, Loss 2.8227035999298096\n",
            "Step 16560, Loss 0.6216706037521362\n",
            "Step 16570, Loss 1.0704153776168823\n",
            "Step 16580, Loss 1.060373067855835\n",
            "Step 16590, Loss 1.873132586479187\n",
            "Step 16600, Loss 2.7252933979034424\n",
            "Step 16610, Loss 2.3558006286621094\n",
            "Step 16620, Loss 2.031355857849121\n",
            "Step 16630, Loss 1.0736658573150635\n",
            "Step 16640, Loss 1.530970811843872\n",
            "Step 16650, Loss 0.7467984557151794\n",
            "Step 16660, Loss 1.1279442310333252\n",
            "Step 16670, Loss 1.3116445541381836\n",
            "Step 16680, Loss 1.1169447898864746\n",
            "Step 16690, Loss 1.5195798873901367\n",
            "Step 16700, Loss 1.4528690576553345\n",
            "Step 16710, Loss 1.2958192825317383\n",
            "Step 16720, Loss 0.6239907741546631\n",
            "Step 16730, Loss 1.184032678604126\n",
            "Step 16740, Loss 1.0039008855819702\n",
            "Step 16750, Loss 1.612547755241394\n",
            "Step 16760, Loss 1.5420734882354736\n",
            "Step 16770, Loss 1.2970585823059082\n",
            "Step 16780, Loss 1.168029546737671\n",
            "Step 16790, Loss 0.9804705381393433\n",
            "Step 16800, Loss 0.6769406795501709\n",
            "Step 16810, Loss 1.1659687757492065\n",
            "Step 16820, Loss 0.96295166015625\n",
            "Step 16830, Loss 1.8365392684936523\n",
            "Step 16840, Loss 1.3559153079986572\n",
            "Step 16850, Loss 1.7082875967025757\n",
            "Step 16860, Loss 1.1290645599365234\n",
            "Step 16870, Loss 1.2678688764572144\n",
            "Step 16880, Loss 1.6868029832839966\n",
            "Step 16890, Loss 3.3405866622924805\n",
            "Step 16900, Loss 1.1381452083587646\n",
            "Step 16910, Loss 0.6814725995063782\n",
            "Step 16920, Loss 0.7934665679931641\n",
            "Step 16930, Loss 2.1213650703430176\n",
            "Step 16940, Loss 1.217545986175537\n",
            "Step 16950, Loss 0.8550691604614258\n",
            "Step 16960, Loss 1.976151466369629\n",
            "Step 16970, Loss 1.1070491075515747\n",
            "Step 16980, Loss 1.9971767663955688\n",
            "Step 16990, Loss 1.732919692993164\n",
            "Step 17000, Loss 1.8139201402664185\n",
            "Step 17010, Loss 2.2549526691436768\n",
            "Step 17020, Loss 0.8623912930488586\n",
            "Step 17030, Loss 1.3466503620147705\n",
            "Step 17040, Loss 1.3285772800445557\n",
            "Step 17050, Loss 1.6210564374923706\n",
            "Step 17060, Loss 1.4526487588882446\n",
            "Step 17070, Loss 1.339125633239746\n",
            "Step 17080, Loss 1.1819971799850464\n",
            "Step 17090, Loss 1.7714335918426514\n",
            "Step 17100, Loss 1.4996678829193115\n",
            "Step 17110, Loss 0.9415209293365479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FHx9lK_FHZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgH2fMG9I0p4",
        "colab_type": "code",
        "outputId": "53652fc5-3540-4623-ecc6-f57f4df30465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "#FINAL TEST ACCURACY ON TEST DATASET \n",
        "#Evaluation for target encoder by source classifier on target dataset\n",
        "\n",
        "top_classes = 3\n",
        "\n",
        "softmax = torch.nn.Softmax()\n",
        "\n",
        "#full vgg for compute saliency map\n",
        "tgt_full = vgg16(pretrained = True)\n",
        "features_fn = tgt_full.features\n",
        "classifier_fn = nn.Sequential(*([Flatten()] + list(tgt_full.classifier.children())))\n",
        "\n",
        "features_fn = features_fn.to(DEVICE)\n",
        "classifier_fn = classifier_fn.to(DEVICE)\n",
        "\n",
        "tgt_encoder = tgt_encoder.to(DEVICE)\n",
        "tgt_encoder.train(False)\n",
        "\n",
        "src_classfier = src_classifier.to(DEVICE)\n",
        "src_classifier.train(False)\n",
        "\n",
        "evidence_encoder = evidence_encoder.to(DEVICE)\n",
        "evidence_encoder.train(False)\n",
        "\n",
        "evidence_classifier = evidence_classifier.to(DEVICE)\n",
        "evidence_classifier.train(False)  \n",
        "\n",
        "# init loss and accuracy\n",
        "\n",
        "running_corrects = 0\n",
        "running_corrects_ev = 0\n",
        "running_corrects_gz = 0\n",
        "# set loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "tgt_accuracy = 0\n",
        "# evaluate network\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    outputs = src_classifier(tgt_encoder(images))\n",
        "\n",
        "    #Get predictions conventional cnn\n",
        "    _, pred = torch.max(outputs.data, 1)\n",
        "\n",
        "    pb = np.array([])\n",
        "    pb_ev = np.array([])\n",
        "    lb = []\n",
        "    lb_ev = []\n",
        "    images_batch = np.zeros((16,3,224,224))\n",
        "\n",
        "    \n",
        "    for i in range(BATCH_SIZE):\n",
        "\n",
        "      probabilities = softmax(outputs[i])\n",
        "      top3_prob, top3_label = torch.topk(probabilities,top_classes)\n",
        "      top3_prob = torch.tensor(top3_prob, device = 'cpu').float()\n",
        "\n",
        "      if(top3_label[0]< top3_label[1]):\n",
        "        if(top3_label[0]<top3_label[2]):\n",
        "          lb.append(top3_label[0])\n",
        "          pb = np.append(pb,top3_prob[0])\n",
        "          if(top3_label[1]<top3_label[2]):\n",
        "            lb.append(top3_label[1])\n",
        "            pb = np.append(pb,top3_prob[1])\n",
        "            lb.append(top3_label[2])\n",
        "            pb = np.append(pb,top3_prob[2])\n",
        "          else:\n",
        "            lb.append(top3_label[2])\n",
        "            pb = np.append(pb,top3_prob[2])\n",
        "            lb.append(top3_label[1])\n",
        "            pb = np.append(pb,top3_prob[1])\n",
        "        else:\n",
        "          lb.append(top3_label[2])\n",
        "          pb = np.append(pb,top3_prob[2])\n",
        "          lb.append(top3_label[0])\n",
        "          pb = np.append(pb,top3_prob[0])\n",
        "          lb.append(top3_label[1])\n",
        "          pb = np.append(pb,top3_prob[1])\n",
        "      else:\n",
        "        if(top3_label[1]<top3_label[2]):\n",
        "          lb.append(top3_label[1])\n",
        "          pb = np.append(pb,top3_prob[1])           \n",
        "          if(top3_label[0]<top3_label[2]):\n",
        "            lb.append(top3_label[0])\n",
        "            pb = np.append(pb,top3_prob[0])\n",
        "            lb.append(top3_label[2])\n",
        "            pb = np.append(pb,top3_prob[2])\n",
        "          else:\n",
        "            lb.append(top3_label[2])\n",
        "            pb = np.append(pb,top3_prob[2])\n",
        "            lb.append(top3_label[0])\n",
        "            pb = np.append(pb,top3_prob[0])\n",
        "        else:\n",
        "          lb.append(top3_label[2])\n",
        "          pb = np.append(pb,top3_prob[2])\n",
        "          lb.append(top3_label[1])\n",
        "          pb = np.append(pb,top3_prob[1])\n",
        "          lb.append(top3_label[0])\n",
        "          pb = np.append(pb,top3_prob[0])           \n",
        "\n",
        "      \n",
        "      sal = GradCAM(images, i, features_fn, classifier_fn)\n",
        "      sal = Image.fromarray(sal)\n",
        "      sal = sal.resize((224, 224), resample=Image.LINEAR)\n",
        "      #from cuda a cpu to numpy\n",
        "      images = images.cpu()\n",
        "\n",
        "      #from tensor to image\n",
        "      img = denormalize(images[i])\n",
        "      images_batch[i] = images[i]\n",
        "      \n",
        "    #to tensor and to device(gpu)\n",
        "    img = torch.tensor(img, device = DEVICE).float()\n",
        "    images_batch = torch.tensor(images_batch, device = DEVICE).float()\n",
        "    images_batch.to(DEVICE)\n",
        "\n",
        "    #Test on evidence cnn  \n",
        "    out = evidence_classifier(evidence_encoder(images_batch))\n",
        "\n",
        "    _, pred_ev = torch.max(out.data, 1)\n",
        "\n",
        "    mean = np.array([])\n",
        "\n",
        "    for j in range(BATCH_SIZE):\n",
        "      probabilities_evidence = softmax(out[j])\n",
        "      top3_prob_ev, top3_label_ev = torch.topk(probabilities_evidence,top_classes)\n",
        "      top3_prob_ev = torch.tensor(top3_prob_ev, device = 'cpu').float()\n",
        "\n",
        "      if(top3_label_ev[0]< top3_label_ev[1]):\n",
        "        if(top3_label_ev[0]<top3_label_ev[2]):\n",
        "          lb_ev.append(top3_label_ev[0])\n",
        "          pb_ev = np.append(pb_ev,top3_prob_ev[0])\n",
        "          if(top3_label_ev[1]<top3_label_ev[2]):\n",
        "            lb_ev.append(top3_label_ev[1])\n",
        "            pb_ev = np.append(pb_ev,top3_prob_ev[1])\n",
        "            lb_ev.append(top3_label_ev[2])\n",
        "            pb_ev = np.append(pb_ev,top3_prob_ev[2])\n",
        "          else:\n",
        "            lb_ev.append(top3_label_ev[2])\n",
        "            pb_ev = np.append(pb_ev,top3_prob_ev[2])\n",
        "            lb_ev.append(top3_label_ev[1])\n",
        "            pb_ev = np.append(pb_ev,top3_prob_ev[1])\n",
        "        else:\n",
        "          lb_ev.append(top3_label_ev[2])\n",
        "          pb_ev = np.append(pb_ev,top3_prob_ev[2])\n",
        "          lb_ev.append(top3_label_ev[0])\n",
        "          pb_ev = np.append(pb_ev,top3_prob_ev[0])\n",
        "          lb_ev.append(top3_label_ev[1])\n",
        "          pb_ev = np.append(pb_ev,top3_prob_ev[1])\n",
        "      else:\n",
        "        if(top3_label_ev[1]<top3_label_ev[2]):\n",
        "          lb_ev.append(top3_label_ev[1])\n",
        "          pb_ev = np.append(pb_ev,top3_prob_ev[1])           \n",
        "          if(top3_label_ev[0]<top3_label_ev[2]):\n",
        "            lb_ev.append(top3_label_ev[0])\n",
        "            pb_ev = np.append(pb_ev,top3_prob_ev[0])\n",
        "            lb_ev.append(top3_label_ev[2])\n",
        "            pb_ev = np.append(pb_ev,top3_prob_ev[2])\n",
        "          else:\n",
        "            lb_ev.append(top3_label_ev[2])\n",
        "            pb_ev = np.append(pb_ev,top3_prob_ev[2])\n",
        "            lb_ev.append(top3_label_ev[0])\n",
        "            pb_ev = np.append(pb_ev,top3_prob_ev[0])\n",
        "        else:\n",
        "          lb_ev.append(top3_label_ev[2])\n",
        "          pb_ev = np.append(pb_ev,top3_prob_ev[2])\n",
        "          lb_ev.append(top3_label_ev[1])\n",
        "          pb_ev = np.append(pb_ev,top3_prob_ev[1])\n",
        "          lb_ev.append(top3_label_ev[0])\n",
        "          pb_ev = np.append(pb_ev,top3_prob_ev[0])  \n",
        "\n",
        "      mean = np.append(mean,(pb[j*3:(j*3)+3]+pb_ev[j*3:(j*3)+3])/2)\n",
        "      ind = np.argmax(mean[j*3:(j*3)+3])\n",
        "      if(labels[j] == lb[ind]):\n",
        "        running_corrects_gz+=1\n",
        "\n",
        "\n",
        "    # Update Corrects conventional\n",
        "    running_corrects += torch.sum(pred == labels.data).data.item()\n",
        "    #Update corrects evidence \n",
        "    running_corrects_ev += torch.sum(pred_ev == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "tgt_accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "evidence_accuracy = running_corrects_ev / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy on Target Domain: {}'.format(tgt_accuracy)) \n",
        "print()\n",
        "print('Test Accuracy of the Evidence CNN on the target Domain: {}'.format(evidence_accuracy)) \n",
        "print()\n",
        "print(\"Refined Prediction: {}\".format(running_corrects_gz))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1610 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:150: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "100%|██████████| 1610/1610 [43:30<00:00,  1.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy on Target Domain: 0.5230201863354037\n",
            "\n",
            "Test Accuracy of the Evidence CNN on the target Domain: 0.5230201863354037\n",
            "\n",
            "Refined Prediction: 1237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qx99--tc8TL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}